#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é—®å·åˆ†æFlaskåç«¯æœåŠ¡
æä¾›ä¸å‰ç«¯QuestionnaireAnalysis.jså…¼å®¹çš„APIæ¥å£
"""

import os
import sys
import logging
import time
import json
import uuid
from pathlib import Path
from datetime import datetime
from flask import Flask, request, jsonify, send_file
from flask_cors import CORS
from werkzeug.utils import secure_filename
import pandas as pd
import numpy as np
import traceback

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("api_processing.log", encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger()

# åˆ›å»ºFlaskåº”ç”¨
app = Flask(__name__)
CORS(app)  # å¯ç”¨è·¨åŸŸæ”¯æŒ

# é…ç½®ä¸Šä¼ æ–‡ä»¶å¤¹
UPLOAD_FOLDER = Path(__file__).parent / 'uploads'
UPLOAD_FOLDER.mkdir(exist_ok=True)

# é…ç½®å…è®¸çš„æ–‡ä»¶ç±»å‹
ALLOWED_EXTENSIONS = {'csv', 'xlsx', 'xls', 'txt'}

# å­˜å‚¨åˆ†æç»“æœçš„ä¸´æ—¶æ•°æ®ç»“æ„
analysis_results = {}

def convert_pandas_types(obj):
    """é€’å½’è½¬æ¢pandaså’Œnumpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹ï¼Œç”¨äºJSONåºåˆ—åŒ–"""
    if isinstance(obj, dict):
        return {k: convert_pandas_types(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_pandas_types(item) for item in obj]
    elif isinstance(obj, (np.int64, np.int32, np.int16, np.int8)):
        return int(obj)
    elif isinstance(obj, (np.float64, np.float32, np.float16)):
        return float(obj)
    elif isinstance(obj, np.bool_):
        return bool(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif pd.isna(obj):
        return None
    else:
        return obj

def allowed_file(filename):
    """æ£€æŸ¥æ–‡ä»¶ç±»å‹æ˜¯å¦å…è®¸"""
    return '.' in filename and \
           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def get_file_extension(filename):
    """è·å–æ–‡ä»¶æ‰©å±•å"""
    return filename.rsplit('.', 1)[1].lower() if '.' in filename else ''

def secure_chinese_filename(filename):
    """å®‰å…¨åœ°å¤„ç†ä¸­æ–‡æ–‡ä»¶å"""
    # ç§»é™¤è·¯å¾„åˆ†éš”ç¬¦å’Œå…¶ä»–ä¸å®‰å…¨å­—ç¬¦
    filename = filename.replace('/', '_').replace('\\', '_')
    filename = filename.replace('..', '_')
    # ä¿ç•™æ–‡ä»¶åä¸­çš„ä¸­æ–‡å­—ç¬¦
    return filename

@app.route('/upload-questionnaire', methods=['POST'])
def upload_questionnaire():
    """ä¸Šä¼ é—®å·æ•°æ®æ–‡ä»¶"""
    try:
        logger.info("ğŸ“¤ æ”¶åˆ°æ–‡ä»¶ä¸Šä¼ è¯·æ±‚")
        
        if 'file' not in request.files:
            return jsonify({'error': 'æ²¡æœ‰æ–‡ä»¶è¢«ä¸Šä¼ '}), 400
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({'error': 'æ²¡æœ‰é€‰æ‹©æ–‡ä»¶'}), 400
        
        if not allowed_file(file.filename):
            return jsonify({'error': 'ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œè¯·ä¸Šä¼ CSVã€Excelæˆ–TXTæ–‡ä»¶'}), 400
        
        # å®‰å…¨åœ°ä¿å­˜æ–‡ä»¶ï¼Œä¿ç•™ä¸­æ–‡å­—ç¬¦
        filename = secure_chinese_filename(file.filename or 'unknown_file')
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        unique_filename = f"{timestamp}_{filename}"
        file_path = UPLOAD_FOLDER / unique_filename
        
        file.save(str(file_path))
        logger.info(f"âœ… æ–‡ä»¶ä¿å­˜æˆåŠŸ: {file_path}")
        
        # ç”Ÿæˆåˆ†æID
        analysis_id = str(uuid.uuid4())
        
        # è¯»å–æ–‡ä»¶å†…å®¹å¹¶åˆ†æå­—æ®µ
        try:
            try:
                from universal_questionnaire_analyzer import UniversalQuestionnaireAnalyzer
                logger.info("âœ… æˆåŠŸå¯¼å…¥ UniversalQuestionnaireAnalyzer")
            except ImportError as e:
                logger.error(f"âŒ å¯¼å…¥ UniversalQuestionnaireAnalyzer å¤±è´¥: {e}")
                return jsonify({'error': f'å¯¼å…¥åˆ†ææ¨¡å—å¤±è´¥: {str(e)}'}), 500
            
            analyzer = UniversalQuestionnaireAnalyzer()
            df = analyzer.read_data_file(str(file_path))
            
            if df is None:
                return jsonify({'error': 'æ— æ³•è¯»å–æ–‡ä»¶å†…å®¹'}), 400
            
            # è¯†åˆ«é—®é¢˜ç±»å‹
            logger.info(f"ğŸ” å¼€å§‹è¯†åˆ«é¢˜å‹ï¼Œå…±{len(df.columns)}ä¸ªå­—æ®µ")
            logger.info(f"ğŸ“‹ å­—æ®µåˆ—è¡¨: {df.columns.tolist()}")
            
            question_types = analyzer.identify_all_question_types(df)
            
            # è¾“å‡ºè¯†åˆ«ç»“æœç”¨äºè°ƒè¯•
            logger.info(f"ğŸ“Š é¢˜å‹è¯†åˆ«å®Œæˆ:")
            logger.info(f"  - é‡è¡¨é¢˜: {len(question_types.get('scale_questions', []))} ä¸ª")
            logger.info(f"  - å•é€‰é¢˜: {len(question_types.get('single_choice', []))} ä¸ª")
            logger.info(f"  - å¼€æ”¾é¢˜: {len(question_types.get('open_ended', []))} ä¸ª")
            
            # å­˜å‚¨åˆ†æä¿¡æ¯
            analysis_results[analysis_id] = {
                'file_path': str(file_path),
                'filename': filename,
                'dataframe': df,
                'question_types': question_types,
                'upload_time': datetime.now().isoformat()
            }
            
            # è½¬æ¢æ•°æ®ç»“æ„ä»¥åŒ¹é…å‰ç«¯æœŸæœ›
            def transform_question_types(question_types):
                """è½¬æ¢åç«¯æ•°æ®ç»“æ„ä¸ºå‰ç«¯æœŸæœ›çš„æ ¼å¼ - é€‚åº”æ–°çš„å­—æ®µçº§é¢˜å‹è¯†åˆ«"""
                logger.info(f"ğŸ”„ å¼€å§‹è½¬æ¢æ•°æ®ç»“æ„")
                logger.info(f"åŸå§‹æ•°æ®keys: {list(question_types.keys())}")
                
                # å…ˆè¾“å‡ºåŸå§‹æ•°æ®çš„å†…å®¹
                logger.info(f"åŸå§‹æ•°æ®å†…å®¹è¯¦æƒ…:")
                for key, value in question_types.items():
                    logger.info(f"  {key}: {type(value)} - {len(value) if isinstance(value, (list, dict)) else 'N/A'}")
                
                transformed = {
                    'scaleQuestions': [],
                    'singleChoice': [],
                    'openEnded': [],
                    'field_types': [],  # æ‰€æœ‰å­—æ®µçš„é¢˜å‹ä¿¡æ¯
                    'summary': {
                        'breakdown': {}
                    }
                }
                
                # å¤„ç†æ‰€æœ‰å­—æ®µç±»å‹
                field_types = question_types.get('field_types', [])
                logger.info(f"å­—æ®µç±»å‹åŸå§‹æ•°æ®: {len(field_types)} ä¸ª")
                transformed['field_types'] = field_types
                
                # å¤„ç†é‡è¡¨é¢˜
                scale_questions = question_types.get('scale_questions', [])
                logger.info(f"é‡è¡¨é¢˜åŸå§‹æ•°æ®: {len(scale_questions)} ä¸ª")
                transformed['scaleQuestions'] = scale_questions
                
                # å¤„ç†å•é€‰é¢˜
                single_choice = question_types.get('single_choice', [])
                logger.info(f"å•é€‰é¢˜åŸå§‹æ•°æ®: {len(single_choice)} ä¸ª")
                transformed['singleChoice'] = single_choice
                
                # å¤„ç†å¼€æ”¾é¢˜
                open_ended = question_types.get('open_ended', [])
                logger.info(f"å¼€æ”¾é¢˜åŸå§‹æ•°æ®: {len(open_ended)} ä¸ª")
                transformed['openEnded'] = open_ended
                
                # æ–°é€»è¾‘ä¸­æ— å…¶ä»–é¢˜å‹ï¼Œæ‰€æœ‰å­—æ®µéƒ½è¢«åˆ†ç±»ä¸ºé‡è¡¨é¢˜ã€å•é€‰é¢˜æˆ–å¼€æ”¾é¢˜
                logger.info(f"æ–°é€»è¾‘å·²è¦†ç›–æ‰€æœ‰å­—æ®µï¼Œæ— å…¶ä»–é¢˜å‹")
                
                # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
                transformed['summary']['breakdown'] = {
                    'total_fields': len(field_types),
                    'scaleQuestions': len(scale_questions),
                    'singleChoice': len(single_choice),
                    'openEnded': len(open_ended)
                }
                
                logger.info(f"âœ… è½¬æ¢å®Œæˆï¼Œç»Ÿè®¡ä¿¡æ¯: {transformed['summary']['breakdown']}")
                
                # è¾“å‡ºè½¬æ¢åçš„æ•°æ®ç»“æ„ç”¨äºè°ƒè¯•
                logger.info(f"è½¬æ¢åçš„æ•°æ®ç»“æ„:")
                for key, value in transformed.items():
                    if key != 'summary':
                        logger.info(f"  {key}: {len(value) if isinstance(value, list) else type(value)}")
                
                return transformed
            
            # è¿”å›ä¸Šä¼ ä¿¡æ¯
            response_data = {
                'analysisId': analysis_id,
                'filename': filename,
                'fileSize': file_path.stat().st_size,
                'rowCount': len(df),
                'columnCount': len(df.columns),
                'questionTypes': convert_pandas_types(transform_question_types(question_types)),
                'columns': df.columns.tolist()
            }
            
            logger.info(f"âœ… æ–‡ä»¶åˆ†æå®Œæˆï¼Œåˆ†æID: {analysis_id}")
            return jsonify(response_data)
            
        except Exception as e:
            logger.error(f"âŒ æ–‡ä»¶åˆ†æå¤±è´¥: {e}")
            return jsonify({'error': f'æ–‡ä»¶åˆ†æå¤±è´¥: {str(e)}'}), 500
            
    except Exception as e:
        logger.error(f"âŒ ä¸Šä¼ å¤„ç†å¤±è´¥: {e}")
        return jsonify({'error': f'ä¸Šä¼ å¤„ç†å¤±è´¥: {str(e)}'}), 500

@app.route('/analyze-text', methods=['POST'])
def analyze_text():
    """åˆ†æé—®å·æ•°æ® - å¤„ç†æ‰€æœ‰å­—æ®µ"""
    try:
        logger.info("ğŸ” æ”¶åˆ°åˆ†æè¯·æ±‚")
        
        data = request.get_json()
        analysis_id = data.get('analysisId')
        
        if not analysis_id or analysis_id not in analysis_results:
            return jsonify({'error': 'æ— æ•ˆçš„åˆ†æID'}), 400
        
        analysis_info = analysis_results[analysis_id]
        df = analysis_info['dataframe']
        
        # ä½¿ç”¨æ‰€æœ‰å­—æ®µè¿›è¡Œåˆ†æ
        all_fields = df.columns.tolist()
        logger.info(f"ğŸ“Š å¼€å§‹åˆ†ææ‰€æœ‰å­—æ®µï¼Œå…± {len(all_fields)} ä¸ªå­—æ®µ")
        
        # æ‰§è¡Œå®Œæ•´çš„åˆ†ææµç¨‹
        try:
            # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨classificationå¤„ç†
            logger.info("ğŸ”§ ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨classificationå¤„ç†")
            
            try:
                from classification import QuestionnaireTranslationClassifier
                logger.info("âœ… æˆåŠŸå¯¼å…¥ QuestionnaireTranslationClassifier")
            except ImportError as e:
                logger.error(f"âŒ å¯¼å…¥ QuestionnaireTranslationClassifier å¤±è´¥: {e}")
                return jsonify({'error': f'å¯¼å…¥ classification æ¨¡å—å¤±è´¥: {str(e)}'}), 500
            
            classifier = QuestionnaireTranslationClassifier()
            input_file = analysis_info['file_path']
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            classification_output = Path(input_file).parent / f"classification_{timestamp}.xlsx"
            
            success = classifier.process_table(input_file, str(classification_output))
            
            if not success:
                return jsonify({'error': 'classificationå¤„ç†å¤±è´¥'}), 500
            
            logger.info(f"âœ… classificationå¤„ç†å®Œæˆ: {classification_output}")
            
            # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨universal_questionnaire_analyzeråˆ†æ
            logger.info("ğŸ” ç¬¬äºŒæ­¥ï¼šä½¿ç”¨universal_questionnaire_analyzeråˆ†æ")
            
            try:
                from universal_questionnaire_analyzer import UniversalQuestionnaireAnalyzer
                logger.info("âœ… æˆåŠŸå¯¼å…¥ UniversalQuestionnaireAnalyzer")
            except ImportError as e:
                logger.error(f"âŒ å¯¼å…¥ UniversalQuestionnaireAnalyzer å¤±è´¥: {e}")
                return jsonify({'error': f'å¯¼å…¥åˆ†ææ¨¡å—å¤±è´¥: {str(e)}'}), 500
            
            analyzer = UniversalQuestionnaireAnalyzer()
            processed_df = analyzer.read_data_file(str(classification_output))
            
            if processed_df is None:
                return jsonify({'error': 'æ— æ³•è¯»å–å¤„ç†åçš„æ–‡ä»¶'}), 500
            
            # ä½¿ç”¨æ‰€æœ‰å¯ç”¨å­—æ®µ
            available_fields = processed_df.columns.tolist()
            logger.info(f"âœ… ä½¿ç”¨æ‰€æœ‰å¯ç”¨å­—æ®µè¿›è¡Œåˆ†æï¼Œå…± {len(available_fields)} ä¸ªå­—æ®µ")
            
            # è¯†åˆ«é—®é¢˜ç±»å‹
            question_types = analyzer.identify_all_question_types(processed_df)
            
            # ç”Ÿæˆåˆ†ææŠ¥å‘Š - ä½¿ç”¨æ–°çš„å­—æ®µçº§é¢˜å‹è¯†åˆ«ç»“æœ
            analysis_result = {
                'summary': {
                    'total_responses': len(processed_df),
                    'total_fields': len(processed_df.columns),
                    'scale_questions': len(question_types['scale_questions']),
                    'single_choice': len(question_types['single_choice']),
                    'open_ended': len(question_types['open_ended']),
                    'processing_time': datetime.now().isoformat()
                },
                'field_types': question_types['field_types'],  # æ‰€æœ‰å­—æ®µçš„é¢˜å‹ä¿¡æ¯
                'scale_questions': question_types['scale_questions'],
                'single_choice': question_types['single_choice'],
                'open_ended': question_types['open_ended']
            }
            
            # ä¸ºæ¯ä¸ªå­—æ®µæ·»åŠ æ ·æœ¬æ•°æ®
            for field_info in analysis_result['field_types']:
                field = field_info['column']
                field_data = processed_df[field]
                # è¿‡æ»¤NaNå€¼
                field_data_clean = field_data.dropna() if hasattr(field_data, 'dropna') else field_data[pd.notna(field_data)]
                
                # è·å–æ ·æœ¬æ•°æ®
                if len(field_data_clean) > 0:
                    if hasattr(field_data_clean, 'head'):
                        sample_data = field_data_clean.head(5).tolist()
                    else:
                        sample_data = field_data_clean[:5].tolist()
                    
                    field_info['sample_data'] = sample_data
                    field_info['response_count'] = len(field_data_clean)
            
            # å­˜å‚¨åˆ†æç»“æœ
            analysis_results[analysis_id]['analysis_result'] = analysis_result
            analysis_results[analysis_id]['processed_file'] = str(classification_output)
            
            logger.info(f"âœ… åˆ†æå®Œæˆï¼Œåˆ†æID: {analysis_id}")
            return jsonify({
                'results': convert_pandas_types(analysis_result),
                'analysisId': analysis_id
            })
            
        except Exception as e:
            logger.error(f"âŒ åˆ†æè¿‡ç¨‹å¤±è´¥: {e}")
            return jsonify({'error': f'åˆ†æè¿‡ç¨‹å¤±è´¥: {str(e)}'}), 500
            
    except Exception as e:
        logger.error(f"âŒ åˆ†æè¯·æ±‚å¤„ç†å¤±è´¥: {e}")
        return jsonify({'error': f'åˆ†æè¯·æ±‚å¤„ç†å¤±è´¥: {str(e)}'}), 500

# å¤„ç†Classificationåˆ†æ å¼€å§‹åˆ†æ
@app.route('/classification', methods=['POST'])
def handle_classification():
    """å¤„ç†Classificationåˆ†æ"""
    try:
        logger.info("ğŸ” æ”¶åˆ°Classificationå¤„ç†è¯·æ±‚")
        
        data = request.get_json()
        analysis_id = data.get('analysisId')
        selected_fields = data.get('selectedFields', [])
        
        if not analysis_id or analysis_id not in analysis_results:
            return jsonify({'error': 'æ— æ•ˆçš„åˆ†æID'}), 400
        
        # æ³¨æ„ï¼šé€‰æ‹©å­—æ®µæ£€æŸ¥è¢«æ³¨é‡Šæ‰æ˜¯ä¸ºäº†å…è®¸å¤„ç†æ‰€æœ‰å­—æ®µ
        # if not selected_fields:
        #     return jsonify({'error': 'è¯·é€‰æ‹©è¦åˆ†æçš„å­—æ®µ'}), 400
        
        analysis_info = analysis_results[analysis_id]
        input_file = analysis_info['file_path']
        
        logger.info(f"ğŸ“Š å¼€å§‹Classificationå¤„ç†ï¼Œæ–‡ä»¶: {input_file}")
        
        try:
            # å¯¼å…¥classificationæ¨¡å—
            try:
                from classification import QuestionnaireTranslationClassifier
                logger.info("âœ… æˆåŠŸå¯¼å…¥ QuestionnaireTranslationClassifier")
            except ImportError as e:
                logger.error(f"âŒ å¯¼å…¥ QuestionnaireTranslationClassifier å¤±è´¥: {e}")
                return jsonify({'error': f'å¯¼å…¥ classification æ¨¡å—å¤±è´¥: {str(e)}'}), 500
            
            classifier = QuestionnaireTranslationClassifier()
            
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = Path(input_file).parent / f"classification_{timestamp}.xlsx"
            
            # æ‰§è¡Œclassificationå¤„ç†
            logger.info(f"ğŸ”§ å¼€å§‹æ‰§è¡Œclassificationå¤„ç†: {input_file} -> {output_file}")
            logger.info(f"ğŸ“‹ è¾“å…¥æ–‡ä»¶å­˜åœ¨: {Path(input_file).exists()}")
            logger.info(f"ğŸ“‹ è¾“å…¥æ–‡ä»¶å¤§å°: {Path(input_file).stat().st_size if Path(input_file).exists() else 'N/A'} bytes")
            
            try:
                logger.info("ğŸš€ å³å°†è°ƒç”¨ classifier.process_table æ–¹æ³•")
                success = classifier.process_table(input_file, str(output_file))
                logger.info(f"ğŸ”§ classifier.process_table è¿”å›ç»“æœ: {success}")
                logger.info(f"ğŸ“‹ è¾“å‡ºæ–‡ä»¶å­˜åœ¨: {Path(output_file).exists()}")
                if Path(output_file).exists():
                    logger.info(f"ğŸ“‹ è¾“å‡ºæ–‡ä»¶å¤§å°: {Path(output_file).stat().st_size} bytes")
                
                if not success:
                    logger.error("âŒ classifier.process_table è¿”å› False")
                    return jsonify({'error': 'Classificationå¤„ç†å¤±è´¥ - process_tableè¿”å›False'}), 500
                    
            except Exception as process_error:
                logger.error(f"âŒ classifier.process_table æ‰§è¡Œæ—¶å‘ç”Ÿå¼‚å¸¸: {process_error}")
                logger.error(f"âŒ å¼‚å¸¸ç±»å‹: {type(process_error)}")
                import traceback
                logger.error(f"âŒ å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
                return jsonify({'error': f'Classificationå¤„ç†å¼‚å¸¸: {str(process_error)}'}), 500
            
            logger.info(f"âœ… Classificationå¤„ç†å®Œæˆ: {output_file}")
            
            # è¯»å–å¤„ç†åçš„æ–‡ä»¶
            processed_df = pd.read_excel(str(output_file))
            
            # ç­›é€‰é€‰ä¸­çš„å­—æ®µï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            available_fields = [col for col in processed_df.columns if col in selected_fields or col in [col.replace('_ç¿»è¯‘', '') for col in selected_fields]]
            if not available_fields:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„å­—æ®µï¼Œä½¿ç”¨æ‰€æœ‰å­—æ®µ
                available_fields = processed_df.columns.tolist()
            
            # ç”Ÿæˆå¤„ç†ç»“æœ
            result = {
                'summary': {
                    'total_responses': len(processed_df),
                    'processed_fields': len(available_fields),
                    'processing_time': datetime.now().isoformat(),
                    'output_file': str(output_file)
                },
                'processed_data': {},
                'field_analysis': {},
                'sample_size': min(10, len(processed_df))
            }
            
            # å‡†å¤‡å¤„ç†åçš„æ•°æ®ï¼ˆå‰10è¡Œï¼‰- è½¬æ¢ä¸ºæ•°ç»„æ ¼å¼
            result['processed_data'] = []
            
            # è¯»å–å¤„ç†åçš„æ–‡ä»¶å¹¶è¯†åˆ«é¢˜å‹
            try:
                from universal_questionnaire_analyzer import UniversalQuestionnaireAnalyzer
                logger.info("âœ… æˆåŠŸå¯¼å…¥ UniversalQuestionnaireAnalyzer")
            except ImportError as e:
                logger.error(f"âŒ å¯¼å…¥ UniversalQuestionnaireAnalyzer å¤±è´¥: {e}")
                return jsonify({'error': f'å¯¼å…¥åˆ†ææ¨¡å—å¤±è´¥: {str(e)}'}), 500
            
            analyzer = UniversalQuestionnaireAnalyzer()
            question_types = analyzer.identify_all_question_types(processed_df)
            
            # æ‰“å°é¢˜å‹è¯†åˆ«ç»“æœ
            logger.info("é¢˜å‹è¯†åˆ«ç»“æœ:")
            logger.info(f"å•é€‰é¢˜: {question_types.get('single_choice', [])}")
            logger.info(f"é‡è¡¨é¢˜: {question_types.get('scale_questions', [])}")
            logger.info(f"å¼€æ”¾é¢˜: {question_types.get('open_ended', [])}")
            
            # åˆ›å»ºå­—æ®µåˆ°ç±»å‹çš„æ˜ å°„
            field_type_map = {}
            
            # 1. å¤„ç†å•é€‰é¢˜ (type: 0)
            for q in question_types.get('single_choice', []):
                if isinstance(q, dict) and 'column' in q:
                    field_type_map[q['column']] = 0
            
            # 2. å¤„ç†é‡è¡¨é¢˜ (type: 1)
            for q in question_types.get('scale_questions', []):
                if isinstance(q, dict) and 'column' in q:
                    field_type_map[q['column']] = 1
            
            # 3. å¤„ç†å¼€æ”¾é¢˜ (type: 2)
            for q in question_types.get('open_ended', []):
                if isinstance(q, dict) and 'column' in q:
                    field_type_map[q['column']] = 2
            
            # æ–°é€»è¾‘ä¸­æ— å…¶ä»–é¢˜å‹ï¼Œæ‰€æœ‰å­—æ®µéƒ½å·²è¢«åˆ†ç±»


            
            for col in available_fields:
                if col in processed_df.columns:
                    result['processed_data'].append({
                        'field': col,
                        'values': processed_df[col].head(10).fillna('').astype(str).tolist(),
                        'type': field_type_map.get(col)  # å¦‚æœå­—æ®µæ²¡æœ‰è¢«åˆ†ç±»ï¼Œå°†è¿”å› None
                    })
            
            # ä¸ºæ¯ä¸ªå­—æ®µç”Ÿæˆè¯¦ç»†åˆ†æ
            for field in available_fields:
                if field in processed_df.columns:
                    field_data = processed_df[field]
                    field_data_clean = field_data.dropna()
                    
                    if len(field_data_clean) > 0:
                        # è·å–æ ·æœ¬æ•°æ®
                        sample_data = field_data_clean.head(10).tolist()
                        
                        # è·å–å”¯ä¸€å€¼æ•°é‡
                        unique_count = len(field_data_clean.unique())
                        
                        # å¦‚æœæ˜¯ç¿»è¯‘å­—æ®µï¼Œå°è¯•æå–ä¸»è¦ä¸»é¢˜
                        main_topics = []
                        if '_ç¿»è¯‘' in field or '_æ ‡ç­¾' in field:
                            # ä»æ•°æ®ä¸­æå–å‰5ä¸ªæœ€å¸¸è§çš„å€¼ä½œä¸ºä¸»é¢˜
                            value_counts = field_data_clean.value_counts().head(5)
                            main_topics = value_counts.index.tolist()
                        
                        result['field_analysis'][field] = {
                            'response_count': len(field_data_clean),
                            'unique_values': unique_count,
                            'sample_data': sample_data,
                            'main_topics': main_topics
                        }
            
            # å­˜å‚¨å¤„ç†ç»“æœ
            analysis_results[analysis_id]['classification_result'] = result
            analysis_results[analysis_id]['classification_output'] = str(output_file)
            
            logger.info(f"âœ… Classificationå¤„ç†å®Œæˆï¼Œåˆ†æID: {analysis_id}")
            return jsonify(convert_pandas_types(result))
            
        except Exception as e:
            logger.error(f"âŒ Classificationå¤„ç†å¤±è´¥: {e}")
            logger.error(f"âŒ å¼‚å¸¸ç±»å‹: {type(e)}")
            import traceback
            logger.error(f"âŒ å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
            return jsonify({'error': f'Classificationå¤„ç†å¤±è´¥: {str(e)}'}), 500
            
    except Exception as e:
        logger.error(f"âŒ Classificationè¯·æ±‚å¤„ç†å¤±è´¥: {e}")
        logger.error(f"âŒ å¼‚å¸¸ç±»å‹: {type(e)}")
        import traceback
        logger.error(f"âŒ å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
        return jsonify({'error': f'Classificationè¯·æ±‚å¤„ç†å¤±è´¥: {str(e)}'}), 500

@app.route('/test/analysis-results/<analysis_id>', methods=['GET'])
def get_analysis_results(analysis_id):
    """è·å–åˆ†æç»“æœ"""
    try:
        if analysis_id not in analysis_results:
            return jsonify({'error': 'åˆ†æIDä¸å­˜åœ¨'}), 404
        
        analysis_info = analysis_results[analysis_id]
        result = analysis_info.get('analysis_result')
        
        if not result:
            return jsonify({'error': 'åˆ†æç»“æœä¸å­˜åœ¨'}), 404
        
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"âŒ è·å–åˆ†æç»“æœå¤±è´¥: {e}")
        return jsonify({'error': f'è·å–åˆ†æç»“æœå¤±è´¥: {str(e)}'}), 500

# ç»Ÿè®¡åˆ†æ
@app.route('/statistics', methods=['POST'])
def statistics():
    """ç»Ÿè®¡åˆ†ææ¥å£ - ç‹¬ç«‹è¿›è¡Œç»Ÿè®¡åˆ†æ"""
    try:
        logger.info("ğŸ“Š æ”¶åˆ°ç»Ÿè®¡åˆ†æè¯·æ±‚")
        
        data = request.get_json()
        analysis_id = data.get('analysisId')
        selected_fields = data.get('selectedFields', [])
        question_types = data.get('questionTypes', {})
        
        if not analysis_id or analysis_id not in analysis_results:
            return jsonify({'error': 'æ— æ•ˆçš„åˆ†æID'}), 400
        
        if not selected_fields:
            return jsonify({'error': 'è¯·é€‰æ‹©è¦åˆ†æçš„å­—æ®µ'}), 400
        
        analysis_info = analysis_results[analysis_id]
        df = analysis_info['dataframe']
        
        logger.info(f"ğŸ“ˆ å¼€å§‹ç»Ÿè®¡åˆ†æ {len(selected_fields)} ä¸ªå­—æ®µ")
        
        # å¯¼å…¥åˆ†ææ¨¡å—
        try:
            from universal_questionnaire_analyzer import UniversalQuestionnaireAnalyzer
            logger.info("âœ… æˆåŠŸå¯¼å…¥ UniversalQuestionnaireAnalyzer")
        except ImportError as e:
            logger.error(f"âŒ å¯¼å…¥ UniversalQuestionnaireAnalyzer å¤±è´¥: {e}")
            return jsonify({'error': f'å¯¼å…¥åˆ†ææ¨¡å—å¤±è´¥: {str(e)}'}), 500
        
        analyzer = UniversalQuestionnaireAnalyzer()
        
        # ç­›é€‰é€‰ä¸­çš„å­—æ®µ
        available_fields = [col for col in selected_fields if col in df.columns]
        if not available_fields:
            return jsonify({'error': 'é€‰ä¸­çš„å­—æ®µåœ¨æ•°æ®ä¸­ä¸å­˜åœ¨'}), 400
        
        filtered_df = df[available_fields]
        
        # é‡æ–°è¯†åˆ«é—®é¢˜ç±»å‹ï¼ˆåŸºäºé€‰ä¸­çš„å­—æ®µï¼‰
        field_question_types = analyzer.identify_all_question_types(filtered_df)
        
        # ç”Ÿæˆç»Ÿè®¡åˆ†æç»“æœ
        analysis_result = {
            'summary': {
                'totalFields': len(available_fields),
                'analyzedQuestions': len(available_fields),
                'totalResponses': len(filtered_df)
            },
            'scaleQuestions': [],
            'singleChoiceQuestions': [],
                        'multipleChoiceQuestions': [],
            'openEndedQuestions': [],
            'crossAnalysis': None
        }
        
        # å¤„ç†é‡è¡¨é¢˜
        scale_questions = field_question_types.get('scale_questions', [])
        for q in scale_questions:
            col = q['column']
            if col in filtered_df.columns:
                data = filtered_df[col].dropna()
                if len(data) > 0:
                    # è½¬æ¢ä¸ºæ•°å€¼
                    numeric_data = pd.to_numeric(data, errors='coerce').dropna()
                    if len(numeric_data) > 0:
                        # åŸºç¡€ç»Ÿè®¡
                        stats = {
                            'count': len(numeric_data),
                            'mean': float(numeric_data.mean()),
                            'std': float(numeric_data.std()),
                            'min': float(numeric_data.min()),
                            'max': float(numeric_data.max()),
                            'median': float(numeric_data.median())
                        }
                        
                        # åˆ†å¸ƒç»Ÿè®¡
                        distribution = {}
                        value_counts = numeric_data.value_counts().sort_index()
                        total = len(numeric_data)
                        for score, count in value_counts.items():
                            distribution[str(int(score))] = {
                                'count': int(count),
                                'percentage': float(count / total * 100)
                            }
                        
                        # NPSåˆ†æï¼ˆé€‚ç”¨äº1-10åˆ†åˆ¶ï¼‰
                        nps_analysis = None
                        if stats['max'] <= 10 and stats['min'] >= 1:
                            promoters = len(numeric_data[numeric_data >= 9])
                            passives = len(numeric_data[(numeric_data >= 7) & (numeric_data <= 8)])
                            detractors = len(numeric_data[numeric_data <= 6])
                            
                            nps_score = (promoters - detractors) / total * 100
                            
                            nps_analysis = {
                                'promoters': {'count': promoters, 'percentage': promoters / total * 100},
                                'passives': {'count': passives, 'percentage': passives / total * 100},
                                'detractors': {'count': detractors, 'percentage': detractors / total * 100},
                                'nps': nps_score,
                                'evaluation': 'ä¼˜ç§€' if nps_score > 50 else 'è‰¯å¥½' if nps_score > 0 else 'éœ€æ”¹è¿›'
                            }
                        
                        analysis_result['scaleQuestions'].append({
                            'column': col,
                            'statistics': stats,
                            'distribution': distribution,
                            'npsAnalysis': nps_analysis
                        })
        
        # å¤„ç†å•é€‰é¢˜
        single_choice = field_question_types.get('single_choice', [])
        for q in single_choice:
            col = q['column']
            if col in filtered_df.columns:
                data = filtered_df[col].dropna()
                if len(data) > 0:
                    value_counts = data.value_counts()
                    total = len(data)
                    
                    options = []
                    for option, count in value_counts.items():
                        options.append({
                            'option': str(option),
                            'count': int(count),
                            'percentage': float(count / total * 100)
                        })
                    
                    most_selected = options[0] if options else None
                    
                    analysis_result['singleChoiceQuestions'].append({
                        'column': col,
                        'validResponses': total,
                        'totalOptions': len(options),
                        'options': options,
                        'mostSelected': most_selected
                    })
        
        # å¤„ç†å¼€æ”¾é¢˜
        open_ended = field_question_types.get('open_ended', [])
        for q in open_ended:
            col = q['column']
            if col in filtered_df.columns:
                data = filtered_df[col].dropna()
                if len(data) > 0:
                    # åŸºç¡€ç»Ÿè®¡
                    avg_length = data.astype(str).str.len().mean()
                    unique_count = len(data.unique())
                    uniqueness_ratio = unique_count / len(data)
                    
                    # æå–å…³é”®è¯ï¼ˆç®€å•çš„è¯é¢‘ç»Ÿè®¡ï¼‰
                    text_data = ' '.join(data.astype(str))
                    words = text_data.split()
                    word_freq = pd.Series(words).value_counts().head(10)
                    
                    top_keywords = []
                    for word, count in word_freq.items():
                        if len(word) > 1:  # è¿‡æ»¤å•å­—ç¬¦
                            top_keywords.append({'word': word, 'count': int(count)})
                    
                    # ç¤ºä¾‹å›ç­”
                    sample_responses = data.head(5).tolist()
                    
                    analysis_result['openEndedQuestions'].append({
                        'column': col,
                        'validResponses': len(data),
                        'statistics': {
                            'averageLength': avg_length,
                            'uniqueCount': unique_count,
                            'uniquenessRatio': uniqueness_ratio
                        },
                        'topKeywords': top_keywords[:8],
                        'sampleResponses': sample_responses
                    })
        

        # å¤„ç†å¤šé€‰é¢˜
        multiple_choice = field_question_types.get('multiple_choice', {})
        for question_stem, options in multiple_choice.items():
            # æå–æ‰€æœ‰é€‰é¡¹çš„åˆ—å
            option_columns = [opt['full_column'] for opt in options if opt['full_column'] in filtered_df.columns]
            
            if option_columns:
                # åˆ›å»ºå¤šé€‰é¢˜åˆ†æç»“æœ
                multi_question = {
                    'column': question_stem,  # ä¸å•é€‰é¢˜ä¿æŒä¸€è‡´ï¼Œä½¿ç”¨columnè€Œä¸æ˜¯questionStem
                    'totalOptions': len(option_columns),
                    'validResponses': len(filtered_df),  # ä¸å•é€‰é¢˜ä¿æŒä¸€è‡´ï¼Œä½¿ç”¨validResponsesè€Œä¸æ˜¯totalResponses
                    'options': []
                }
                
                # è®¡ç®—æ€»å“åº”æ•°ï¼ˆè‡³å°‘é€‰æ‹©äº†ä¸€ä¸ªé€‰é¡¹çš„è¡Œæ•°ï¼‰
                respondents = 0
                for i, row in filtered_df[option_columns].iterrows():
                    if row.notna().any():
                        respondents += 1
                
                multi_question['validResponses'] = respondents if respondents > 0 else len(filtered_df)
                
                # å¤„ç†æ¯ä¸ªé€‰é¡¹
                for opt_info in options:
                    col = opt_info['full_column']
                    if col in filtered_df.columns:
                        # è·å–é€‰é¡¹æ–‡æœ¬
                        option_text = opt_info['option']
                        
                        # ç»Ÿè®¡é€‰æ‹©è¯¥é€‰é¡¹çš„äººæ•°
                        try:
                            # å°è¯•å°†åˆ—è½¬æ¢ä¸ºå¸ƒå°”å€¼ï¼ˆé’ˆå¯¹0/1ç¼–ç çš„å¤šé€‰é¢˜ï¼‰
                            data = filtered_df[col].fillna(0).astype(int).astype(bool)
                            selected_count = data.sum()
                        except:
                            # å¦‚æœå¤±è´¥ï¼Œåˆ™æŒ‰éç©ºå€¼è®¡æ•°
                            selected_count = filtered_df[col].notna().sum()
                        
                        # è®¡ç®—ç™¾åˆ†æ¯”
                        percentage = (selected_count / multi_question['validResponses'] * 100) if multi_question['validResponses'] > 0 else 0
                        
                        # æ·»åŠ é€‰é¡¹åˆ†æ - ä¸å•é€‰é¢˜ä¿æŒä¸€è‡´çš„æ ¼å¼
                        multi_question['options'].append({
                            'option': option_text,
                            'count': int(selected_count),
                            'percentage': float(percentage)
                        })
                
                # æ·»åŠ æœ€å¤šé€‰æ‹©çš„é€‰é¡¹ - ä¸å•é€‰é¢˜ä¿æŒä¸€è‡´
                if multi_question['options']:
                    # æ’åºé€‰é¡¹ï¼ˆæŒ‰é€‰æ‹©æ•°é‡é™åºï¼‰
                    sorted_options = sorted(multi_question['options'], key=lambda x: x['count'], reverse=True)
                    multi_question['mostSelected'] = sorted_options[0]
                    
                    # ä¿ç•™summaryå­—æ®µï¼Œä½†ç¡®ä¿æ ¼å¼ä¸€è‡´
                    multi_question['summary'] = {
                        'mostSelected': sorted_options[0],
                        'leastSelected': sorted_options[-1],
                        'averageSelectionRate': sum(opt['percentage'] for opt in multi_question['options']) / len(multi_question['options'])
                    }
                
                # æ·»åŠ åˆ°ç»“æœé›†
                analysis_result['multipleChoiceQuestions'].append(multi_question)
        
        # å­˜å‚¨åˆ†æç»“æœ
        analysis_results[analysis_id]['statistics_result'] = analysis_result
        
        logger.info(f"âœ… ç»Ÿè®¡åˆ†æå®Œæˆï¼Œåˆ†æID: {analysis_id}")
        return jsonify({
            'results': convert_pandas_types(analysis_result),
            'analysisId': analysis_id
        })
        
    except Exception as e:
        logger.error(f"âŒ ç»Ÿè®¡åˆ†æå¤±è´¥: {e}")
        logger.error(traceback.format_exc())
        return jsonify({'error': f'ç»Ÿè®¡åˆ†æå¤±è´¥: {str(e)}'}), 500

@app.route('/test/export/<analysis_id>', methods=['GET'])
def export_results(analysis_id):
    """å¯¼å‡ºåˆ†æç»“æœ"""
    try:
        format_type = request.args.get('format', 'csv')
        
        if analysis_id not in analysis_results:
            return jsonify({'error': 'åˆ†æIDä¸å­˜åœ¨'}), 404
        
        analysis_info = analysis_results[analysis_id]
        processed_file = analysis_info.get('processed_file')
        
        if not processed_file or not os.path.exists(processed_file):
            return jsonify({'error': 'å¤„ç†åçš„æ–‡ä»¶ä¸å­˜åœ¨'}), 404
        
        # è¿”å›å¤„ç†åçš„æ–‡ä»¶
        return send_file(
            processed_file,
            as_attachment=True,
            download_name=f"analysis_{analysis_id}.{format_type}"
        )
        
    except Exception as e:
        logger.error(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")
        return jsonify({'error': f'å¯¼å‡ºå¤±è´¥: {str(e)}'}), 500

@app.route('/download-classification/<analysis_id>', methods=['GET'])
def download_classification(analysis_id):
    """ä¸‹è½½Classificationå¤„ç†åçš„æ–‡ä»¶"""
    try:
        if analysis_id not in analysis_results:
            return jsonify({'error': 'åˆ†æIDä¸å­˜åœ¨'}), 404
        
        analysis_info = analysis_results[analysis_id]
        classification_output = analysis_info.get('classification_output')
        
        if not classification_output or not os.path.exists(classification_output):
            return jsonify({'error': 'Classificationå¤„ç†ç»“æœæ–‡ä»¶ä¸å­˜åœ¨'}), 404
        
        return send_file(
            classification_output,
            as_attachment=True,
            download_name=f"classification_result_{analysis_id}.xlsx",
            mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
        )
        
    except Exception as e:
        logger.error(f"âŒ ä¸‹è½½Classificationç»“æœå¤±è´¥: {e}")
        return jsonify({'error': f'ä¸‹è½½Classificationç»“æœå¤±è´¥: {str(e)}'}), 500

@app.route('/test/analysis-history', methods=['GET'])
def get_analysis_history():
    """è·å–åˆ†æå†å²"""
    try:
        history = []
        for analysis_id, info in analysis_results.items():
            history.append({
                'analysisId': analysis_id,
                'filename': info.get('filename', 'Unknown'),
                'uploadTime': info.get('upload_time', ''),
                'hasResult': 'analysis_result' in info
            })
        
        return jsonify(history)
        
    except Exception as e:
        logger.error(f"âŒ è·å–åˆ†æå†å²å¤±è´¥: {e}")
        return jsonify({'error': f'è·å–åˆ†æå†å²å¤±è´¥: {str(e)}'}), 500

@app.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return jsonify({
        'status': 'healthy',
        'timestamp': datetime.now().isoformat(),
        'service': 'questionnaire-analysis-api'
    })

if __name__ == '__main__':
    logger.info("ğŸš€ é—®å·åˆ†æFlaskæœåŠ¡å¯åŠ¨")
    logger.info(f"ğŸ“ ä¸Šä¼ æ–‡ä»¶å¤¹: {UPLOAD_FOLDER}")
    logger.info("ğŸŒ æœåŠ¡åœ°å€: http://localhost:9001")
    
    # å¯åŠ¨Flaskåº”ç”¨
    app.run(
        host='0.0.0.0',
        port=9001,
        debug=False  # å…³é—­debugæ¨¡å¼é¿å…watchdogé—®é¢˜
    ) 