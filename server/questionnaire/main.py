#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é—®å·åˆ†æFlaskåç«¯æœåŠ¡
æä¾›ä¸å‰ç«¯QuestionnaireAnalysis.jså…¼å®¹çš„APIæ¥å£
"""

import os
import sys
import logging
import time
import json
import uuid
import re
import glob
from pathlib import Path
from datetime import datetime
from flask import Flask, request, jsonify, send_file
from flask_cors import CORS
from werkzeug.utils import secure_filename
import pandas as pd
import numpy as np
import traceback

# æ•°æ®åº“è¿æ¥ç›¸å…³å¯¼å…¥
try:
    import pymysql
    pymysql.install_as_MySQLdb()
    DB_AVAILABLE = True
    logger_db = logging.getLogger('database')
except ImportError:
    DB_AVAILABLE = False
    print("âš ï¸ pymysqlåº“æœªå®‰è£…ï¼Œæ•°æ®åº“åŠŸèƒ½å°†è¢«ç¦ç”¨")

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("api_processing.log", encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger()

# åˆ›å»ºFlaskåº”ç”¨
app = Flask(__name__)
CORS(app)  # å¯ç”¨è·¨åŸŸæ”¯æŒ

# é…ç½®ä¸Šä¼ æ–‡ä»¶å¤¹åŠå­ç›®å½•
UPLOAD_FOLDER = Path(__file__).parent / 'uploads'
QUESTIONNAIRE_FOLDER = UPLOAD_FOLDER / 'questionnaire'  # åŸå§‹é—®å·æ–‡ä»¶
# CLASSIFICATION_FOLDER = UPLOAD_FOLDER / 'classification'  # åˆ†ç±»å¤„ç†åçš„æ–‡ä»¶ï¼ˆæ—§çš„æ··åˆæµç¨‹ï¼‰
# RETAG_FOLDER = UPLOAD_FOLDER / 'retag_result'  # é‡æ–°æ‰“æ ‡åçš„æ–‡ä»¶ï¼ˆæ—§ï¼‰

# æ–°çš„åˆ†ç¦»ç›®å½•ç»“æ„
TRANSLATE_FOLDER = UPLOAD_FOLDER / 'translate'  # ç¿»è¯‘ç»“æœ
TRANSLATE_AI_FOLDER = UPLOAD_FOLDER / 'translate_ai'  # æ ‡å‡†AIæ‰“æ ‡ç»“æœ
TRANSLATE_AI_MANUAL_FOLDER = UPLOAD_FOLDER / 'translate_ai_manual'  # æ ‡å‡†AIæ‰“æ ‡æ‰‹åŠ¨ç¼–è¾‘ç»“æœ
TRANSLATE_CUSTOM_FOLDER = UPLOAD_FOLDER / 'translate_custom'  # é…ç½®å‚è€ƒæ ‡ç­¾æ‰“æ ‡ç»“æœ
TRANSLATE_CUSTOM_MANUAL_FOLDER = UPLOAD_FOLDER / 'translate_custom_manual'  # é…ç½®å‚è€ƒæ ‡ç­¾æ‰“æ ‡æ‰‹åŠ¨ç¼–è¾‘ç»“æœ

# åˆ›å»ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
UPLOAD_FOLDER.mkdir(exist_ok=True)
QUESTIONNAIRE_FOLDER.mkdir(exist_ok=True)
# CLASSIFICATION    _FOLDER.mkdir(exist_ok=True)
# RETAG_FOLDER.mkdir(exist_ok=True)
TRANSLATE_FOLDER.mkdir(exist_ok=True)
TRANSLATE_AI_FOLDER.mkdir(exist_ok=True)
TRANSLATE_AI_MANUAL_FOLDER.mkdir(exist_ok=True)
TRANSLATE_CUSTOM_FOLDER.mkdir(exist_ok=True)
TRANSLATE_CUSTOM_MANUAL_FOLDER.mkdir(exist_ok=True)

# é…ç½®å…è®¸çš„æ–‡ä»¶ç±»å‹
ALLOWED_EXTENSIONS = {'csv', 'xlsx', 'xls', 'txt'}

# å­˜å‚¨åˆ†æç»“æœçš„ä¸´æ—¶æ•°æ®ç»“æ„
analysis_results = {}

# æ•°æ®åº“è¿æ¥é…ç½®
DB_CONFIG = {
    'host': os.getenv('TIDB_HOST') or '10.51.32.12',
    'port': int(os.getenv('TIDB_PORT') or 4000),
    'user': os.getenv('TIDB_USER') or 'root',
    'password': os.getenv('TIDB_PASSWORD') or 'Z8OiMjawmSSqgRFs',
    'database': 'mkt',
    'charset': 'utf8mb4',
    'ssl_disabled': False
}

def get_database_connection():
    """è·å–æ•°æ®åº“è¿æ¥"""
    if not DB_AVAILABLE:
        return None, "æ•°æ®åº“åŠŸèƒ½æœªå¯ç”¨ï¼Œè¯·å®‰è£…pymysql"
    
    # æ£€æŸ¥å¿…è¦çš„é…ç½®ä¿¡æ¯
    if not DB_CONFIG['user'] or not DB_CONFIG['password']:
        return None, f"æ•°æ®åº“è¿æ¥é…ç½®ä¸å®Œæ•´: user={DB_CONFIG['user']}, password={'*' * len(DB_CONFIG['password']) if DB_CONFIG['password'] else 'None'}"
    
    try:
        # æ‰“å°è¿æ¥ä¿¡æ¯ï¼ˆéšè—å¯†ç ï¼‰
        logger.info(f"ğŸ”— å°è¯•è¿æ¥æ•°æ®åº“: {DB_CONFIG['user']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}")
        connection = pymysql.connect(**DB_CONFIG)
        logger.info("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
        return connection, None
    except Exception as e:
        error_msg = f"æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}"
        logger.error(f"âŒ {error_msg}")
        return None, error_msg

def test_database_connection():
    """æµ‹è¯•æ•°æ®åº“è¿æ¥"""
    connection, error = get_database_connection()
    if error:
        return False, error
    
    try:
        cursor = connection.cursor()
        cursor.execute("SELECT 1")
        cursor.close()
        connection.close()
        return True, "æ•°æ®åº“è¿æ¥æµ‹è¯•æˆåŠŸ"
    except Exception as e:
        return False, f"æ•°æ®åº“è¿æ¥æµ‹è¯•å¤±è´¥: {str(e)}"

# æ•°æ®å¯¼å…¥ç›¸å…³å·¥å…·å‡½æ•°
def extract_question_code(question_text):
    """ä»é—®é¢˜æ–‡æœ¬ä¸­æå–ç¼–å·"""
    match = re.match(r'^(Q\d+)', question_text)
    return match.group(1) if match else ''

def extract_survey_name_from_filename(filename):
    """ä»æ–‡ä»¶åæå–é—®å·åç§°"""
    # ç¤ºä¾‹: "ç®€å•é—®å·3_retag_manual_20250716_152339.xlsx"
    match = re.match(r'^(.+?)_retag_manual_', filename)
    if match:
        return match.group(1)
    return filename.replace('.xlsx', '')

def determine_question_type(question_text):
    """åˆ¤æ–­é—®é¢˜ç±»å‹"""
    question_lower = question_text.lower()
    
    if 'scale' in question_lower or 'rate' in question_lower or 'æ‰“åˆ†' in question_text:
        return 'scale'
    elif 'choice' in question_lower or 'é€‰æ‹©' in question_text:
        return 'single'
    else:
        return 'open'

def safe_get_value(row, column_name):
    """å®‰å…¨åœ°ä»è¡Œä¸­è·å–å€¼"""
    if column_name in row and pd.notna(row[column_name]):
        value = str(row[column_name]).strip()
        return value if value else ''
    return ''

def parse_column_structure(df):
    """è§£æExcelåˆ—ç»“æ„ï¼Œè¯†åˆ«é—®é¢˜åˆ—"""
    column_info = {
        'question_columns': [],      # åŸºç¡€é—®é¢˜åˆ—
        'translation_columns': [],   # ç¿»è¯‘åˆ—(-CN)
        'label_columns': [],        # æ ‡ç­¾åˆ—(æ ‡ç­¾)
        'category_columns': [],     # åˆ†ç±»åˆ—(ä¸€çº§ä¸»é¢˜ã€äºŒçº§ä¸»é¢˜)
        'other_columns': []         # å…¶ä»–åˆ—(å¦‚ID)
    }
    
    for col in df.columns:
        if col == 'ID':
            column_info['other_columns'].append(col)
        elif col.endswith('-CN'):
            column_info['translation_columns'].append(col)
        elif col.endswith('æ ‡ç­¾'):
            column_info['label_columns'].append(col)
        elif col.endswith(('ä¸€çº§ä¸»é¢˜', 'äºŒçº§ä¸»é¢˜')):
            column_info['category_columns'].append(col)
        elif not col.endswith(('-CN', 'æ ‡ç­¾', 'ä¸€çº§ä¸»é¢˜', 'äºŒçº§ä¸»é¢˜')):
            column_info['question_columns'].append(col)
    
    logger.info(f"ğŸ” è¯†åˆ«åˆ° {len(column_info['question_columns'])} ä¸ªé—®é¢˜åˆ—")
    return column_info

def transform_data_to_records(df, column_info, analysis_id, survey_name, survey_topic=''):
    """å°†DataFrameè½¬æ¢ä¸ºæ•°æ®åº“è®°å½•æ ¼å¼"""
    records = []
    question_columns = column_info['question_columns']
    
    for index, row in df.iterrows():
        # è·å–å›ç­”è€…ID
        respondent_id = safe_get_value(row, 'ID')
        
        for question_col in question_columns:
            # æ„å»ºç›¸å…³åˆ—å
            translation_col = f"{question_col}-CN"
            label_col = f"{question_col}æ ‡ç­¾"
            primary_category_col = f"{question_col}ä¸€çº§ä¸»é¢˜"
            secondary_category_col = f"{question_col}äºŒçº§ä¸»é¢˜"
            
            # æå–æ•°æ®
            record = {
                'analysis_id': analysis_id,
                'respondent_id': respondent_id,
                'survey_name': survey_name,
                'survey_topic': survey_topic,
                'question_code': extract_question_code(question_col),
                'question': question_col,
                'question_type': determine_question_type(question_col),
                'respondent_row': index + 1,
                'user_answer': safe_get_value(row, question_col),
                'translation': safe_get_value(row, translation_col),
                'labels': safe_get_value(row, label_col),
                'primary_category': safe_get_value(row, primary_category_col),
                'secondary_category': safe_get_value(row, secondary_category_col)
            }
            
            records.append(record)
    
    return records

def validate_records(records):
    """éªŒè¯è®°å½•çš„å®Œæ•´æ€§å’Œåˆç†æ€§"""
    validation_result = {
        'valid': True,
        'warnings': [],
        'errors': [],
        'statistics': {}
    }
    
    if not records:
        validation_result['valid'] = False
        validation_result['errors'].append('æ²¡æœ‰å¯å¯¼å…¥çš„è®°å½•')
        return validation_result
    
    # æ£€æŸ¥å¿…å¡«å­—æ®µ
    required_fields = ['analysis_id', 'survey_name', 'question', 'respondent_row']
    for i, record in enumerate(records):
        for field in required_fields:
            if not record.get(field):
                validation_result['errors'].append(f"è®°å½• {i+1}: ç¼ºå°‘å¿…å¡«å­—æ®µ '{field}'")
    
    # ç»Ÿè®¡ä¿¡æ¯
    validation_result['statistics'] = {
        'total_records': len(records),
        'unique_respondents': len(set(r['respondent_id'] for r in records if r['respondent_id'])),
        'unique_questions': len(set(r['question_code'] for r in records if r['question_code'])),
        'records_with_labels': len([r for r in records if r.get('labels')])
    }
    
    # è®¾ç½®éªŒè¯ç»“æœ
    if validation_result['errors']:
        validation_result['valid'] = False
    
    return validation_result

def convert_pandas_types(obj):
    """é€’å½’è½¬æ¢pandaså’Œnumpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹ï¼Œç”¨äºJSONåºåˆ—åŒ–"""
    if isinstance(obj, dict):
        return {k: convert_pandas_types(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_pandas_types(item) for item in obj]
    elif isinstance(obj, (np.int64, np.int32, np.int16, np.int8)):
        return int(obj)
    elif isinstance(obj, (np.float64, np.float32, np.float16)):
        return float(obj)
    elif isinstance(obj, np.bool_):
        return bool(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif pd.isna(obj):
        return None
    else:
        return obj

def allowed_file(filename):
    """æ£€æŸ¥æ–‡ä»¶ç±»å‹æ˜¯å¦å…è®¸"""
    return '.' in filename and \
           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def get_file_extension(filename):
    """è·å–æ–‡ä»¶æ‰©å±•å"""
    return filename.rsplit('.', 1)[1].lower() if '.' in filename else ''

def secure_chinese_filename(filename):
    """å®‰å…¨åœ°å¤„ç†ä¸­æ–‡æ–‡ä»¶å"""
    # ç§»é™¤è·¯å¾„åˆ†éš”ç¬¦å’Œå…¶ä»–ä¸å®‰å…¨å­—ç¬¦
    filename = filename.replace('/', '_').replace('\\', '_')
    filename = filename.replace('..', '_')
    # ä¿ç•™æ–‡ä»¶åä¸­çš„ä¸­æ–‡å­—ç¬¦
    return filename

def extract_file_info(file_path):
    """
    ä»æ–‡ä»¶è·¯å¾„ä¸­æå–åŸºç¡€æ–‡ä»¶åå’Œæ—¶é—´æˆ³
    æ”¯æŒå¤šç§æ ¼å¼ï¼Œèƒ½å¤Ÿæ­£ç¡®å¤„ç†å¤æ‚çš„æ–‡ä»¶å‘½åï¼š
    - åŸå§‹æ–‡ä»¶ï¼šç®€å•é—®å·3_20250714_180649.xlsx -> ('ç®€å•é—®å·3', '20250714_180649')
    - ç¿»è¯‘æ–‡ä»¶ï¼šç®€å•é—®å·3_translate_20250714_180649.xlsx -> ('ç®€å•é—®å·3', '20250714_180649')
    - æ ‡å‡†æ‰“æ ‡ï¼šç®€å•é—®å·3_translate_ai_20250714_180649.xlsx -> ('ç®€å•é—®å·3', '20250714_180649')
    - é…ç½®æ‰“æ ‡ï¼šç®€å•é—®å·3_translate_custom_20250714_180649.xlsx -> ('ç®€å•é—®å·3', '20250714_180649')
    - æ‰‹åŠ¨ç¼–è¾‘ï¼šç®€å•é—®å·3_translate_ai_manual_20250714_180649.xlsx -> ('ç®€å•é—®å·3', '20250714_180649')
    """
    filename = Path(file_path).stem  # ä¸å«æ‰©å±•åçš„æ–‡ä»¶å
    
    import re
    
    # å®šä¹‰å¤„ç†ç±»å‹æ ‡è¯†ç¬¦ï¼ŒæŒ‰é•¿åº¦æ’åºï¼ˆé•¿æ¨¡å¼ä¼˜å…ˆåŒ¹é…ï¼‰
    processing_types = [
        'translate_ai',     # æ ‡å‡†AIæ‰“æ ‡ï¼ˆå®Œæ•´ç‰ˆï¼‰ - å¿…é¡»åœ¨aiä¹‹å‰
        'translate_custom', # é…ç½®å‚è€ƒæ ‡ç­¾æ‰“æ ‡ - å¿…é¡»åœ¨customä¹‹å‰  
        'manual',           # æ‰‹åŠ¨ç¼–è¾‘ï¼ˆæœ€åä¸€æ­¥ï¼‰
        'translate',        # ç¿»è¯‘å¤„ç† - å¿…é¡»åœ¨aiå’Œcustomä¹‹å‰
        'custom',           # é…ç½®æ‰“æ ‡ï¼ˆç®€åŒ–ç‰ˆï¼‰
        'ai',               # æ ‡å‡†AIæ‰“æ ‡ï¼ˆç®€åŒ–ç‰ˆï¼‰ - å¿…é¡»åœ¨æœ€å
    ]
    
    # å°è¯•åŒ¹é…å¤æ‚æ–‡ä»¶æ ¼å¼ï¼šåŸºç¡€åç§°_[å¤„ç†ç±»å‹]_æ—¶é—´æˆ³
    # æ„å»ºåŠ¨æ€æ­£åˆ™è¡¨è¾¾å¼
    types_pattern = '|'.join(processing_types)
    
    # åŒ¹é…æ ¼å¼ï¼šåŸºç¡€åç§°_å¤„ç†ç±»å‹1_å¤„ç†ç±»å‹2_..._æ—¶é—´æˆ³
    pattern = rf'^(.+)_(?:{types_pattern})(?:_(?:{types_pattern}))*_(\d{{8}}_\d{{6}})$'
    match = re.match(pattern, filename)
    if match:
        base_name = match.group(1)  # åŸºç¡€æ–‡ä»¶å
        timestamp = match.group(2)  # æ—¶é—´æˆ³
        logger.debug(f"ğŸ” å¤æ‚æ ¼å¼è§£æ: {filename} -> åŸºç¡€åç§°: {base_name}, æ—¶é—´æˆ³: {timestamp}")
        return base_name, timestamp
    
    # å°è¯•åŒ¹é…ç®€å•æ–‡ä»¶æ ¼å¼ï¼šåŸºç¡€åç§°_æ—¶é—´æˆ³
    match = re.match(r'^(.+)_(\d{8}_\d{6})$', filename)
    if match:
        base_name = match.group(1)  # åŸºç¡€æ–‡ä»¶å
        timestamp = match.group(2)  # æ—¶é—´æˆ³
        logger.debug(f"ğŸ” ç®€å•æ ¼å¼è§£æ: {filename} -> åŸºç¡€åç§°: {base_name}, æ—¶é—´æˆ³: {timestamp}")
        return base_name, timestamp
    
    # å¦‚æœä¸åŒ¹é…ä»»ä½•é¢„æœŸæ ¼å¼ï¼Œè¿”å›å®Œæ•´æ–‡ä»¶åå’Œå½“å‰æ—¶é—´æˆ³
    current_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    logger.warning(f"âš ï¸ æ— æ³•è§£ææ–‡ä»¶åæ ¼å¼: {filename}ï¼Œä½¿ç”¨å®Œæ•´æ–‡ä»¶åä½œä¸ºåŸºç¡€åç§°")
    return filename, current_timestamp

@app.route('/upload-questionnaire', methods=['POST'])
def upload_questionnaire():
    """ä¸Šä¼ é—®å·æ•°æ®æ–‡ä»¶"""
    try:
        logger.info("ğŸ“¤ æ”¶åˆ°æ–‡ä»¶ä¸Šä¼ è¯·æ±‚")
        
        if 'file' not in request.files:
            return jsonify({'error': 'æ²¡æœ‰æ–‡ä»¶è¢«ä¸Šä¼ '}), 400
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({'error': 'æ²¡æœ‰é€‰æ‹©æ–‡ä»¶'}), 400
        
        if not allowed_file(file.filename):
            return jsonify({'error': 'ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œè¯·ä¸Šä¼ CSVã€Excelæˆ–TXTæ–‡ä»¶'}), 400
        
        # å®‰å…¨åœ°ä¿å­˜æ–‡ä»¶åˆ°questionnaireå­ç›®å½•ï¼Œä¿ç•™ä¸­æ–‡å­—ç¬¦
        original_filename = secure_chinese_filename(file.filename or 'unknown_file')
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # æå–æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰å’Œæ‰©å±•å
        file_stem = Path(original_filename).stem  # æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
        file_extension = Path(original_filename).suffix  # æ‰©å±•å
        
        # æ–°çš„å‘½åæ ¼å¼ï¼šåŸå§‹æ–‡ä»¶å_æ—¶é—´æˆ³.æ‰©å±•å
        unique_filename = f"{file_stem}_{timestamp}{file_extension}"
        file_path = QUESTIONNAIRE_FOLDER / unique_filename
        
        file.save(str(file_path))
        logger.info(f"âœ… åŸå§‹é—®å·æ–‡ä»¶ä¿å­˜æˆåŠŸ: {file_path}")
        
        # ç”Ÿæˆåˆ†æID
        analysis_id = str(uuid.uuid4())
        
        # è¯»å–æ–‡ä»¶å†…å®¹å¹¶åˆ†æå­—æ®µ
        try:
            try:
                from universal_questionnaire_analyzer import UniversalQuestionnaireAnalyzer
                logger.info("âœ… æˆåŠŸå¯¼å…¥ UniversalQuestionnaireAnalyzer")
            except ImportError as e:
                logger.error(f"âŒ å¯¼å…¥ UniversalQuestionnaireAnalyzer å¤±è´¥: {e}")
                return jsonify({'error': f'å¯¼å…¥åˆ†ææ¨¡å—å¤±è´¥: {str(e)}'}), 500
            
            analyzer = UniversalQuestionnaireAnalyzer()
            df = analyzer.read_data_file(str(file_path))
            
            if df is None:
                return jsonify({'error': 'æ— æ³•è¯»å–æ–‡ä»¶å†…å®¹'}), 400
            
            # è¯†åˆ«é—®é¢˜ç±»å‹
            logger.info(f"ğŸ” å¼€å§‹è¯†åˆ«é¢˜å‹ï¼Œå…±{len(df.columns)}ä¸ªå­—æ®µ")
            logger.info(f"ğŸ“‹ å­—æ®µåˆ—è¡¨: {df.columns.tolist()}")
            
            question_types = analyzer.identify_all_question_types(df)
            
            # è¾“å‡ºè¯†åˆ«ç»“æœç”¨äºè°ƒè¯•
            logger.info(f"ğŸ“Š é¢˜å‹è¯†åˆ«å®Œæˆ:")
            logger.info(f"  - é‡è¡¨é¢˜: {len(question_types.get('scale_questions', []))} ä¸ª")
            logger.info(f"  - å•é€‰é¢˜: {len(question_types.get('single_choice', []))} ä¸ª")
            logger.info(f"  - å¼€æ”¾é¢˜: {len(question_types.get('open_ended', []))} ä¸ª")
            
            # å­˜å‚¨åˆ†æä¿¡æ¯
            analysis_results[analysis_id] = {
                'file_path': str(file_path),
                'filename': unique_filename,  # ä½¿ç”¨å®Œæ•´çš„æ–‡ä»¶å
                'dataframe': df,
                'question_types': question_types,
                'upload_time': datetime.now().isoformat()
            }
            
            # è½¬æ¢æ•°æ®ç»“æ„ä»¥åŒ¹é…å‰ç«¯æœŸæœ›
            def transform_question_types(question_types):
                """è½¬æ¢åç«¯æ•°æ®ç»“æ„ä¸ºå‰ç«¯æœŸæœ›çš„æ ¼å¼ - é€‚åº”æ–°çš„å­—æ®µçº§é¢˜å‹è¯†åˆ«"""
                logger.info(f"ğŸ”„ å¼€å§‹è½¬æ¢æ•°æ®ç»“æ„")
                logger.info(f"åŸå§‹æ•°æ®keys: {list(question_types.keys())}")
                
                # å…ˆè¾“å‡ºåŸå§‹æ•°æ®çš„å†…å®¹
                logger.info(f"åŸå§‹æ•°æ®å†…å®¹è¯¦æƒ…:")
                for key, value in question_types.items():
                    logger.info(f"  {key}: {type(value)} - {len(value) if isinstance(value, (list, dict)) else 'N/A'}")
                
                transformed = {
                    'scaleQuestions': [],
                    'singleChoice': [],
                    'openEnded': [],
                    'field_types': [],  # æ‰€æœ‰å­—æ®µçš„é¢˜å‹ä¿¡æ¯
                    'summary': {
                        'breakdown': {}
                    }
                }
                
                # å¤„ç†æ‰€æœ‰å­—æ®µç±»å‹
                field_types = question_types.get('field_types', [])
                logger.info(f"å­—æ®µç±»å‹åŸå§‹æ•°æ®: {len(field_types)} ä¸ª")
                transformed['field_types'] = field_types
                
                # å¤„ç†é‡è¡¨é¢˜
                scale_questions = question_types.get('scale_questions', [])
                logger.info(f"é‡è¡¨é¢˜åŸå§‹æ•°æ®: {len(scale_questions)} ä¸ª")
                transformed['scaleQuestions'] = scale_questions
                
                # å¤„ç†å•é€‰é¢˜
                single_choice = question_types.get('single_choice', [])
                logger.info(f"å•é€‰é¢˜åŸå§‹æ•°æ®: {len(single_choice)} ä¸ª")
                transformed['singleChoice'] = single_choice
                
                # å¤„ç†å¼€æ”¾é¢˜
                open_ended = question_types.get('open_ended', [])
                logger.info(f"å¼€æ”¾é¢˜åŸå§‹æ•°æ®: {len(open_ended)} ä¸ª")
                transformed['openEnded'] = open_ended
                
                # æ–°é€»è¾‘ä¸­æ— å…¶ä»–é¢˜å‹ï¼Œæ‰€æœ‰å­—æ®µéƒ½è¢«åˆ†ç±»ä¸ºé‡è¡¨é¢˜ã€å•é€‰é¢˜æˆ–å¼€æ”¾é¢˜
                logger.info(f"æ–°é€»è¾‘å·²è¦†ç›–æ‰€æœ‰å­—æ®µï¼Œæ— å…¶ä»–é¢˜å‹")
                
                # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
                transformed['summary']['breakdown'] = {
                    'total_fields': len(field_types),
                    'scaleQuestions': len(scale_questions),
                    'singleChoice': len(single_choice),
                    'openEnded': len(open_ended)
                }
                
                logger.info(f"âœ… è½¬æ¢å®Œæˆï¼Œç»Ÿè®¡ä¿¡æ¯: {transformed['summary']['breakdown']}")
                
                # è¾“å‡ºè½¬æ¢åçš„æ•°æ®ç»“æ„ç”¨äºè°ƒè¯•
                logger.info(f"è½¬æ¢åçš„æ•°æ®ç»“æ„:")
                for key, value in transformed.items():
                    if key != 'summary':
                        logger.info(f"  {key}: {len(value) if isinstance(value, list) else type(value)}")
                
                return transformed
            
            # è¿”å›ä¸Šä¼ ä¿¡æ¯
            response_data = {
                'analysisId': analysis_id,
                'filename': unique_filename,  # ä½¿ç”¨å®Œæ•´çš„æ–‡ä»¶å
                'fileSize': file_path.stat().st_size,
                'rowCount': len(df),
                'columnCount': len(df.columns),
                'questionTypes': convert_pandas_types(transform_question_types(question_types)),
                'columns': df.columns.tolist()
            }
            
            logger.info(f"âœ… æ–‡ä»¶åˆ†æå®Œæˆï¼Œåˆ†æID: {analysis_id}")
            return jsonify(response_data)
            
        except Exception as e:
            logger.error(f"âŒ æ–‡ä»¶åˆ†æå¤±è´¥: {e}")
            return jsonify({'error': f'æ–‡ä»¶åˆ†æå¤±è´¥: {str(e)}'}), 500
            
    except Exception as e:
        logger.error(f"âŒ ä¸Šä¼ å¤„ç†å¤±è´¥: {e}")
        return jsonify({'error': f'ä¸Šä¼ å¤„ç†å¤±è´¥: {str(e)}'}), 500

@app.route('/analyze-text', methods=['POST'])
def analyze_text():
    """åˆ†æé—®å·æ•°æ® - å¤„ç†æ‰€æœ‰å­—æ®µ"""
    try:
        logger.info("ğŸ” æ”¶åˆ°åˆ†æè¯·æ±‚")
        
        data = request.get_json()
        analysis_id = data.get('analysisId')
        
        if not analysis_id or analysis_id not in analysis_results:
            return jsonify({'error': 'æ— æ•ˆçš„åˆ†æID'}), 400
        
        analysis_info = analysis_results[analysis_id]
        df = analysis_info['dataframe']
        
        # ä½¿ç”¨æ‰€æœ‰å­—æ®µè¿›è¡Œåˆ†æ
        all_fields = df.columns.tolist()
        logger.info(f"ğŸ“Š å¼€å§‹åˆ†ææ‰€æœ‰å­—æ®µï¼Œå…± {len(all_fields)} ä¸ªå­—æ®µ")
        
        # æ‰§è¡Œå®Œæ•´çš„åˆ†ææµç¨‹
        try:
            # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨classificationå¤„ç†
            logger.info("ğŸ”§ ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨classificationå¤„ç†")
            
            try:
                from classification import QuestionnaireTranslationClassifier
                logger.info("âœ… æˆåŠŸå¯¼å…¥ QuestionnaireTranslationClassifier")
            except ImportError as e:
                logger.error(f"âŒ å¯¼å…¥ QuestionnaireTranslationClassifier å¤±è´¥: {e}")
                return jsonify({'error': f'å¯¼å…¥ classification æ¨¡å—å¤±è´¥: {str(e)}'}), 500
            
            classifier = QuestionnaireTranslationClassifier()
            input_file = analysis_info['file_path']
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            classification_output = Path(input_file).parent / f"classification_{timestamp}.xlsx"
            
            success = classifier.process_table(input_file, str(classification_output))
            
            if not success:
                return jsonify({'error': 'classificationå¤„ç†å¤±è´¥'}), 500
            
            logger.info(f"âœ… classificationå¤„ç†å®Œæˆ: {classification_output}")
            
            # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨universal_questionnaire_analyzeråˆ†æ
            logger.info("ğŸ” ç¬¬äºŒæ­¥ï¼šä½¿ç”¨universal_questionnaire_analyzeråˆ†æ")
            
            try:
                from universal_questionnaire_analyzer import UniversalQuestionnaireAnalyzer
                logger.info("âœ… æˆåŠŸå¯¼å…¥ UniversalQuestionnaireAnalyzer")
            except ImportError as e:
                logger.error(f"âŒ å¯¼å…¥ UniversalQuestionnaireAnalyzer å¤±è´¥: {e}")
                return jsonify({'error': f'å¯¼å…¥åˆ†ææ¨¡å—å¤±è´¥: {str(e)}'}), 500
            
            analyzer = UniversalQuestionnaireAnalyzer()
            processed_df = analyzer.read_data_file(str(classification_output))
            
            if processed_df is None:
                return jsonify({'error': 'æ— æ³•è¯»å–å¤„ç†åçš„æ–‡ä»¶'}), 500
            
            # ä½¿ç”¨æ‰€æœ‰å¯ç”¨å­—æ®µ
            available_fields = processed_df.columns.tolist()
            logger.info(f"âœ… ä½¿ç”¨æ‰€æœ‰å¯ç”¨å­—æ®µè¿›è¡Œåˆ†æï¼Œå…± {len(available_fields)} ä¸ªå­—æ®µ")
            
            # è¯†åˆ«é—®é¢˜ç±»å‹
            question_types = analyzer.identify_all_question_types(processed_df)
            
            # ç”Ÿæˆåˆ†ææŠ¥å‘Š - ä½¿ç”¨æ–°çš„å­—æ®µçº§é¢˜å‹è¯†åˆ«ç»“æœ
            analysis_result = {
                'summary': {
                    'total_responses': len(processed_df),
                    'total_fields': len(processed_df.columns),
                    'scale_questions': len(question_types['scale_questions']),
                    'single_choice': len(question_types['single_choice']),
                    'open_ended': len(question_types['open_ended']),
                    'processing_time': datetime.now().isoformat()
                },
                'field_types': question_types['field_types'],  # æ‰€æœ‰å­—æ®µçš„é¢˜å‹ä¿¡æ¯
                'scale_questions': question_types['scale_questions'],
                'single_choice': question_types['single_choice'],
                'open_ended': question_types['open_ended']
            }
            
            # ä¸ºæ¯ä¸ªå­—æ®µæ·»åŠ æ ·æœ¬æ•°æ®
            for field_info in analysis_result['field_types']:
                field = field_info['column']
                field_data = processed_df[field]
                # è¿‡æ»¤NaNå€¼
                field_data_clean = field_data.dropna() if hasattr(field_data, 'dropna') else field_data[pd.notna(field_data)]
                
                # è·å–æ ·æœ¬æ•°æ®
                if len(field_data_clean) > 0:
                    if hasattr(field_data_clean, 'head'):
                        sample_data = field_data_clean.head(5).tolist()
                    else:
                        sample_data = field_data_clean[:5].tolist()
                    
                    field_info['sample_data'] = sample_data
                    field_info['response_count'] = len(field_data_clean)
            
            # å­˜å‚¨åˆ†æç»“æœ
            analysis_results[analysis_id]['analysis_result'] = analysis_result
            analysis_results[analysis_id]['processed_file'] = str(classification_output)
            
            logger.info(f"âœ… åˆ†æå®Œæˆï¼Œåˆ†æID: {analysis_id}")
            return jsonify({
                'results': convert_pandas_types(analysis_result),
                'analysisId': analysis_id
            })
            
        except Exception as e:
            logger.error(f"âŒ åˆ†æè¿‡ç¨‹å¤±è´¥: {e}")
            return jsonify({'error': f'åˆ†æè¿‡ç¨‹å¤±è´¥: {str(e)}'}), 500
            
    except Exception as e:
        logger.error(f"âŒ åˆ†æè¯·æ±‚å¤„ç†å¤±è´¥: {e}")
        return jsonify({'error': f'åˆ†æè¯·æ±‚å¤„ç†å¤±è´¥: {str(e)}'}), 500

@app.route('/translate-open-questions', methods=['POST'])
def translate_open_questions():
    """ç¿»è¯‘å¼€æ”¾é¢˜å­—æ®µ - ä¸ºåç»­çš„æ ‡å‡†æ‰“æ ‡æˆ–å‚è€ƒæ ‡ç­¾æ‰“æ ‡åšå‡†å¤‡"""
    try:
        logger.info("ğŸ” æ”¶åˆ°å¼€æ”¾é¢˜ç¿»è¯‘è¯·æ±‚")
        
        data = request.get_json()
        analysis_id = data.get('analysisId')
        
        if not analysis_id or analysis_id not in analysis_results:
            return jsonify({'error': 'æ— æ•ˆçš„åˆ†æID'}), 400
        
        analysis_info = analysis_results[analysis_id]
        input_file = analysis_info['file_path']
        question_types = analysis_info.get('question_types', {})
        
        logger.info(f"ğŸ“Š å¼€å§‹ç¿»è¯‘å¼€æ”¾é¢˜å­—æ®µï¼Œæ–‡ä»¶: {input_file}")
        
        # è·å–å¼€æ”¾é¢˜å­—æ®µåˆ—è¡¨
        open_ended_fields = []
        if 'open_ended' in question_types:
            for q in question_types['open_ended']:
                if isinstance(q, dict) and 'column' in q:
                    open_ended_fields.append(q['column'])
        
        if not open_ended_fields:
            return jsonify({'error': 'æ²¡æœ‰æ‰¾åˆ°å¼€æ”¾é¢˜å­—æ®µ'}), 400
        
        logger.info(f"ğŸ” è¯†åˆ«åˆ° {len(open_ended_fields)} ä¸ªå¼€æ”¾é¢˜å­—æ®µ: {open_ended_fields}")
        
        try:
            # å¯¼å…¥classificationæ¨¡å—è¿›è¡Œç¿»è¯‘
            try:
                from classification import QuestionnaireTranslationClassifier
                logger.info("âœ… æˆåŠŸå¯¼å…¥ QuestionnaireTranslationClassifier")
            except ImportError as e:
                logger.error(f"âŒ å¯¼å…¥ QuestionnaireTranslationClassifier å¤±è´¥: {e}")
                return jsonify({'error': f'å¯¼å…¥ classification æ¨¡å—å¤±è´¥: {str(e)}'}), 500
            
            classifier = QuestionnaireTranslationClassifier()
            
            # ç”Ÿæˆç¿»è¯‘è¾“å‡ºæ–‡ä»¶è·¯å¾„
            base_name, original_timestamp = extract_file_info(input_file)
            translate_output = TRANSLATE_FOLDER / f"{base_name}_translate_{original_timestamp}.xlsx"
            
            # æ‰§è¡Œç¿»è¯‘ï¼ˆåªç¿»è¯‘ï¼Œä¸è¿›è¡ŒAIåˆ†ç±»ï¼‰
            logger.info(f"ğŸ”§ å¼€å§‹æ‰§è¡Œå¼€æ”¾é¢˜ç¿»è¯‘: {input_file} -> {translate_output}")
            
            success = classifier.translate_only(input_file, str(translate_output), open_ended_fields)
            
            if not success:
                return jsonify({'error': 'å¼€æ”¾é¢˜ç¿»è¯‘å¤„ç†å¤±è´¥'}), 500
            
            logger.info(f"âœ… å¼€æ”¾é¢˜ç¿»è¯‘å®Œæˆ: {translate_output}")
            
            # è¯»å–ç¿»è¯‘åçš„æ–‡ä»¶
            translated_df = pd.read_excel(str(translate_output))
            
            # ç”Ÿæˆç¿»è¯‘ç»“æœ
            result = {
                'summary': {
                    'total_responses': len(translated_df),
                    'translated_fields': len(open_ended_fields),
                    'open_ended_fields': open_ended_fields,
                    'processing_time': datetime.now().isoformat(),
                    'output_file': str(translate_output)
                },
                'translated_data': [],
                'open_ended_fields': open_ended_fields,
                'available_for_labeling': True
            }
            
            # å‡†å¤‡ç¿»è¯‘åçš„æ•°æ®é¢„è§ˆï¼ˆå‰10è¡Œï¼‰
            for col in translated_df.columns:
                result['translated_data'].append({
                    'field': col,
                    'values': translated_df[col].head(10).fillna('').astype(str).tolist(),
                    'is_translated': col.endswith('-CN'),
                    'is_open_ended': col.replace('-CN', '') in open_ended_fields
                })
            
            # å­˜å‚¨ç¿»è¯‘ç»“æœ
            analysis_results[analysis_id]['translation_result'] = result
            analysis_results[analysis_id]['translation_output'] = str(translate_output)
            
            logger.info(f"âœ… å¼€æ”¾é¢˜ç¿»è¯‘å®Œæˆï¼Œåˆ†æID: {analysis_id}")
            return jsonify(convert_pandas_types(result))
            
        except Exception as e:
            logger.error(f"âŒ å¼€æ”¾é¢˜ç¿»è¯‘å¤±è´¥: {e}")
            logger.error(f"âŒ å¼‚å¸¸ç±»å‹: {type(e)}")
            import traceback
            logger.error(f"âŒ å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
            return jsonify({'error': f'å¼€æ”¾é¢˜ç¿»è¯‘å¤±è´¥: {str(e)}'}), 500
            
    except Exception as e:
        logger.error(f"âŒ ç¿»è¯‘è¯·æ±‚å¤„ç†å¤±è´¥: {e}")
        return jsonify({'error': f'ç¿»è¯‘è¯·æ±‚å¤„ç†å¤±è´¥: {str(e)}'}), 500

@app.route('/standard-labeling', methods=['POST'])
def handle_standard_labeling():
    """æ ‡å‡†AIæ‰“æ ‡ - åŸºäºç¿»è¯‘åçš„æ•°æ®è¿›è¡ŒAIè‡ªåŠ¨åˆ†ç±»"""
    try:
        logger.info("ğŸ” æ”¶åˆ°æ ‡å‡†AIæ‰“æ ‡è¯·æ±‚")
        
        data = request.get_json()
        analysis_id = data.get('analysisId')
        
        if not analysis_id or analysis_id not in analysis_results:
            return jsonify({'error': 'æ— æ•ˆçš„åˆ†æID'}), 400
        
        analysis_info = analysis_results[analysis_id]
        
        # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆç¿»è¯‘
        if 'translation_output' not in analysis_info:
            return jsonify({'error': 'è¯·å…ˆå®Œæˆå¼€æ”¾é¢˜ç¿»è¯‘'}), 400
        
        translation_output = analysis_info['translation_output']
        if not os.path.exists(translation_output):
            return jsonify({'error': 'ç¿»è¯‘ç»“æœæ–‡ä»¶ä¸å­˜åœ¨'}), 400
        
        logger.info(f"ğŸ“Š åŸºäºç¿»è¯‘æ–‡ä»¶è¿›è¡Œæ ‡å‡†AIæ‰“æ ‡: {translation_output}")
        
        try:
            # å¯¼å…¥classificationæ¨¡å—
            try:
                from classification import QuestionnaireTranslationClassifier
                logger.info("âœ… æˆåŠŸå¯¼å…¥ QuestionnaireTranslationClassifier")
            except ImportError as e:
                logger.error(f"âŒ å¯¼å…¥ QuestionnaireTranslationClassifier å¤±è´¥: {e}")
                return jsonify({'error': f'å¯¼å…¥ classification æ¨¡å—å¤±è´¥: {str(e)}'}), 500
            
            classifier = QuestionnaireTranslationClassifier()
            
            # ç”Ÿæˆæ ‡å‡†æ‰“æ ‡è¾“å‡ºæ–‡ä»¶è·¯å¾„
            base_name, original_timestamp = extract_file_info(translation_output)
            standard_labeling_output = TRANSLATE_AI_FOLDER / f"{base_name}_ai_{original_timestamp}.xlsx"
            
            # æ‰§è¡Œæ ‡å‡†AIæ‰“æ ‡ï¼ˆåŸºäºå·²ç¿»è¯‘çš„æ•°æ®ï¼‰
            logger.info(f"ğŸ”§ å¼€å§‹æ‰§è¡Œæ ‡å‡†AIæ‰“æ ‡: {translation_output} -> {standard_labeling_output}")
            
            success = classifier.standard_labeling_only(str(translation_output), str(standard_labeling_output))
            
            if not success:
                return jsonify({'error': 'æ ‡å‡†AIæ‰“æ ‡å¤„ç†å¤±è´¥'}), 500
            
            logger.info(f"âœ… æ ‡å‡†AIæ‰“æ ‡å®Œæˆ: {standard_labeling_output}")
            
            # è¯»å–å¤„ç†åçš„æ–‡ä»¶
            processed_df = pd.read_excel(str(standard_labeling_output))
            
            # è·å–å¼€æ”¾é¢˜å­—æ®µï¼ˆç”¨äºé¢˜å‹è¯†åˆ«ï¼‰
            open_ended_fields = analysis_info.get('translation_result', {}).get('open_ended_fields', [])
            
            # ç”Ÿæˆå¤„ç†ç»“æœ
            result = {
                'summary': {
                    'total_responses': len(processed_df),
                    'processed_fields': len(open_ended_fields),
                    'processing_time': datetime.now().isoformat(),
                    'output_file': str(standard_labeling_output),
                    'labeling_type': 'standard_ai'
                },
                'processed_data': [],
                'field_analysis': {},
                'sample_size': len(processed_df)
            }
            
            # å‡†å¤‡å¤„ç†åçš„æ•°æ®ï¼ˆå®Œæ•´æ•°æ®ï¼‰- è½¬æ¢ä¸ºæ•°ç»„æ ¼å¼
            for col in processed_df.columns:
                result['processed_data'].append({
                    'field': col,
                    'values': processed_df[col].fillna('').astype(str).tolist()
                })
            
            # ä¸ºæ¯ä¸ªå¼€æ”¾é¢˜å­—æ®µç”Ÿæˆè¯¦ç»†åˆ†æ
            for field in open_ended_fields:
                if field in processed_df.columns:
                    # åˆ†æç¿»è¯‘åˆ—
                    cn_field = f"{field}-CN"
                    main_topic_field = f"{field}ä¸€çº§ä¸»é¢˜"
                    sub_tag_field = f"{field}äºŒçº§æ ‡ç­¾"
                    
                    field_analysis = {
                        'original_field': field,
                        'has_translation': cn_field in processed_df.columns,
                        'has_main_topics': main_topic_field in processed_df.columns,
                        'has_sub_tags': sub_tag_field in processed_df.columns
                    }
                    
                    # ç»Ÿè®¡ä¸»é¢˜åˆ†å¸ƒ
                    if main_topic_field in processed_df.columns:
                        main_topics = processed_df[main_topic_field].dropna()
                        if len(main_topics) > 0:
                            topic_counts = main_topics.value_counts().head(10)
                            field_analysis['main_topics'] = topic_counts.to_dict()
                    
                    # ç»Ÿè®¡äºŒçº§æ ‡ç­¾åˆ†å¸ƒ
                    if sub_tag_field in processed_df.columns:
                        sub_tags = processed_df[sub_tag_field].dropna()
                        if len(sub_tags) > 0:
                            tag_counts = sub_tags.value_counts().head(10)
                            field_analysis['sub_tags'] = tag_counts.to_dict()
                    
                    result['field_analysis'][field] = field_analysis
            
            # å­˜å‚¨æ ‡å‡†æ‰“æ ‡ç»“æœ
            analysis_results[analysis_id]['standard_labeling_result'] = result
            analysis_results[analysis_id]['standard_labeling_output'] = str(standard_labeling_output)
            
            logger.info(f"âœ… æ ‡å‡†AIæ‰“æ ‡å®Œæˆï¼Œåˆ†æID: {analysis_id}")
            return jsonify(convert_pandas_types(result))
            
        except Exception as e:
            logger.error(f"âŒ æ ‡å‡†AIæ‰“æ ‡å¤±è´¥: {e}")
            logger.error(f"âŒ å¼‚å¸¸ç±»å‹: {type(e)}")
            import traceback
            logger.error(f"âŒ å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
            return jsonify({'error': f'æ ‡å‡†AIæ‰“æ ‡å¤±è´¥: {str(e)}'}), 500
            
    except Exception as e:
        logger.error(f"âŒ æ ‡å‡†AIæ‰“æ ‡è¯·æ±‚å¤„ç†å¤±è´¥: {e}")
        return jsonify({'error': f'æ ‡å‡†AIæ‰“æ ‡è¯·æ±‚å¤„ç†å¤±è´¥: {str(e)}'}), 500

# # å¤„ç†Classificationåˆ†æ å¼€å§‹åˆ†æ (ä¿æŒå‘åå…¼å®¹)
# @app.route('/classification', methods=['POST'])
# def handle_classification():
#     """å¤„ç†Classificationåˆ†æ"""
#     try:
#         logger.info("ğŸ” æ”¶åˆ°Classificationå¤„ç†è¯·æ±‚")
        
#         data = request.get_json()
#         analysis_id = data.get('analysisId')
#         selected_fields = data.get('selectedFields', [])
        
#         if not analysis_id or analysis_id not in analysis_results:
#             return jsonify({'error': 'æ— æ•ˆçš„åˆ†æID'}), 400
        
#         # æ³¨æ„ï¼šé€‰æ‹©å­—æ®µæ£€æŸ¥è¢«æ³¨é‡Šæ‰æ˜¯ä¸ºäº†å…è®¸å¤„ç†æ‰€æœ‰å­—æ®µ
#         # if not selected_fields:
#         #     return jsonify({'error': 'è¯·é€‰æ‹©è¦åˆ†æçš„å­—æ®µ'}), 400
        
#         analysis_info = analysis_results[analysis_id]
#         input_file = analysis_info['file_path']
        
#         logger.info(f"ğŸ“Š å¼€å§‹Classificationå¤„ç†ï¼Œæ–‡ä»¶: {input_file}")
        
#         try:
#             # å¯¼å…¥classificationæ¨¡å—
#             try:
#                 from classification import QuestionnaireTranslationClassifier
#                 logger.info("âœ… æˆåŠŸå¯¼å…¥ QuestionnaireTranslationClassifier")
#             except ImportError as e:
#                 logger.error(f"âŒ å¯¼å…¥ QuestionnaireTranslationClassifier å¤±è´¥: {e}")
#                 return jsonify({'error': f'å¯¼å…¥ classification æ¨¡å—å¤±è´¥: {str(e)}'}), 500
            
#             classifier = QuestionnaireTranslationClassifier()
            
#             # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„åˆ°classificationå­ç›®å½•
#             # ä»åŸå§‹æ–‡ä»¶è·¯å¾„æå–åŸºç¡€ä¿¡æ¯
#             base_name, original_timestamp = extract_file_info(input_file)
#             output_file = CLASSIFICATION_FOLDER / f"{base_name}_class_{original_timestamp}.xlsx"
            
#             # æ‰§è¡Œclassificationå¤„ç†
#             logger.info(f"ğŸ”§ å¼€å§‹æ‰§è¡Œclassificationå¤„ç†: {input_file} -> {output_file}")
#             logger.info(f"ğŸ“‹ è¾“å…¥æ–‡ä»¶å­˜åœ¨: {Path(input_file).exists()}")
#             logger.info(f"ğŸ“‹ è¾“å…¥æ–‡ä»¶å¤§å°: {Path(input_file).stat().st_size if Path(input_file).exists() else 'N/A'} bytes")
            
#             try:
#                 logger.info("ğŸš€ å³å°†è°ƒç”¨ classifier.process_table æ–¹æ³•")
#                 success = classifier.process_table(input_file, str(output_file))
#                 logger.info(f"ğŸ”§ classifier.process_table è¿”å›ç»“æœ: {success}")
#                 logger.info(f"ğŸ“‹ è¾“å‡ºæ–‡ä»¶å­˜åœ¨: {Path(output_file).exists()}")
#                 if Path(output_file).exists():
#                     logger.info(f"ğŸ“‹ è¾“å‡ºæ–‡ä»¶å¤§å°: {Path(output_file).stat().st_size} bytes")
                
#                 if not success:
#                     logger.error("âŒ classifier.process_table è¿”å› False")
#                     return jsonify({'error': 'Classificationå¤„ç†å¤±è´¥ - process_tableè¿”å›False'}), 500
                    
#             except Exception as process_error:
#                 logger.error(f"âŒ classifier.process_table æ‰§è¡Œæ—¶å‘ç”Ÿå¼‚å¸¸: {process_error}")
#                 logger.error(f"âŒ å¼‚å¸¸ç±»å‹: {type(process_error)}")
#                 import traceback
#                 logger.error(f"âŒ å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
#                 return jsonify({'error': f'Classificationå¤„ç†å¼‚å¸¸: {str(process_error)}'}), 500
            
#             logger.info(f"âœ… Classificationå¤„ç†å®Œæˆ: {output_file}")
            
#             # è¯»å–å¤„ç†åçš„æ–‡ä»¶
#             processed_df = pd.read_excel(str(output_file))
            
#             # ç­›é€‰é€‰ä¸­çš„å­—æ®µï¼ˆå¦‚æœå­˜åœ¨ï¼‰
#             available_fields = [col for col in processed_df.columns if col in selected_fields or col in [col.replace('_ç¿»è¯‘', '') for col in selected_fields]]
#             if not available_fields:
#                 # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„å­—æ®µï¼Œä½¿ç”¨æ‰€æœ‰å­—æ®µ
#                 available_fields = processed_df.columns.tolist()
            
#             # ç”Ÿæˆå¤„ç†ç»“æœ
#             result = {
#                 'summary': {
#                     'total_responses': len(processed_df),
#                     'processed_fields': len(available_fields),
#                     'processing_time': datetime.now().isoformat(),
#                     'output_file': str(output_file)
#                 },
#                 'processed_data': {},
#                 'field_analysis': {},
#                 'sample_size': min(10, len(processed_df))
#             }
            
#             # å‡†å¤‡å¤„ç†åçš„æ•°æ®ï¼ˆå‰10è¡Œï¼‰- è½¬æ¢ä¸ºæ•°ç»„æ ¼å¼
#             result['processed_data'] = []
            
#             # è¯»å–å¤„ç†åçš„æ–‡ä»¶å¹¶è¯†åˆ«é¢˜å‹
#             try:
#                 from universal_questionnaire_analyzer import UniversalQuestionnaireAnalyzer
#                 logger.info("âœ… æˆåŠŸå¯¼å…¥ UniversalQuestionnaireAnalyzer")
#             except ImportError as e:
#                 logger.error(f"âŒ å¯¼å…¥ UniversalQuestionnaireAnalyzer å¤±è´¥: {e}")
#                 return jsonify({'error': f'å¯¼å…¥åˆ†ææ¨¡å—å¤±è´¥: {str(e)}'}), 500
            
#             analyzer = UniversalQuestionnaireAnalyzer()
#             question_types = analyzer.identify_all_question_types(processed_df)
            
#             # æ‰“å°é¢˜å‹è¯†åˆ«ç»“æœ
#             logger.info("é¢˜å‹è¯†åˆ«ç»“æœ:")
#             logger.info(f"å•é€‰é¢˜: {question_types.get('single_choice', [])}")
#             logger.info(f"é‡è¡¨é¢˜: {question_types.get('scale_questions', [])}")
#             logger.info(f"å¼€æ”¾é¢˜: {question_types.get('open_ended', [])}")
            
#             # åˆ›å»ºå­—æ®µåˆ°ç±»å‹çš„æ˜ å°„
#             field_type_map = {}
            
#             # 1. å¤„ç†å•é€‰é¢˜ (type: 0)
#             for q in question_types.get('single_choice', []):
#                 if isinstance(q, dict) and 'column' in q:
#                     field_type_map[q['column']] = 0
            
#             # 2. å¤„ç†é‡è¡¨é¢˜ (type: 1)
#             for q in question_types.get('scale_questions', []):
#                 if isinstance(q, dict) and 'column' in q:
#                     field_type_map[q['column']] = 1
            
#             # 3. å¤„ç†å¼€æ”¾é¢˜ (type: 2)
#             for q in question_types.get('open_ended', []):
#                 if isinstance(q, dict) and 'column' in q:
#                     field_type_map[q['column']] = 2
            
#             # æ–°é€»è¾‘ä¸­æ— å…¶ä»–é¢˜å‹ï¼Œæ‰€æœ‰å­—æ®µéƒ½å·²è¢«åˆ†ç±»


            
#             for col in available_fields:
#                 if col in processed_df.columns:
#                     result['processed_data'].append({
#                         'field': col,
#                         'values': processed_df[col].head(10).fillna('').astype(str).tolist(),
#                         'type': field_type_map.get(col)  # å¦‚æœå­—æ®µæ²¡æœ‰è¢«åˆ†ç±»ï¼Œå°†è¿”å› None
#                     })
            
#             # ä¸ºæ¯ä¸ªå­—æ®µç”Ÿæˆè¯¦ç»†åˆ†æ
#             for field in available_fields:
#                 if field in processed_df.columns:
#                     field_data = processed_df[field]
#                     field_data_clean = field_data.dropna()
                    
#                     if len(field_data_clean) > 0:
#                         # è·å–æ ·æœ¬æ•°æ®
#                         sample_data = field_data_clean.head(10).tolist()
                        
#                         # è·å–å”¯ä¸€å€¼æ•°é‡
#                         unique_count = len(field_data_clean.unique())
                        
#                         # å¦‚æœæ˜¯ç¿»è¯‘å­—æ®µï¼Œå°è¯•æå–ä¸»è¦ä¸»é¢˜
#                         main_topics = []
#                         if '_ç¿»è¯‘' in field or '_æ ‡ç­¾' in field:
#                             # ä»æ•°æ®ä¸­æå–å‰5ä¸ªæœ€å¸¸è§çš„å€¼ä½œä¸ºä¸»é¢˜
#                             value_counts = field_data_clean.value_counts().head(5)
#                             main_topics = value_counts.index.tolist()
                        
#                         result['field_analysis'][field] = {
#                             'response_count': len(field_data_clean),
#                             'unique_values': unique_count,
#                             'sample_data': sample_data,
#                             'main_topics': main_topics
#                         }
            
#             # å­˜å‚¨å¤„ç†ç»“æœ
#             analysis_results[analysis_id]['classification_result'] = result
#             analysis_results[analysis_id]['classification_output'] = str(output_file)
            
#             logger.info(f"âœ… Classificationå¤„ç†å®Œæˆï¼Œåˆ†æID: {analysis_id}")
#             return jsonify(convert_pandas_types(result))
            
#         except Exception as e:
#             logger.error(f"âŒ Classificationå¤„ç†å¤±è´¥: {e}")
#             logger.error(f"âŒ å¼‚å¸¸ç±»å‹: {type(e)}")
#             import traceback
#             logger.error(f"âŒ å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
#             return jsonify({'error': f'Classificationå¤„ç†å¤±è´¥: {str(e)}'}), 500
            
#     except Exception as e:
#         logger.error(f"âŒ Classificationè¯·æ±‚å¤„ç†å¤±è´¥: {e}")
#         logger.error(f"âŒ å¼‚å¸¸ç±»å‹: {type(e)}")
#         import traceback
#         logger.error(f"âŒ å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
#         return jsonify({'error': f'Classificationè¯·æ±‚å¤„ç†å¤±è´¥: {str(e)}'}), 500

@app.route('/retag-with-reference', methods=['POST'])
def retag_with_reference():
    """åŸºäºå‚è€ƒæ ‡ç­¾é‡æ–°æ‰“æ ‡"""
    try:
        logger.info("ğŸ·ï¸  æ”¶åˆ°å‚è€ƒæ ‡ç­¾é‡æ–°æ‰“æ ‡è¯·æ±‚")
        
        data = request.get_json()
        analysis_id = data.get('analysisId')
        reference_tags = data.get('reference_tags', [])
        
        if not analysis_id or analysis_id not in analysis_results:
            return jsonify({'error': 'æ— æ•ˆçš„åˆ†æID'}), 400
        
        if not reference_tags:
            return jsonify({'error': 'è¯·æä¾›å‚è€ƒæ ‡ç­¾'}), 400
        
        # éªŒè¯å‚è€ƒæ ‡ç­¾æ ¼å¼
        for tag in reference_tags:
            if not isinstance(tag, dict) or 'name' not in tag or 'definition' not in tag:
                return jsonify({'error': 'å‚è€ƒæ ‡ç­¾æ ¼å¼é”™è¯¯ï¼Œéœ€è¦åŒ…å«nameå’Œdefinitionå­—æ®µ'}), 400
        
        analysis_info = analysis_results[analysis_id]
        
        # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆç¿»è¯‘
        if 'translation_output' not in analysis_info:
            return jsonify({'error': 'è¯·å…ˆå®Œæˆå¼€æ”¾é¢˜ç¿»è¯‘'}), 400
        
        translation_output = analysis_info['translation_output']
        if not os.path.exists(translation_output):
            return jsonify({'error': 'ç¿»è¯‘ç»“æœæ–‡ä»¶ä¸å­˜åœ¨'}), 400
        
        input_file = translation_output
        logger.info(f"ğŸ“ åŸºäºç¿»è¯‘æ–‡ä»¶è¿›è¡Œå‚è€ƒæ ‡ç­¾æ‰“æ ‡: {input_file}")
        
        logger.info(f"ğŸ“Š å¼€å§‹åŸºäºå‚è€ƒæ ‡ç­¾é‡æ–°æ‰“æ ‡ï¼Œæ–‡ä»¶: {input_file}")
        logger.info(f"ğŸ“‹ å‚è€ƒæ ‡ç­¾æ•°é‡: {len(reference_tags)}")
        for tag in reference_tags:
            logger.info(f"  - {tag['name']}: {tag['definition']}")
        
        try:
            # å¯¼å…¥classificationæ¨¡å—
            try:
                from classification import QuestionnaireTranslationClassifier
                logger.info("âœ… æˆåŠŸå¯¼å…¥ QuestionnaireTranslationClassifier")
            except ImportError as e:
                logger.error(f"âŒ å¯¼å…¥ QuestionnaireTranslationClassifier å¤±è´¥: {e}")
                return jsonify({'error': f'å¯¼å…¥ classification æ¨¡å—å¤±è´¥: {str(e)}'}), 500
            
            classifier = QuestionnaireTranslationClassifier()
            
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„åˆ°translate_customå­ç›®å½•
            # ä»ç¿»è¯‘æ–‡ä»¶è·¯å¾„æå–åŸºç¡€ä¿¡æ¯
            base_name, original_timestamp = extract_file_info(input_file)
            output_file = TRANSLATE_CUSTOM_FOLDER / f"{base_name}_custom_{original_timestamp}.xlsx"
            
            # ç›´æ¥è¯»å–ç°æœ‰æ–‡ä»¶å¹¶è¿›è¡Œé‡æ–°æ‰“æ ‡ï¼ˆä¸é‡æ–°ç¿»è¯‘ï¼‰
            logger.info(f"ğŸ”§ å¼€å§‹åŸºäºç°æœ‰ç¿»è¯‘è¿›è¡Œå‚è€ƒæ ‡ç­¾é‡æ–°æ‰“æ ‡")
            
            try:
                # è¯»å–ç°æœ‰æ–‡ä»¶
                df = pd.read_excel(input_file)
                logger.info(f"ğŸ“‹ è¯»å–æ–‡ä»¶æˆåŠŸï¼Œå…± {len(df)} è¡Œï¼Œ{len(df.columns)} åˆ—")
                
                # è¯†åˆ«å·²ç¿»è¯‘çš„-CNå­—æ®µ
                cn_columns = [col for col in df.columns if col.endswith('-CN')]
                logger.info(f"ğŸ” å‘ç° {len(cn_columns)} ä¸ªå·²ç¿»è¯‘çš„-CNå­—æ®µ: {cn_columns}")
                
                if not cn_columns:
                    return jsonify({'error': 'æœªæ‰¾åˆ°å·²ç¿»è¯‘çš„-CNå­—æ®µï¼Œè¯·å…ˆè¿›è¡Œåˆå§‹åˆ†ç±»'}), 400
                
                # è®¾ç½®å‚è€ƒæ ‡ç­¾
                classifier.set_reference_tags(reference_tags)
                
                # ä¸ºæ¯ä¸ª-CNå­—æ®µé‡æ–°æ‰“æ ‡
                for cn_col in cn_columns:
                    original_col = cn_col.replace('-CN', '')
                    tag_col = f"{original_col}æ ‡ç­¾"
                    
                    logger.info(f"ğŸ·ï¸  æ­£åœ¨ä¸º {cn_col} é‡æ–°æ‰“æ ‡...")
                    
                    # è·å–å·²ç¿»è¯‘çš„ä¸­æ–‡æ–‡æœ¬
                    cn_texts = df[cn_col].fillna('').astype(str).tolist()
                    
                    # åŸºäºå‚è€ƒæ ‡ç­¾æ‰“æ ‡
                    assigned_tags = classifier.assign_tags_based_on_reference(cn_texts)
                    
                    # æ›´æ–°DataFrame - åªç”Ÿæˆä¸€ä¸ªæ ‡ç­¾å­—æ®µ
                    df[tag_col] = assigned_tags
                    
                    logger.info(f"âœ… {cn_col} é‡æ–°æ‰“æ ‡å®Œæˆ")
                
                # ä¿å­˜ç»“æœ
                df.to_excel(str(output_file), index=False)
                logger.info(f"ğŸ’¾ é‡æ–°æ‰“æ ‡ç»“æœå·²ä¿å­˜: {output_file}")
                
                success = True
                    
            except Exception as process_error:
                logger.error(f"âŒ é‡æ–°æ‰“æ ‡æ‰§è¡Œæ—¶å‘ç”Ÿå¼‚å¸¸: {process_error}")
                logger.error(f"âŒ å¼‚å¸¸ç±»å‹: {type(process_error)}")
                import traceback
                logger.error(f"âŒ å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
                return jsonify({'error': f'å‚è€ƒæ ‡ç­¾é‡æ–°æ‰“æ ‡å¼‚å¸¸: {str(process_error)}'}), 500
            
            logger.info(f"âœ… å‚è€ƒæ ‡ç­¾é‡æ–°æ‰“æ ‡å®Œæˆ: {output_file}")
            
            # è¯»å–å¤„ç†åçš„æ–‡ä»¶
            processed_df = pd.read_excel(str(output_file))
            
            # ç”Ÿæˆä¸åˆå§‹åˆ†ç±»ç»“æœç›¸åŒæ ¼å¼çš„ç»“æœ
            result = {
                'summary': {
                    'total_responses': len(processed_df),
                    'processed_fields': len([col for col in processed_df.columns if col.endswith('-CN') or col.endswith('æ ‡ç­¾')]),
                    'reference_tags_count': len(reference_tags),
                    'processing_time': datetime.now().isoformat(),
                    'output_file': str(output_file)
                },
                'processed_data': [],
                'reference_tags': reference_tags,
                'sample_size': len(processed_df)
            }
            
            # æŒ‰ç…§åˆå§‹åˆ†ç±»ç»“æœçš„æ ¼å¼ç”Ÿæˆprocessed_data
            for col in processed_df.columns:
                result['processed_data'].append({
                    'field': col,
                    'values': processed_df[col].fillna('').astype(str).tolist()
                })
            
            # å­˜å‚¨å¤„ç†ç»“æœ - ä½¿ç”¨æ–°çš„å­—æ®µåä»¥åŒºåˆ†é…ç½®å‚è€ƒæ ‡ç­¾æ‰“æ ‡
            analysis_results[analysis_id]['custom_labeling_result'] = result  
            analysis_results[analysis_id]['custom_labeling_output'] = str(output_file)
            
            logger.info(f"âœ… å‚è€ƒæ ‡ç­¾é‡æ–°æ‰“æ ‡å®Œæˆï¼Œåˆ†æID: {analysis_id}")
            return jsonify(convert_pandas_types(result))
            
        except Exception as e:
            logger.error(f"âŒ å‚è€ƒæ ‡ç­¾é‡æ–°æ‰“æ ‡å¤±è´¥: {e}")
            logger.error(f"âŒ å¼‚å¸¸ç±»å‹: {type(e)}")
            import traceback
            logger.error(f"âŒ å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
            return jsonify({'error': f'å‚è€ƒæ ‡ç­¾é‡æ–°æ‰“æ ‡å¤±è´¥: {str(e)}'}), 500
            
    except Exception as e:
        logger.error(f"âŒ å‚è€ƒæ ‡ç­¾é‡æ–°æ‰“æ ‡è¯·æ±‚å¤„ç†å¤±è´¥: {e}")
        logger.error(f"âŒ å¼‚å¸¸ç±»å‹: {type(e)}")
        import traceback
        logger.error(f"âŒ å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
        return jsonify({'error': f'å‚è€ƒæ ‡ç­¾é‡æ–°æ‰“æ ‡è¯·æ±‚å¤„ç†å¤±è´¥: {str(e)}'}), 500

@app.route('/analysis-results/<analysis_id>', methods=['GET'])
def get_analysis_results(analysis_id):
    """è·å–åˆ†æç»“æœ"""
    try:
        if analysis_id not in analysis_results:
            return jsonify({'error': 'åˆ†æIDä¸å­˜åœ¨'}), 404
        
        analysis_info = analysis_results[analysis_id]
        result = analysis_info.get('analysis_result')
        
        if not result:
            return jsonify({'error': 'åˆ†æç»“æœä¸å­˜åœ¨'}), 404
        
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"âŒ è·å–åˆ†æç»“æœå¤±è´¥: {e}")
        return jsonify({'error': f'è·å–åˆ†æç»“æœå¤±è´¥: {str(e)}'}), 500

# ç»Ÿè®¡åˆ†æ
@app.route('/statistics', methods=['POST'])
def statistics():
    """ç»Ÿè®¡åˆ†ææ¥å£ - ç‹¬ç«‹è¿›è¡Œç»Ÿè®¡åˆ†æ"""
    try:
        logger.info("ğŸ“Š æ”¶åˆ°ç»Ÿè®¡åˆ†æè¯·æ±‚")
        
        data = request.get_json()
        analysis_id = data.get('analysisId')
        selected_fields = data.get('selectedFields', [])
        question_types = data.get('questionTypes', {})
        
        if not analysis_id or analysis_id not in analysis_results:
            return jsonify({'error': 'æ— æ•ˆçš„åˆ†æID'}), 400
        
        if not selected_fields:
            return jsonify({'error': 'è¯·é€‰æ‹©è¦åˆ†æçš„å­—æ®µ'}), 400
        
        analysis_info = analysis_results[analysis_id]
        df = analysis_info['dataframe']
        
        logger.info(f"ğŸ“ˆ å¼€å§‹ç»Ÿè®¡åˆ†æ {len(selected_fields)} ä¸ªå­—æ®µ")
        
        # å¯¼å…¥åˆ†ææ¨¡å—
        try:
            from universal_questionnaire_analyzer import UniversalQuestionnaireAnalyzer
            logger.info("âœ… æˆåŠŸå¯¼å…¥ UniversalQuestionnaireAnalyzer")
        except ImportError as e:
            logger.error(f"âŒ å¯¼å…¥ UniversalQuestionnaireAnalyzer å¤±è´¥: {e}")
            return jsonify({'error': f'å¯¼å…¥åˆ†ææ¨¡å—å¤±è´¥: {str(e)}'}), 500
        
        analyzer = UniversalQuestionnaireAnalyzer()
        
        # ç­›é€‰é€‰ä¸­çš„å­—æ®µ
        available_fields = [col for col in selected_fields if col in df.columns]
        if not available_fields:
            return jsonify({'error': 'é€‰ä¸­çš„å­—æ®µåœ¨æ•°æ®ä¸­ä¸å­˜åœ¨'}), 400
        
        filtered_df = df[available_fields]
        
        # é‡æ–°è¯†åˆ«é—®é¢˜ç±»å‹ï¼ˆåŸºäºé€‰ä¸­çš„å­—æ®µï¼‰
        field_question_types = analyzer.identify_all_question_types(filtered_df)
        
        # ç”Ÿæˆç»Ÿè®¡åˆ†æç»“æœ
        analysis_result = {
            'summary': {
                'totalFields': len(available_fields),
                'analyzedQuestions': len(available_fields),
                'totalResponses': len(filtered_df)
            },
            'scaleQuestions': [],
            'singleChoiceQuestions': [],
                        'multipleChoiceQuestions': [],
            'openEndedQuestions': [],
            'crossAnalysis': None
        }
        
        # å¤„ç†é‡è¡¨é¢˜
        scale_questions = field_question_types.get('scale_questions', [])
        for q in scale_questions:
            col = q['column']
            if col in filtered_df.columns:
                data = filtered_df[col].dropna()
                if len(data) > 0:
                    # è½¬æ¢ä¸ºæ•°å€¼
                    numeric_data = pd.to_numeric(data, errors='coerce').dropna()
                    if len(numeric_data) > 0:
                        # åŸºç¡€ç»Ÿè®¡
                        stats = {
                            'count': len(numeric_data),
                            'mean': float(numeric_data.mean()),
                            'std': float(numeric_data.std()),
                            'min': float(numeric_data.min()),
                            'max': float(numeric_data.max()),
                            'median': float(numeric_data.median())
                        }
                        
                        # åˆ†å¸ƒç»Ÿè®¡
                        distribution = {}
                        value_counts = numeric_data.value_counts().sort_index()
                        total = len(numeric_data)
                        for score, count in value_counts.items():
                            distribution[str(int(score))] = {
                                'count': int(count),
                                'percentage': float(count / total * 100)
                            }
                        
                        # NPSåˆ†æï¼ˆé€‚ç”¨äº1-10åˆ†åˆ¶ï¼‰
                        nps_analysis = None
                        if stats['max'] <= 10 and stats['min'] >= 1:
                            promoters = len(numeric_data[numeric_data >= 9])
                            passives = len(numeric_data[(numeric_data >= 7) & (numeric_data <= 8)])
                            detractors = len(numeric_data[numeric_data <= 6])
                            
                            nps_score = (promoters - detractors) / total * 100
                            
                            nps_analysis = {
                                'promoters': {'count': promoters, 'percentage': promoters / total * 100},
                                'passives': {'count': passives, 'percentage': passives / total * 100},
                                'detractors': {'count': detractors, 'percentage': detractors / total * 100},
                                'nps': nps_score,
                                'evaluation': 'ä¼˜ç§€' if nps_score > 50 else 'è‰¯å¥½' if nps_score > 0 else 'éœ€æ”¹è¿›'
                            }
                        
                        analysis_result['scaleQuestions'].append({
                            'column': col,
                            'statistics': stats,
                            'distribution': distribution,
                            'npsAnalysis': nps_analysis
                        })
        
        # å¤„ç†å•é€‰é¢˜
        single_choice = field_question_types.get('single_choice', [])
        for q in single_choice:
            col = q['column']
            if col in filtered_df.columns:
                data = filtered_df[col].dropna()
                if len(data) > 0:
                    value_counts = data.value_counts()
                    total = len(data)
                    
                    options = []
                    for option, count in value_counts.items():
                        options.append({
                            'option': str(option),
                            'count': int(count),
                            'percentage': float(count / total * 100)
                        })
                    
                    most_selected = options[0] if options else None
                    
                    analysis_result['singleChoiceQuestions'].append({
                        'column': col,
                        'validResponses': total,
                        'totalOptions': len(options),
                        'options': options,
                        'mostSelected': most_selected
                    })
        
        # å¤„ç†å¼€æ”¾é¢˜
        open_ended = field_question_types.get('open_ended', [])
        for q in open_ended:
            col = q['column']
            if col in filtered_df.columns:
                data = filtered_df[col].dropna()
                if len(data) > 0:
                    # åŸºç¡€ç»Ÿè®¡
                    avg_length = data.astype(str).str.len().mean()
                    unique_count = len(data.unique())
                    uniqueness_ratio = unique_count / len(data)
                    
                    # æå–å…³é”®è¯ï¼ˆç®€å•çš„è¯é¢‘ç»Ÿè®¡ï¼‰
                    text_data = ' '.join(data.astype(str))
                    words = text_data.split()
                    word_freq = pd.Series(words).value_counts().head(10)
                    
                    top_keywords = []
                    for word, count in word_freq.items():
                        if len(word) > 1:  # è¿‡æ»¤å•å­—ç¬¦
                            top_keywords.append({'word': word, 'count': int(count)})
                    
                    # ç¤ºä¾‹å›ç­”
                    sample_responses = data.head(5).tolist()
                    
                    analysis_result['openEndedQuestions'].append({
                        'column': col,
                        'validResponses': len(data),
                        'statistics': {
                            'averageLength': avg_length,
                            'uniqueCount': unique_count,
                            'uniquenessRatio': uniqueness_ratio
                        },
                        'topKeywords': top_keywords[:8],
                        'sampleResponses': sample_responses
                    })
        

        # å¤„ç†å¤šé€‰é¢˜
        multiple_choice = field_question_types.get('multiple_choice', {})
        for question_stem, options in multiple_choice.items():
            # æå–æ‰€æœ‰é€‰é¡¹çš„åˆ—å
            option_columns = [opt['full_column'] for opt in options if opt['full_column'] in filtered_df.columns]
            
            if option_columns:
                # åˆ›å»ºå¤šé€‰é¢˜åˆ†æç»“æœ
                multi_question = {
                    'column': question_stem,  # ä¸å•é€‰é¢˜ä¿æŒä¸€è‡´ï¼Œä½¿ç”¨columnè€Œä¸æ˜¯questionStem
                    'totalOptions': len(option_columns),
                    'validResponses': len(filtered_df),  # ä¸å•é€‰é¢˜ä¿æŒä¸€è‡´ï¼Œä½¿ç”¨validResponsesè€Œä¸æ˜¯totalResponses
                    'options': []
                }
                
                # è®¡ç®—æ€»å“åº”æ•°ï¼ˆè‡³å°‘é€‰æ‹©äº†ä¸€ä¸ªé€‰é¡¹çš„è¡Œæ•°ï¼‰
                respondents = 0
                for i, row in filtered_df[option_columns].iterrows():
                    if row.notna().any():
                        respondents += 1
                
                multi_question['validResponses'] = respondents if respondents > 0 else len(filtered_df)
                
                # å¤„ç†æ¯ä¸ªé€‰é¡¹
                for opt_info in options:
                    col = opt_info['full_column']
                    if col in filtered_df.columns:
                        # è·å–é€‰é¡¹æ–‡æœ¬
                        option_text = opt_info['option']
                        
                        # ç»Ÿè®¡é€‰æ‹©è¯¥é€‰é¡¹çš„äººæ•°
                        try:
                            # å°è¯•å°†åˆ—è½¬æ¢ä¸ºå¸ƒå°”å€¼ï¼ˆé’ˆå¯¹0/1ç¼–ç çš„å¤šé€‰é¢˜ï¼‰
                            data = filtered_df[col].fillna(0).astype(int).astype(bool)
                            selected_count = data.sum()
                        except:
                            # å¦‚æœå¤±è´¥ï¼Œåˆ™æŒ‰éç©ºå€¼è®¡æ•°
                            selected_count = filtered_df[col].notna().sum()
                        
                        # è®¡ç®—ç™¾åˆ†æ¯”
                        percentage = (selected_count / multi_question['validResponses'] * 100) if multi_question['validResponses'] > 0 else 0
                        
                        # æ·»åŠ é€‰é¡¹åˆ†æ - ä¸å•é€‰é¢˜ä¿æŒä¸€è‡´çš„æ ¼å¼
                        multi_question['options'].append({
                            'option': option_text,
                            'count': int(selected_count),
                            'percentage': float(percentage)
                        })
                
                # æ·»åŠ æœ€å¤šé€‰æ‹©çš„é€‰é¡¹ - ä¸å•é€‰é¢˜ä¿æŒä¸€è‡´
                if multi_question['options']:
                    # æ’åºé€‰é¡¹ï¼ˆæŒ‰é€‰æ‹©æ•°é‡é™åºï¼‰
                    sorted_options = sorted(multi_question['options'], key=lambda x: x['count'], reverse=True)
                    multi_question['mostSelected'] = sorted_options[0]
                    
                    # ä¿ç•™summaryå­—æ®µï¼Œä½†ç¡®ä¿æ ¼å¼ä¸€è‡´
                    multi_question['summary'] = {
                        'mostSelected': sorted_options[0],
                        'leastSelected': sorted_options[-1],
                        'averageSelectionRate': sum(opt['percentage'] for opt in multi_question['options']) / len(multi_question['options'])
                    }
                
                # æ·»åŠ åˆ°ç»“æœé›†
                analysis_result['multipleChoiceQuestions'].append(multi_question)
        
        # å­˜å‚¨åˆ†æç»“æœ
        analysis_results[analysis_id]['statistics_result'] = analysis_result
        
        logger.info(f"âœ… ç»Ÿè®¡åˆ†æå®Œæˆï¼Œåˆ†æID: {analysis_id}")
        return jsonify({
            'results': convert_pandas_types(analysis_result),
            'analysisId': analysis_id
        })
        
    except Exception as e:
        logger.error(f"âŒ ç»Ÿè®¡åˆ†æå¤±è´¥: {e}")
        logger.error(traceback.format_exc())
        return jsonify({'error': f'ç»Ÿè®¡åˆ†æå¤±è´¥: {str(e)}'}), 500

@app.route('/export/<analysis_id>', methods=['GET'])
def export_results(analysis_id):
    """å¯¼å‡ºåˆ†æç»“æœ"""
    try:
        format_type = request.args.get('format', 'csv')
        
        if analysis_id not in analysis_results:
            return jsonify({'error': 'åˆ†æIDä¸å­˜åœ¨'}), 404
        
        analysis_info = analysis_results[analysis_id]
        processed_file = analysis_info.get('processed_file')
        
        if not processed_file or not os.path.exists(processed_file):
            return jsonify({'error': 'å¤„ç†åçš„æ–‡ä»¶ä¸å­˜åœ¨'}), 404
        
        # è¿”å›å¤„ç†åçš„æ–‡ä»¶
        return send_file(
            processed_file,
            as_attachment=True,
            download_name=f"analysis_{analysis_id}.{format_type}"
        )
        
    except Exception as e:
        logger.error(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")
        return jsonify({'error': f'å¯¼å‡ºå¤±è´¥: {str(e)}'}), 500

# @app.route('/download-classification/<analysis_id>', methods=['GET'])
# def download_classification(analysis_id):
#     """ä¸‹è½½Classificationå¤„ç†åçš„æ–‡ä»¶"""
#     try:
#         if analysis_id not in analysis_results:
#             return jsonify({'error': 'åˆ†æIDä¸å­˜åœ¨'}), 404
        
#         analysis_info = analysis_results[analysis_id]
#         classification_output = analysis_info.get('classification_output')
        
#         if not classification_output or not os.path.exists(classification_output):
#             return jsonify({'error': 'Classificationå¤„ç†ç»“æœæ–‡ä»¶ä¸å­˜åœ¨'}), 404
        
#         # ä½¿ç”¨å®é™…æ–‡ä»¶åè€Œä¸æ˜¯ç¡¬ç¼–ç çš„åç§°
#         actual_filename = Path(classification_output).name
        
#         return send_file(
#             classification_output,
#             as_attachment=True,
#             download_name=actual_filename,
#             mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
#         )
        
#     except Exception as e:
#         logger.error(f"âŒ ä¸‹è½½Classificationç»“æœå¤±è´¥: {e}")
#         return jsonify({'error': f'ä¸‹è½½Classificationç»“æœå¤±è´¥: {str(e)}'}), 500

@app.route('/get-ai-tags-for-editing/<analysis_id>', methods=['GET'])
def get_ai_tags_for_editing(analysis_id):
    """è·å–æ ‡å‡†AIæ‰“æ ‡åçš„å¯ç¼–è¾‘æ ‡ç­¾æ•°æ®"""
    try:
        logger.info(f"ğŸ” æ”¶åˆ°æ ‡å‡†AIæ‰“æ ‡ç¼–è¾‘è¯·æ±‚ï¼Œanalysis_id: {analysis_id}")
        
        if analysis_id not in analysis_results:
            logger.error(f"âŒ åˆ†æID {analysis_id} ä¸å­˜åœ¨äºanalysis_resultsä¸­")
            return jsonify({'error': 'åˆ†æIDä¸å­˜åœ¨'}), 404
        
        analysis_info = analysis_results[analysis_id]
        
        # ä¸“é—¨è¯»å–æ ‡å‡†AIæ‰“æ ‡æ–‡ä»¶
        standard_labeling_output = analysis_info.get('standard_labeling_output')
        manual_ai_output = analysis_info.get('manual_ai_output')  # æ‰‹åŠ¨ä¿®æ”¹åçš„AIæ‰“æ ‡æ–‡ä»¶
        
        input_file = None
        use_tag_columns = False
        
        # ä¼˜å…ˆè¯»å–æ‰‹åŠ¨ä¿®æ”¹åçš„AIæ‰“æ ‡æ–‡ä»¶ï¼Œå…¶æ¬¡è¯»å–åŸå§‹AIæ‰“æ ‡æ–‡ä»¶
        if manual_ai_output and os.path.exists(manual_ai_output):
            input_file = manual_ai_output
            use_tag_columns = False  # AIæ‰“æ ‡ä½¿ç”¨ä¸€çº§ä¸»é¢˜å’ŒäºŒçº§æ ‡ç­¾
            logger.info(f"ğŸ“ è¯»å–æ‰‹åŠ¨ä¿®æ”¹åçš„AIæ‰“æ ‡æ–‡ä»¶: {input_file}")
        elif standard_labeling_output and os.path.exists(standard_labeling_output):
            input_file = standard_labeling_output
            use_tag_columns = False  # AIæ‰“æ ‡ä½¿ç”¨ä¸€çº§ä¸»é¢˜å’ŒäºŒçº§æ ‡ç­¾
            logger.info(f"ğŸ“ è¯»å–æ ‡å‡†AIæ‰“æ ‡æ–‡ä»¶: {input_file}")
        else:
            return jsonify({'error': 'æ²¡æœ‰æ‰¾åˆ°æ ‡å‡†AIæ‰“æ ‡ç»“æœæ–‡ä»¶'}), 404
        
        return _process_tags_for_editing(analysis_id, input_file, use_tag_columns, "AIæ‰“æ ‡")
        
    except Exception as e:
        logger.error(f"âŒ è·å–AIæ‰“æ ‡ç¼–è¾‘æ•°æ®å¤±è´¥: {e}")
        return jsonify({'error': f'è·å–AIæ‰“æ ‡ç¼–è¾‘æ•°æ®å¤±è´¥: {str(e)}'}), 500

@app.route('/get-custom-tags-for-editing/<analysis_id>', methods=['GET'])
def get_custom_tags_for_editing(analysis_id):
    """è·å–å‚è€ƒæ ‡ç­¾æ‰“æ ‡åçš„å¯ç¼–è¾‘æ ‡ç­¾æ•°æ®"""
    try:
        logger.info(f"ğŸ” æ”¶åˆ°å‚è€ƒæ ‡ç­¾æ‰“æ ‡ç¼–è¾‘è¯·æ±‚ï¼Œanalysis_id: {analysis_id}")
        
        if analysis_id not in analysis_results:
            logger.error(f"âŒ åˆ†æID {analysis_id} ä¸å­˜åœ¨äºanalysis_resultsä¸­")
            return jsonify({'error': 'åˆ†æIDä¸å­˜åœ¨'}), 404
        
        analysis_info = analysis_results[analysis_id]
        
        # ä¸“é—¨è¯»å–å‚è€ƒæ ‡ç­¾æ‰“æ ‡æ–‡ä»¶
        custom_labeling_output = analysis_info.get('custom_labeling_output')
        manual_custom_output = analysis_info.get('manual_custom_output')  # æ‰‹åŠ¨ä¿®æ”¹åçš„å‚è€ƒæ ‡ç­¾æ–‡ä»¶
        
        input_file = None
        use_tag_columns = True
        
        # ä¼˜å…ˆè¯»å–æ‰‹åŠ¨ä¿®æ”¹åçš„å‚è€ƒæ ‡ç­¾æ–‡ä»¶ï¼Œå…¶æ¬¡è¯»å–åŸå§‹å‚è€ƒæ ‡ç­¾æ–‡ä»¶
        if manual_custom_output and os.path.exists(manual_custom_output):
            input_file = manual_custom_output
            use_tag_columns = True  # å‚è€ƒæ ‡ç­¾ä½¿ç”¨æ ‡ç­¾åˆ—
            logger.info(f"ğŸ“ è¯»å–æ‰‹åŠ¨ä¿®æ”¹åçš„å‚è€ƒæ ‡ç­¾æ–‡ä»¶: {input_file}")
        elif custom_labeling_output and os.path.exists(custom_labeling_output):
            input_file = custom_labeling_output
            use_tag_columns = True  # å‚è€ƒæ ‡ç­¾ä½¿ç”¨æ ‡ç­¾åˆ—
            logger.info(f"ğŸ“ è¯»å–å‚è€ƒæ ‡ç­¾æ‰“æ ‡æ–‡ä»¶: {input_file}")
        else:
            # å°è¯•åœ¨translate_customç›®å½•ä¸­æœç´¢å¤‡ç”¨æ–‡ä»¶
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°å‚è€ƒæ ‡ç­¾æ–‡ä»¶ï¼Œå°è¯•æœç´¢å¤‡ç”¨æ–‡ä»¶")
            try:
                custom_files = list(TRANSLATE_CUSTOM_FOLDER.glob(f"*{analysis_id.split('-')[0]}*_translate_custom_*.xlsx"))
                if custom_files:
                    latest_custom_file = max(custom_files, key=os.path.getmtime)
                    input_file = str(latest_custom_file)
                    use_tag_columns = True
                    # æ›´æ–°analysis_resultsä¸­çš„è·¯å¾„
                    analysis_results[analysis_id]['custom_labeling_output'] = str(latest_custom_file)
                    logger.info(f"ğŸ”§ æ‰¾åˆ°å¤‡ç”¨å‚è€ƒæ ‡ç­¾æ–‡ä»¶: {input_file}")
                else:
                    return jsonify({'error': 'æ²¡æœ‰æ‰¾åˆ°å‚è€ƒæ ‡ç­¾æ‰“æ ‡ç»“æœæ–‡ä»¶ï¼Œè¯·å…ˆæ‰§è¡Œå‚è€ƒæ ‡ç­¾æ‰“æ ‡'}), 404
            except Exception as search_error:
                logger.error(f"âŒ æœç´¢å¤‡ç”¨æ–‡ä»¶å¤±è´¥: {search_error}")
                return jsonify({'error': 'æ²¡æœ‰æ‰¾åˆ°å‚è€ƒæ ‡ç­¾æ‰“æ ‡ç»“æœæ–‡ä»¶'}), 404
        
        return _process_tags_for_editing(analysis_id, input_file, use_tag_columns, "å‚è€ƒæ ‡ç­¾æ‰“æ ‡")
        
    except Exception as e:
        logger.error(f"âŒ è·å–å‚è€ƒæ ‡ç­¾ç¼–è¾‘æ•°æ®å¤±è´¥: {e}")
        return jsonify({'error': f'è·å–å‚è€ƒæ ‡ç­¾ç¼–è¾‘æ•°æ®å¤±è´¥: {str(e)}'}), 500

def _process_tags_for_editing(analysis_id, input_file, use_tag_columns, edit_type):
    """å¤„ç†æ ‡ç­¾ç¼–è¾‘æ•°æ®çš„é€šç”¨å‡½æ•°"""
    try:
        analysis_info = analysis_results[analysis_id]
        
        # è¯»å–æ–‡ä»¶
        df = pd.read_excel(input_file)
        logger.info(f"ğŸ“Š è¯»å–åˆ°çš„{edit_type}æ–‡ä»¶åˆ—æ•°: {len(df.columns)}")
        logger.info(f"ğŸ“Š æ–‡ä»¶åˆ—å: {list(df.columns)}")
        
        # è·å–å¼€æ”¾é¢˜å­—æ®µ
        open_ended_fields = []
        if 'question_types' in analysis_info:
            question_types = analysis_info['question_types']
            open_ended_list = question_types.get('open_ended', [])
            for q in open_ended_list:
                if isinstance(q, dict) and 'column' in q:
                    open_ended_fields.append(q['column'])
        
        logger.info(f"ğŸ“Š å¼€æ”¾é¢˜å­—æ®µ: {open_ended_fields}")
        
        # æ„å»ºå¼€æ”¾é¢˜å­—æ®µçš„ç¼–è¾‘æ•°æ®ç»“æ„
        open_questions = []
        
        for open_field in open_ended_fields:
            question_data = {
                'original_field': open_field,
                'cn_field': open_field + '-CN',
                'level1_theme_field': open_field + 'ä¸€çº§ä¸»é¢˜',
                'level2_tag_field': open_field + 'äºŒçº§æ ‡ç­¾',
                'reference_tag_field': open_field + 'æ ‡ç­¾',
                'available_columns': []
            }
            
            # æ£€æŸ¥å„åˆ—æ˜¯å¦å­˜åœ¨
            if open_field in df.columns:
                question_data['available_columns'].append(open_field)
            if question_data['cn_field'] in df.columns:
                question_data['available_columns'].append(question_data['cn_field'])
            if question_data['level1_theme_field'] in df.columns:
                question_data['available_columns'].append(question_data['level1_theme_field'])
            if question_data['level2_tag_field'] in df.columns:
                question_data['available_columns'].append(question_data['level2_tag_field'])
            if question_data['reference_tag_field'] in df.columns:
                question_data['available_columns'].append(question_data['reference_tag_field'])
            
            # è°ƒè¯•æ—¥å¿—ï¼šè¾“å‡ºæ¯ä¸ªé—®é¢˜çš„å­—æ®µä¿¡æ¯
            logger.info(f"ğŸ“Š {edit_type}é—®é¢˜ '{open_field}' çš„å­—æ®µæ£€æŸ¥:")
            logger.info(f"   - åŸå­—æ®µ: {open_field} -> {'å­˜åœ¨' if open_field in df.columns else 'ä¸å­˜åœ¨'}")
            logger.info(f"   - CNå­—æ®µ: {question_data['cn_field']} -> {'å­˜åœ¨' if question_data['cn_field'] in df.columns else 'ä¸å­˜åœ¨'}")
            logger.info(f"   - ä¸€çº§ä¸»é¢˜å­—æ®µ: {question_data['level1_theme_field']} -> {'å­˜åœ¨' if question_data['level1_theme_field'] in df.columns else 'ä¸å­˜åœ¨'}")
            logger.info(f"   - äºŒçº§æ ‡ç­¾å­—æ®µ: {question_data['level2_tag_field']} -> {'å­˜åœ¨' if question_data['level2_tag_field'] in df.columns else 'ä¸å­˜åœ¨'}")
            logger.info(f"   - å‚è€ƒæ ‡ç­¾å­—æ®µ: {question_data['reference_tag_field']} -> {'å­˜åœ¨' if question_data['reference_tag_field'] in df.columns else 'ä¸å­˜åœ¨'}")
            
            # åªæœ‰å½“è‡³å°‘æœ‰åŸå­—æ®µæ—¶æ‰æ·»åŠ 
            if open_field in df.columns:
                open_questions.append(question_data)
        
        if not open_questions:
            logger.error(f"âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç¼–è¾‘çš„å¼€æ”¾é¢˜å­—æ®µ")
            return jsonify({'error': 'æ²¡æœ‰æ‰¾åˆ°å¯ç¼–è¾‘çš„å¼€æ”¾é¢˜å­—æ®µ'}), 400
        
        # å‡†å¤‡ç¼–è¾‘æ•°æ®
        edit_data = []
        for index, row in df.iterrows():
            row_data = {
                'id': index,
                'questions': {}
            }
            
            # ä¸ºæ¯ä¸ªå¼€æ”¾é¢˜å­—æ®µå‡†å¤‡æ•°æ®
            for question in open_questions:
                question_data = {
                    'original_text': str(row[question['original_field']]) if pd.notna(row[question['original_field']]) else '',
                    'cn_text': str(row[question['cn_field']]) if question['cn_field'] in df.columns and pd.notna(row[question['cn_field']]) else '',
                    'level1_themes': [],
                    'level2_tags': [],
                    'reference_tags': []
                }
                
                # å¤„ç†ä¸€çº§ä¸»é¢˜
                if question['level1_theme_field'] in df.columns:
                    theme_str = str(row[question['level1_theme_field']]) if pd.notna(row[question['level1_theme_field']]) else ''
                    if theme_str:
                        if ',' in theme_str:
                            question_data['level1_themes'] = [tag.strip() for tag in theme_str.split(',') if tag.strip()]
                        else:
                            question_data['level1_themes'] = [theme_str.strip()] if theme_str.strip() else []
                
                # å¤„ç†äºŒçº§æ ‡ç­¾
                if question['level2_tag_field'] in df.columns:
                    tags_str = str(row[question['level2_tag_field']]) if pd.notna(row[question['level2_tag_field']]) else ''
                    if tags_str:
                        question_data['level2_tags'] = [tag.strip() for tag in tags_str.split(',') if tag.strip()]
                
                # å¤„ç†å‚è€ƒæ ‡ç­¾
                if question['reference_tag_field'] in df.columns:
                    ref_tags_str = str(row[question['reference_tag_field']]) if pd.notna(row[question['reference_tag_field']]) else ''
                    if ref_tags_str:
                        question_data['reference_tags'] = [tag.strip() for tag in ref_tags_str.split(',') if tag.strip()]
                
                row_data['questions'][question['original_field']] = question_data
            
            edit_data.append(row_data)
        
        # ç»Ÿè®¡æ ‡ç­¾ä½¿ç”¨é¢‘ç‡
        tag_statistics = {}
        for question in open_questions:
            question_stats = {}
            
            # ç»Ÿè®¡ä¸€çº§ä¸»é¢˜
            if question['level1_theme_field'] in df.columns:
                level1_tags = []
                for theme_str in df[question['level1_theme_field']].fillna('').astype(str):
                    if theme_str:
                        if ',' in theme_str:
                            level1_tags.extend([tag.strip() for tag in theme_str.split(',') if tag.strip()])
                        else:
                            if theme_str.strip():
                                level1_tags.append(theme_str.strip())
                
                from collections import Counter
                question_stats['level1_themes'] = dict(Counter(level1_tags))
            
            # ç»Ÿè®¡äºŒçº§æ ‡ç­¾
            if question['level2_tag_field'] in df.columns:
                level2_tags = []
                for tags_str in df[question['level2_tag_field']].fillna('').astype(str):
                    if tags_str:
                        level2_tags.extend([tag.strip() for tag in tags_str.split(',') if tag.strip()])
                
                from collections import Counter
                question_stats['level2_tags'] = dict(Counter(level2_tags))
            
            # ç»Ÿè®¡å‚è€ƒæ ‡ç­¾
            if question['reference_tag_field'] in df.columns:
                reference_tags_list = []
                for ref_tags_str in df[question['reference_tag_field']].fillna('').astype(str):
                    if ref_tags_str:
                        reference_tags_list.extend([tag.strip() for tag in ref_tags_str.split(',') if tag.strip()])
                
                from collections import Counter
                question_stats['reference_tags'] = dict(Counter(reference_tags_list))
            
            tag_statistics[question['original_field']] = question_stats
        
        # è·å–å‚è€ƒæ ‡ç­¾
        reference_tags = []
        if 'retag_result' in analysis_info:
            reference_tags = analysis_info['retag_result'].get('reference_tags', [])
        elif 'custom_labeling_result' in analysis_info:
            reference_tags = analysis_info['custom_labeling_result'].get('reference_tags', [])
        
        # ç”Ÿæˆæ–‡ä»¶é¢„è§ˆæ•°æ®
        preview_data = []
        for col in df.columns:
            field_data = {
                'field': col,
                'values': df[col].fillna('').astype(str).tolist()
            }
            preview_data.append(field_data)

        result = {
            'data': edit_data,  # TagEditoræ ¼å¼çš„æ•°æ®
            'processed_data': preview_data,  # æ–‡ä»¶é¢„è§ˆæ ¼å¼çš„æ•°æ®
            'open_questions': open_questions,
            'reference_tags': reference_tags,
            'tag_statistics': tag_statistics,
            'total_rows': len(df),
            'edit_type': edit_type,  # æ ‡è¯†ç¼–è¾‘ç±»å‹
            'file_info': {
                'filename': os.path.basename(input_file),
                'columns': df.columns.tolist(),
                'shape': df.shape
            }
        }
        
        logger.info(f"âœ… è·å–{edit_type}ç¼–è¾‘æ•°æ®æˆåŠŸï¼Œåˆ†æID: {analysis_id}")
        return jsonify(convert_pandas_types(result))
        
    except Exception as e:
        logger.error(f"âŒ å¤„ç†{edit_type}ç¼–è¾‘æ•°æ®å¤±è´¥: {e}")
        import traceback
        logger.error(f"âŒ å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
        return jsonify({'error': f'å¤„ç†{edit_type}ç¼–è¾‘æ•°æ®å¤±è´¥: {str(e)}'}), 500

@app.route('/get-tags-for-editing/<analysis_id>', methods=['GET'])
def get_tags_for_editing(analysis_id):
    """è·å–å¯ç¼–è¾‘çš„æ ‡ç­¾æ•°æ®"""
    try:
        logger.info(f"ğŸ” æ”¶åˆ°æ‰‹åŠ¨ç¼–è¾‘è¯·æ±‚ï¼Œanalysis_id: {analysis_id}")
        logger.info(f"ğŸ“Š å½“å‰analysis_resultsä¸­çš„IDåˆ—è¡¨: {list(analysis_results.keys())}")
        
        if analysis_id not in analysis_results:
            logger.error(f"âŒ åˆ†æID {analysis_id} ä¸å­˜åœ¨äºanalysis_resultsä¸­")
            return jsonify({'error': 'åˆ†æIDä¸å­˜åœ¨'}), 404
        
        analysis_info = analysis_results[analysis_id]
        logger.info(f"ğŸ“Š analysis_infoçš„keys: {list(analysis_info.keys())}")
        
        # ä¼˜å…ˆè¯»å–æ‰‹åŠ¨ä¿®æ”¹åçš„æ–‡ä»¶ï¼Œå…¶æ¬¡è¯»å–æ‰“æ ‡æ–‡ä»¶ï¼Œæœ€åä½¿ç”¨åŸå§‹åˆ†ææ–‡ä»¶
        manual_output = analysis_info.get('manual_output')
        standard_labeling_output = analysis_info.get('standard_labeling_output')
        custom_labeling_output = analysis_info.get('custom_labeling_output')
        retag_output = analysis_info.get('retag_output')  # å‘åå…¼å®¹
        classification_output = analysis_info.get('classification_output')  # å‘åå…¼å®¹
        original_output = analysis_info.get('output_file') or analysis_info.get('processed_file')  # å…¼å®¹å¤šç§å­—æ®µå
        logger.info(f"ğŸ“ æ–‡ä»¶è·¯å¾„æ£€æŸ¥: manual_output={manual_output}, standard_labeling_output={standard_labeling_output}, custom_labeling_output={custom_labeling_output}, retag_output={retag_output}, classification_output={classification_output}, original_output={original_output}")
        
        if manual_output and os.path.exists(manual_output):
            input_file = manual_output
            use_tag_columns = True
            logger.info(f"ğŸ“ è¯»å–æ‰‹åŠ¨ä¿®æ”¹åçš„æ–‡ä»¶: {input_file}")
        elif custom_labeling_output and os.path.exists(custom_labeling_output):
            # å¼ºåˆ¶ä¼˜å…ˆè¯»å–é…ç½®å‚è€ƒæ ‡ç­¾æ–‡ä»¶
            input_file = custom_labeling_output
            use_tag_columns = True
            logger.info(f"ğŸ“ ä¼˜å…ˆè¯»å–é…ç½®å‚è€ƒæ ‡ç­¾æ‰“æ ‡æ–‡ä»¶: {input_file}")
        elif standard_labeling_output and os.path.exists(standard_labeling_output):
            input_file = standard_labeling_output
            use_tag_columns = False
            logger.info(f"ğŸ“ è¯»å–æ ‡å‡†AIæ‰“æ ‡æ–‡ä»¶: {input_file}")
        elif retag_output and os.path.exists(retag_output):
            input_file = retag_output
            use_tag_columns = True
            logger.info(f"ğŸ“ è¯»å–é‡æ–°æ‰“æ ‡çš„æ–‡ä»¶: {input_file}")
        elif classification_output and os.path.exists(classification_output):
            input_file = classification_output
            use_tag_columns = False
            logger.info(f"ğŸ“ è¯»å–åˆ†æé…ç½®æ–‡ä»¶: {input_file} (å°†ä½¿ç”¨ä¸€çº§ä¸»é¢˜åˆ—ä½œä¸ºæ ‡ç­¾)")
        elif original_output and os.path.exists(original_output):
            input_file = original_output
            use_tag_columns = False
            logger.info(f"ğŸ“ è¯»å–åŸå§‹åˆ†ææ–‡ä»¶: {input_file} (å°†ä½¿ç”¨ä¸€çº§ä¸»é¢˜åˆ—ä½œä¸ºæ ‡ç­¾)")
        else:
            return jsonify({'error': 'æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„åˆ†æç»“æœæ–‡ä»¶'}), 404
        
        # è¯»å–æ–‡ä»¶
        df = pd.read_excel(input_file)
        logger.info(f"ğŸ“Š è¯»å–åˆ°çš„æ–‡ä»¶åˆ—æ•°: {len(df.columns)}")
        logger.info(f"ğŸ“Š æ–‡ä»¶åˆ—å: {list(df.columns)}")
        
        cn_columns = [col for col in df.columns if col.endswith('-CN')]
        logger.info(f"ğŸ“Š CNåˆ—: {cn_columns}")
        
        # è·å–å¼€æ”¾é¢˜å­—æ®µ
        open_ended_fields = []
        if 'question_types' in analysis_info:
            question_types = analysis_info['question_types']
            open_ended_list = question_types.get('open_ended', [])
            for q in open_ended_list:
                if isinstance(q, dict) and 'column' in q:
                    open_ended_fields.append(q['column'])
        
        logger.info(f"ğŸ“Š å¼€æ”¾é¢˜å­—æ®µ: {open_ended_fields}")
        
        # æ„å»ºå¼€æ”¾é¢˜å­—æ®µçš„ç¼–è¾‘æ•°æ®ç»“æ„
        # ä¸ºæ¯ä¸ªå¼€æ”¾é¢˜å­—æ®µæ‰¾åˆ°å¯¹åº”çš„ç›¸å…³åˆ—ï¼ˆåŸå­—æ®µã€CNç¿»è¯‘ã€äºŒçº§æ ‡ç­¾ã€ä¸€çº§ä¸»é¢˜ï¼‰
        open_questions = []
        
        for open_field in open_ended_fields:
            question_data = {
                'original_field': open_field,
                'cn_field': open_field + '-CN',
                'level1_theme_field': open_field + 'ä¸€çº§ä¸»é¢˜',
                'level2_tag_field': open_field + 'äºŒçº§æ ‡ç­¾',
                'reference_tag_field': open_field + 'æ ‡ç­¾',
                'available_columns': []
            }
            
            # æ£€æŸ¥å„åˆ—æ˜¯å¦å­˜åœ¨
            if open_field in df.columns:
                question_data['available_columns'].append(open_field)
            if question_data['cn_field'] in df.columns:
                question_data['available_columns'].append(question_data['cn_field'])
            if question_data['level1_theme_field'] in df.columns:
                question_data['available_columns'].append(question_data['level1_theme_field'])
            if question_data['level2_tag_field'] in df.columns:
                question_data['available_columns'].append(question_data['level2_tag_field'])
            if question_data['reference_tag_field'] in df.columns:
                question_data['available_columns'].append(question_data['reference_tag_field'])
            
            # è°ƒè¯•æ—¥å¿—ï¼šè¾“å‡ºæ¯ä¸ªé—®é¢˜çš„å­—æ®µä¿¡æ¯
            logger.info(f"ğŸ“Š é—®é¢˜ '{open_field}' çš„å­—æ®µæ£€æŸ¥:")
            logger.info(f"   - åŸå­—æ®µ: {open_field} -> {'å­˜åœ¨' if open_field in df.columns else 'ä¸å­˜åœ¨'}")
            logger.info(f"   - CNå­—æ®µ: {question_data['cn_field']} -> {'å­˜åœ¨' if question_data['cn_field'] in df.columns else 'ä¸å­˜åœ¨'}")
            logger.info(f"   - ä¸€çº§ä¸»é¢˜å­—æ®µ: {question_data['level1_theme_field']} -> {'å­˜åœ¨' if question_data['level1_theme_field'] in df.columns else 'ä¸å­˜åœ¨'}")
            logger.info(f"   - äºŒçº§æ ‡ç­¾å­—æ®µ: {question_data['level2_tag_field']} -> {'å­˜åœ¨' if question_data['level2_tag_field'] in df.columns else 'ä¸å­˜åœ¨'}")
            logger.info(f"   - å‚è€ƒæ ‡ç­¾å­—æ®µ: {question_data['reference_tag_field']} -> {'å­˜åœ¨' if question_data['reference_tag_field'] in df.columns else 'ä¸å­˜åœ¨'}")
            logger.info(f"   - available_columns: {question_data['available_columns']}")
            
            # åªæœ‰å½“è‡³å°‘æœ‰åŸå­—æ®µæ—¶æ‰æ·»åŠ 
            if open_field in df.columns:
                open_questions.append(question_data)
        
        logger.info(f"ğŸ“Š æ„å»ºçš„å¼€æ”¾é¢˜å­—æ®µç»“æ„: {[q['original_field'] for q in open_questions]}")
        
        if not open_questions:
            logger.error(f"âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç¼–è¾‘çš„å¼€æ”¾é¢˜å­—æ®µ")
            return jsonify({'error': 'æ²¡æœ‰æ‰¾åˆ°å¯ç¼–è¾‘çš„å¼€æ”¾é¢˜å­—æ®µ'}), 400
        
        # å‡†å¤‡ç¼–è¾‘æ•°æ®
        edit_data = []
        for index, row in df.iterrows():
            row_data = {
                'id': index,
                'questions': {}
            }
            
            # ä¸ºæ¯ä¸ªå¼€æ”¾é¢˜å­—æ®µå‡†å¤‡æ•°æ®
            for question in open_questions:
                question_data = {
                    'original_text': str(row[question['original_field']]) if pd.notna(row[question['original_field']]) else '',
                    'cn_text': str(row[question['cn_field']]) if question['cn_field'] in df.columns and pd.notna(row[question['cn_field']]) else '',
                    'level1_themes': [],
                    'level2_tags': [],
                    'reference_tags': []
                }
                
                # å¤„ç†ä¸€çº§ä¸»é¢˜
                if question['level1_theme_field'] in df.columns:
                    theme_str = str(row[question['level1_theme_field']]) if pd.notna(row[question['level1_theme_field']]) else ''
                    if theme_str:
                        # ä¸€çº§ä¸»é¢˜å¯èƒ½æ˜¯é€—å·åˆ†éš”ï¼Œä¹Ÿå¯èƒ½æ˜¯å•ä¸ªå€¼
                        if ',' in theme_str:
                            question_data['level1_themes'] = [tag.strip() for tag in theme_str.split(',') if tag.strip()]
                        else:
                            question_data['level1_themes'] = [theme_str.strip()] if theme_str.strip() else []
                
                # å¤„ç†äºŒçº§æ ‡ç­¾
                if question['level2_tag_field'] in df.columns:
                    tags_str = str(row[question['level2_tag_field']]) if pd.notna(row[question['level2_tag_field']]) else ''
                    if tags_str:
                        question_data['level2_tags'] = [tag.strip() for tag in tags_str.split(',') if tag.strip()]
                
                # å¤„ç†å‚è€ƒæ ‡ç­¾
                if question['reference_tag_field'] in df.columns:
                    ref_tags_str = str(row[question['reference_tag_field']]) if pd.notna(row[question['reference_tag_field']]) else ''
                    if ref_tags_str:
                        question_data['reference_tags'] = [tag.strip() for tag in ref_tags_str.split(',') if tag.strip()]
                
                row_data['questions'][question['original_field']] = question_data
            
            edit_data.append(row_data)
        
        # ç»Ÿè®¡æ ‡ç­¾ä½¿ç”¨é¢‘ç‡
        tag_statistics = {}
        for question in open_questions:
            question_stats = {}
            
            # ç»Ÿè®¡ä¸€çº§ä¸»é¢˜
            if question['level1_theme_field'] in df.columns:
                level1_tags = []
                for theme_str in df[question['level1_theme_field']].fillna('').astype(str):
                    if theme_str:
                        if ',' in theme_str:
                            level1_tags.extend([tag.strip() for tag in theme_str.split(',') if tag.strip()])
                        else:
                            if theme_str.strip():
                                level1_tags.append(theme_str.strip())
                
                from collections import Counter
                question_stats['level1_themes'] = dict(Counter(level1_tags))
            
            # ç»Ÿè®¡äºŒçº§æ ‡ç­¾
            if question['level2_tag_field'] in df.columns:
                level2_tags = []
                for tags_str in df[question['level2_tag_field']].fillna('').astype(str):
                    if tags_str:
                        level2_tags.extend([tag.strip() for tag in tags_str.split(',') if tag.strip()])
                
                from collections import Counter
                question_stats['level2_tags'] = dict(Counter(level2_tags))
            
            # ç»Ÿè®¡å‚è€ƒæ ‡ç­¾
            if question['reference_tag_field'] in df.columns:
                reference_tags_list = []
                for ref_tags_str in df[question['reference_tag_field']].fillna('').astype(str):
                    if ref_tags_str:
                        reference_tags_list.extend([tag.strip() for tag in ref_tags_str.split(',') if tag.strip()])
                
                from collections import Counter
                question_stats['reference_tags'] = dict(Counter(reference_tags_list))
            
            tag_statistics[question['original_field']] = question_stats
        
        # è·å–å‚è€ƒæ ‡ç­¾
        reference_tags = []
        if 'retag_result' in analysis_info:
            reference_tags = analysis_info['retag_result'].get('reference_tags', [])
        
        # ç”Ÿæˆæ–‡ä»¶é¢„è§ˆæ•°æ®ï¼ˆç±»ä¼¼æ ‡å‡†AIæ‰“æ ‡ç»“æœæ ¼å¼ï¼‰
        preview_data = []
        for col in df.columns:
            field_data = {
                'field': col,
                'values': df[col].fillna('').astype(str).tolist()
            }
            preview_data.append(field_data)

        result = {
            'data': edit_data,  # TagEditoræ ¼å¼çš„æ•°æ®
            'processed_data': preview_data,  # æ–‡ä»¶é¢„è§ˆæ ¼å¼çš„æ•°æ®
            'open_questions': open_questions,
            'reference_tags': reference_tags,
            'tag_statistics': tag_statistics,
            'total_rows': len(df),
            'file_info': {
                'filename': os.path.basename(input_file),
                'columns': df.columns.tolist(),
                'shape': df.shape
            }
        }
        
        logger.info(f"âœ… è·å–æ ‡ç­¾ç¼–è¾‘æ•°æ®æˆåŠŸï¼Œåˆ†æID: {analysis_id}")
        logger.info(f"ğŸ“Š è¿”å›æ•°æ®åŒ…å«: TagEditoræ•°æ®({len(edit_data)}è¡Œ), é¢„è§ˆæ•°æ®({len(preview_data)}åˆ—), å¼€æ”¾é¢˜({len(open_questions)}ä¸ª)")
        return jsonify(convert_pandas_types(result))
        
    except Exception as e:
        logger.error(f"âŒ è·å–æ ‡ç­¾ç¼–è¾‘æ•°æ®å¤±è´¥: {e}")
        return jsonify({'error': f'è·å–æ ‡ç­¾ç¼–è¾‘æ•°æ®å¤±è´¥: {str(e)}'}), 500

@app.route('/save-manual-tags/<analysis_id>', methods=['POST'])
def save_manual_tags(analysis_id):
    """ä¿å­˜æ‰‹åŠ¨ä¿®æ”¹çš„æ ‡ç­¾"""
    try:
        if analysis_id not in analysis_results:
            return jsonify({'error': 'åˆ†æIDä¸å­˜åœ¨'}), 404
        
        data = request.get_json()
        modifications = data.get('modifications', [])
        
        if not modifications:
            return jsonify({'error': 'æ²¡æœ‰æä¾›ä¿®æ”¹æ•°æ®'}), 400
        
        analysis_info = analysis_results[analysis_id]
        
        # ä¼˜å…ˆè¯»å–æ‰‹åŠ¨ä¿®æ”¹åçš„æ–‡ä»¶ï¼Œå…¶æ¬¡è¯»å–æ‰“æ ‡æ–‡ä»¶ï¼Œæœ€åä½¿ç”¨åŸå§‹åˆ†ææ–‡ä»¶
        manual_output = analysis_info.get('manual_output')
        standard_labeling_output = analysis_info.get('standard_labeling_output')
        custom_labeling_output = analysis_info.get('custom_labeling_output')
        retag_output = analysis_info.get('retag_output')  # å‘åå…¼å®¹
        classification_output = analysis_info.get('classification_output')  # å‘åå…¼å®¹
        original_output = analysis_info.get('output_file') or analysis_info.get('processed_file')  # å…¼å®¹å¤šç§å­—æ®µå
        
        if manual_output and os.path.exists(manual_output):
            current_file = manual_output
            logger.info(f"ğŸ“ æ‰‹åŠ¨ä¿®æ”¹åŸºäºå·²æœ‰æ‰‹åŠ¨ä¿®æ”¹æ–‡ä»¶: {current_file}")
        elif standard_labeling_output and os.path.exists(standard_labeling_output):
            current_file = standard_labeling_output
            logger.info(f"ğŸ“ æ‰‹åŠ¨ä¿®æ”¹åŸºäºæ ‡å‡†AIæ‰“æ ‡æ–‡ä»¶: {current_file}")
        elif custom_labeling_output and os.path.exists(custom_labeling_output):
            current_file = custom_labeling_output
            logger.info(f"ğŸ“ æ‰‹åŠ¨ä¿®æ”¹åŸºäºé…ç½®å‚è€ƒæ ‡ç­¾æ‰“æ ‡æ–‡ä»¶: {current_file}")
        elif retag_output and os.path.exists(retag_output):
            current_file = retag_output
            logger.info(f"ğŸ“ æ‰‹åŠ¨ä¿®æ”¹åŸºäºé‡æ–°æ‰“æ ‡çš„æ–‡ä»¶: {current_file}")
        elif classification_output and os.path.exists(classification_output):
            current_file = classification_output
            logger.info(f"ğŸ“ æ‰‹åŠ¨ä¿®æ”¹åŸºäºåˆ†æé…ç½®æ–‡ä»¶: {current_file}")
        elif original_output and os.path.exists(original_output):
            current_file = original_output
            logger.info(f"ğŸ“ æ‰‹åŠ¨ä¿®æ”¹åŸºäºåŸå§‹åˆ†ææ–‡ä»¶: {current_file}")
        else:
            return jsonify({'error': 'æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„åˆ†æç»“æœæ–‡ä»¶'}), 404
        
        # è¯»å–å½“å‰æ–‡ä»¶
        df = pd.read_excel(current_file)
        
        # è®°å½•ä¿®æ”¹å†å²
        if 'manual_modifications' not in analysis_info:
            analysis_info['manual_modifications'] = []
        
        # åº”ç”¨ä¿®æ”¹
        for mod in modifications:
            row_id = mod.get('row_id')
            question_field = mod.get('question_field')  # åŸå§‹å¼€æ”¾é¢˜å­—æ®µå
            tag_type = mod.get('tag_type')  # 'level1_themes' æˆ– 'level2_tags'
            new_tags = mod.get('new_tags', [])
            
            if row_id < 0 or row_id >= len(df):
                continue
            
            # æ ¹æ®æ ‡ç­¾ç±»å‹ç¡®å®šç›®æ ‡åˆ—
            if tag_type == 'level1_themes':
                target_column = question_field + 'ä¸€çº§ä¸»é¢˜'
            elif tag_type == 'level2_tags':
                target_column = question_field + 'äºŒçº§æ ‡ç­¾'
            elif tag_type == 'reference_tags':
                target_column = question_field + 'æ ‡ç­¾'
            else:
                continue
                
            if target_column not in df.columns:
                continue
            
            # è®°å½•åŸå§‹å€¼
            old_value = str(df.iloc[row_id][target_column]) if pd.notna(df.iloc[row_id][target_column]) else ''
            
            # åº”ç”¨æ–°å€¼
            new_value = ','.join(new_tags) if new_tags else ''
            df.iloc[row_id, df.columns.get_loc(target_column)] = new_value
            
            # è®°å½•ä¿®æ”¹å†å²
            analysis_info['manual_modifications'].append({
                'row_id': row_id,
                'question_field': question_field,
                'tag_type': tag_type,
                'target_column': target_column,
                'old_value': old_value,
                'new_value': new_value,
                'timestamp': datetime.now().isoformat()
            })
        
        # ç¡®å®šæ‰‹åŠ¨ä¿®æ”¹æ–‡ä»¶çš„è¾“å‡ºè·¯å¾„ï¼Œç¡®ä¿åªä¿ç•™æœ€æ–°çš„ç¼–è¾‘ç»“æœ
        # é¦–å…ˆï¼Œåˆ é™¤ä¹‹å‰çš„æ‰‹åŠ¨ä¿®æ”¹æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'manual_output' in analysis_info and analysis_info['manual_output']:
            old_manual_file = analysis_info['manual_output']
            if os.path.exists(old_manual_file):
                try:
                    os.remove(old_manual_file)
                    logger.info(f"ğŸ—‘ï¸ åˆ é™¤ä¹‹å‰çš„æ‰‹åŠ¨ä¿®æ”¹æ–‡ä»¶: {old_manual_file}")
                except Exception as e:
                    logger.warning(f"âš ï¸ åˆ é™¤ä¹‹å‰çš„æ‰‹åŠ¨ä¿®æ”¹æ–‡ä»¶å¤±è´¥: {e}")
        
        # ç”Ÿæˆæ–°çš„æ‰‹åŠ¨ä¿®æ”¹æ–‡ä»¶è·¯å¾„ï¼Œæ ¹æ®ä¸åŒçš„æ‰“æ ‡ç±»å‹ä¿å­˜åˆ°ä¸åŒç›®å½•
        if standard_labeling_output and os.path.exists(standard_labeling_output):
            base_name, original_timestamp = extract_file_info(standard_labeling_output)
            manual_output = TRANSLATE_AI_MANUAL_FOLDER / f"{base_name}_ai_manual_{original_timestamp}.xlsx"
            logger.info(f"ğŸ“ åŸºäºæ ‡å‡†AIæ‰“æ ‡æ–‡ä»¶ç”Ÿæˆæ‰‹åŠ¨ä¿®æ”¹æ–‡ä»¶: {manual_output}")
        elif custom_labeling_output and os.path.exists(custom_labeling_output):
            base_name, original_timestamp = extract_file_info(custom_labeling_output)
            manual_output = TRANSLATE_CUSTOM_MANUAL_FOLDER / f"{base_name}_custom_manual_{original_timestamp}.xlsx"
            logger.info(f"ğŸ“ åŸºäºé…ç½®å‚è€ƒæ ‡ç­¾æ‰“æ ‡æ–‡ä»¶ç”Ÿæˆæ‰‹åŠ¨ä¿®æ”¹æ–‡ä»¶: {manual_output}")
        elif retag_output and os.path.exists(retag_output):
            base_name, original_timestamp = extract_file_info(retag_output)
            manual_output = RETAG_FOLDER / f"{base_name}_manual_{original_timestamp}.xlsx"
            logger.info(f"ğŸ“ åŸºäºé‡æ–°æ‰“æ ‡æ–‡ä»¶ç”Ÿæˆæ‰‹åŠ¨ä¿®æ”¹æ–‡ä»¶: {manual_output}")
        elif classification_output and os.path.exists(classification_output):
            base_name, original_timestamp = extract_file_info(classification_output)
            manual_output = CLASSIFICATION_FOLDER / f"{base_name}_manual_{original_timestamp}.xlsx"
            logger.info(f"ğŸ“ åŸºäºåˆ†æé…ç½®æ–‡ä»¶ç”Ÿæˆæ‰‹åŠ¨ä¿®æ”¹æ–‡ä»¶: {manual_output}")
        else:
            base_name, original_timestamp = extract_file_info(original_output)
            manual_output = RETAG_FOLDER / f"{base_name}_manual_{original_timestamp}.xlsx"
            logger.info(f"ğŸ“ åŸºäºåŸå§‹åˆ†ææ–‡ä»¶ç”Ÿæˆæ‰‹åŠ¨ä¿®æ”¹æ–‡ä»¶: {manual_output}")
        
        # ä¿å­˜ä¿®æ”¹åçš„æ–‡ä»¶
        df.to_excel(str(manual_output), index=False)
        
        # æ›´æ–°åˆ†æç»“æœ
        analysis_info['manual_output'] = str(manual_output)
        analysis_info['manual_modification_count'] = len(modifications)
        analysis_info['manual_modification_time'] = datetime.now().isoformat()
        
        logger.info(f"âœ… æ‰‹åŠ¨æ ‡ç­¾ä¿®æ”¹ä¿å­˜æˆåŠŸï¼Œåˆ†æID: {analysis_id}")
        logger.info(f"ğŸ“„ ä¿®æ”¹æ•°é‡: {len(modifications)}")
        logger.info(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶: {manual_output}")
        
        return jsonify({
            'message': 'æ ‡ç­¾ä¿®æ”¹ä¿å­˜æˆåŠŸ',
            'modifications_count': len(modifications),
            'output_file': str(manual_output)
        })
        
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜æ‰‹åŠ¨æ ‡ç­¾ä¿®æ”¹å¤±è´¥: {e}")
        return jsonify({'error': f'ä¿å­˜æ‰‹åŠ¨æ ‡ç­¾ä¿®æ”¹å¤±è´¥: {str(e)}'}), 500

@app.route('/batch-tag-operations/<analysis_id>', methods=['POST'])
def batch_tag_operations(analysis_id):
    """æ‰¹é‡æ ‡ç­¾æ“ä½œ"""
    try:
        if analysis_id not in analysis_results:
            return jsonify({'error': 'åˆ†æIDä¸å­˜åœ¨'}), 404
        
        data = request.get_json()
        operation = data.get('operation')  # 'replace', 'add', 'remove'
        tag_column = data.get('tag_column')
        target_tag = data.get('target_tag')
        replacement_tag = data.get('replacement_tag')
        affected_rows = data.get('affected_rows', [])
        
        if not operation or not tag_column:
            return jsonify({'error': 'ç¼ºå°‘å¿…è¦å‚æ•°'}), 400
        
        analysis_info = analysis_results[analysis_id]
        
        # ä¼˜å…ˆè¯»å–æ‰‹åŠ¨ä¿®æ”¹åçš„æ–‡ä»¶ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¯»å–é‡æ–°æ‰“æ ‡çš„æ–‡ä»¶
        manual_output = analysis_info.get('manual_output')
        retag_output = analysis_info.get('retag_output')
        
        if manual_output and os.path.exists(manual_output):
            current_file = manual_output
            logger.info(f"ğŸ“ æ‰¹é‡æ“ä½œåŸºäºæ‰‹åŠ¨ä¿®æ”¹åçš„æ–‡ä»¶: {current_file}")
        elif retag_output and os.path.exists(retag_output):
            current_file = retag_output
            logger.info(f"ğŸ“ æ‰¹é‡æ“ä½œåŸºäºé‡æ–°æ‰“æ ‡çš„æ–‡ä»¶: {current_file}")
        else:
            return jsonify({'error': 'é‡æ–°æ‰“æ ‡ç»“æœæ–‡ä»¶ä¸å­˜åœ¨'}), 404
        
        # è¯»å–å½“å‰æ–‡ä»¶
        df = pd.read_excel(current_file)
        
        if tag_column not in df.columns:
            return jsonify({'error': f'æ ‡ç­¾åˆ— {tag_column} ä¸å­˜åœ¨'}), 400
        
        # è®°å½•ä¿®æ”¹å†å²
        if 'manual_modifications' not in analysis_info:
            analysis_info['manual_modifications'] = []
        
        modification_count = 0
        
        # æ‰§è¡Œæ‰¹é‡æ“ä½œ
        for row_id in affected_rows:
            if row_id < 0 or row_id >= len(df):
                continue
            
            # è·å–å½“å‰æ ‡ç­¾
            current_tags_str = str(df.iloc[row_id][tag_column]) if pd.notna(df.iloc[row_id][tag_column]) else ''
            current_tags = [tag.strip() for tag in current_tags_str.split(',') if tag.strip()]
            
            # è®°å½•åŸå§‹å€¼
            old_value = current_tags_str
            new_tags = current_tags.copy()
            
            # æ ¹æ®æ“ä½œç±»å‹å¤„ç†
            if operation == 'replace' and target_tag and replacement_tag:
                new_tags = [replacement_tag if tag == target_tag else tag for tag in new_tags]
                
            elif operation == 'add' and replacement_tag:
                if replacement_tag not in new_tags:
                    new_tags.append(replacement_tag)
                    
            elif operation == 'remove' and target_tag:
                new_tags = [tag for tag in new_tags if tag != target_tag]
            
            # åº”ç”¨ä¿®æ”¹
            new_value = ','.join(new_tags) if new_tags else ''
            if new_value != old_value:
                df.iloc[row_id, df.columns.get_loc(tag_column)] = new_value
                modification_count += 1
                
                # è®°å½•ä¿®æ”¹å†å²
                analysis_info['manual_modifications'].append({
                    'row_id': row_id,
                    'tag_column': tag_column,
                    'old_value': old_value,
                    'new_value': new_value,
                    'operation': operation,
                    'timestamp': datetime.now().isoformat()
                })
        
        # ç”Ÿæˆæˆ–ä½¿ç”¨ç°æœ‰çš„è¾“å‡ºæ–‡ä»¶è·¯å¾„
        if 'manual_output' in analysis_info and os.path.exists(analysis_info['manual_output']):
            # å¦‚æœå·²æœ‰æ‰‹åŠ¨ä¿®æ”¹æ–‡ä»¶ï¼Œç›´æ¥åœ¨å…¶åŸºç¡€ä¸Šä¿®æ”¹
            manual_output = analysis_info['manual_output']
            logger.info(f"ğŸ“ åœ¨ç°æœ‰æ‰‹åŠ¨ä¿®æ”¹æ–‡ä»¶åŸºç¡€ä¸Šä¿å­˜: {manual_output}")
        else:
            # ç”Ÿæˆæ–°çš„æ‰‹åŠ¨ä¿®æ”¹æ–‡ä»¶
            base_name, original_timestamp = extract_file_info(retag_output)
            manual_output = RETAG_FOLDER / f"{base_name}_manual_{original_timestamp}.xlsx"
            logger.info(f"ğŸ“ ç”Ÿæˆæ–°çš„æ‰‹åŠ¨ä¿®æ”¹æ–‡ä»¶: {manual_output}")
        
        # ä¿å­˜ä¿®æ”¹åçš„æ–‡ä»¶
        df.to_excel(str(manual_output), index=False)
        
        # æ›´æ–°åˆ†æç»“æœ
        analysis_info['manual_output'] = str(manual_output)
        analysis_info['manual_modification_time'] = datetime.now().isoformat()
        
        logger.info(f"âœ… æ‰¹é‡æ ‡ç­¾æ“ä½œå®Œæˆï¼Œåˆ†æID: {analysis_id}")
        logger.info(f"ğŸ“„ æ“ä½œ: {operation}, å½±å“è¡Œæ•°: {modification_count}")
        
        return jsonify({
            'message': f'æ‰¹é‡{operation}æ“ä½œå®Œæˆ',
            'affected_count': modification_count,
            'output_file': str(manual_output)
        })
        
    except Exception as e:
        logger.error(f"âŒ æ‰¹é‡æ ‡ç­¾æ“ä½œå¤±è´¥: {e}")
        return jsonify({'error': f'æ‰¹é‡æ ‡ç­¾æ“ä½œå¤±è´¥: {str(e)}'}), 500

@app.route('/download-standard-labeling/<analysis_id>', methods=['GET'])
def download_standard_labeling(analysis_id):
    """ä¸‹è½½æ ‡å‡†AIæ‰“æ ‡å¤„ç†åçš„æ–‡ä»¶ (_standard_ æ–‡ä»¶)"""
    try:
        logger.info(f"ğŸ” æ”¶åˆ°æ ‡å‡†AIæ‰“æ ‡ä¸‹è½½è¯·æ±‚ï¼Œanalysis_id: {analysis_id}")
        
        if analysis_id not in analysis_results:
            logger.error(f"âŒ åˆ†æIDä¸å­˜åœ¨: {analysis_id}")
            logger.info(f"ğŸ’¡ å½“å‰å¯ç”¨çš„analysis_ids: {list(analysis_results.keys())}")
            return jsonify({'error': 'åˆ†æIDä¸å­˜åœ¨'}), 404
        
        analysis_info = analysis_results[analysis_id]
        logger.info(f"ğŸ“ analysis_infoä¸­çš„æ‰€æœ‰é”®: {list(analysis_info.keys())}")
        
        standard_output = analysis_info.get('standard_labeling_output')
        logger.info(f"ğŸ“„ æ ‡å‡†æ‰“æ ‡è¾“å‡ºæ–‡ä»¶è·¯å¾„: {standard_output}")
        
        if not standard_output:
            logger.error(f"âŒ standard_labeling_outputå­—æ®µä¸ºç©º")
            return jsonify({'error': 'æ ‡å‡†AIæ‰“æ ‡ç»“æœæ–‡ä»¶è·¯å¾„ä¸å­˜åœ¨'}), 404
            
        if not os.path.exists(standard_output):
            logger.error(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {standard_output}")
            # åˆ—å‡ºç›®å½•å†…å®¹ä»¥ä¾¿è°ƒè¯•
            dir_path = os.path.dirname(standard_output)
            if os.path.exists(dir_path):
                files = os.listdir(dir_path)
                logger.info(f"ğŸ’¡ ç›®å½• {dir_path} ä¸­çš„æ–‡ä»¶: {files}")
            return jsonify({'error': 'æ ‡å‡†AIæ‰“æ ‡ç»“æœæ–‡ä»¶ä¸å­˜åœ¨'}), 404
        
        # ä½¿ç”¨å®é™…æ–‡ä»¶åè€Œä¸æ˜¯ç¡¬ç¼–ç çš„åç§°
        actual_filename = Path(standard_output).name
        logger.info(f"ğŸ“¥ å‡†å¤‡ä¸‹è½½æ ‡å‡†AIæ‰“æ ‡æ–‡ä»¶: {actual_filename}")
        
        return send_file(
            standard_output,
            as_attachment=True,
            download_name=actual_filename,
            mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
        )
        
    except Exception as e:
        logger.error(f"âŒ ä¸‹è½½æ ‡å‡†AIæ‰“æ ‡ç»“æœå¤±è´¥: {e}")
        import traceback
        logger.error(f"âŒ å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
        return jsonify({'error': f'ä¸‹è½½æ ‡å‡†AIæ‰“æ ‡ç»“æœå¤±è´¥: {str(e)}'}), 500

@app.route('/download-ai-manual-result/<analysis_id>', methods=['GET'])
def download_ai_manual_result(analysis_id):
    """ä¸‹è½½æ ‡å‡†AIæ‰“æ ‡æ‰‹åŠ¨ç¼–è¾‘åçš„æ–‡ä»¶ (_ai_manual_ æ–‡ä»¶)"""
    try:
        if analysis_id not in analysis_results:
            return jsonify({'error': 'åˆ†æIDä¸å­˜åœ¨'}), 404
        
        analysis_info = analysis_results[analysis_id]
        # æŸ¥æ‰¾AIæ‰“æ ‡æ‰‹åŠ¨ç¼–è¾‘åçš„æ–‡ä»¶
        ai_manual_output = analysis_info.get('manual_ai_output')
        
        if not ai_manual_output or not os.path.exists(ai_manual_output):
            return jsonify({'error': 'æ ‡å‡†AIæ‰“æ ‡æ‰‹åŠ¨ç¼–è¾‘ç»“æœæ–‡ä»¶ä¸å­˜åœ¨'}), 404
        
        actual_filename = Path(ai_manual_output).name
        logger.info(f"ğŸ“¥ ä¸‹è½½AIæ‰‹åŠ¨ç¼–è¾‘æ–‡ä»¶: {actual_filename}")
        
        return send_file(
            ai_manual_output,
            as_attachment=True,
            download_name=actual_filename,
            mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
        )
        
    except Exception as e:
        logger.error(f"âŒ ä¸‹è½½AIæ‰‹åŠ¨ç¼–è¾‘ç»“æœå¤±è´¥: {e}")
        return jsonify({'error': f'ä¸‹è½½AIæ‰‹åŠ¨ç¼–è¾‘ç»“æœå¤±è´¥: {str(e)}'}), 500

@app.route('/download-custom-labeling/<analysis_id>', methods=['GET'])
def download_custom_labeling(analysis_id):
    """ä¸‹è½½å‚è€ƒæ ‡ç­¾æ‰“æ ‡å¤„ç†åçš„æ–‡ä»¶ (_translate_custom_ æ–‡ä»¶)"""
    try:
        if analysis_id not in analysis_results:
            return jsonify({'error': 'åˆ†æIDä¸å­˜åœ¨'}), 404
        
        analysis_info = analysis_results[analysis_id]
        custom_output = analysis_info.get('custom_labeling_output')
        
        if not custom_output or not os.path.exists(custom_output):
            return jsonify({'error': 'å‚è€ƒæ ‡ç­¾æ‰“æ ‡ç»“æœæ–‡ä»¶ä¸å­˜åœ¨'}), 404
        
        actual_filename = Path(custom_output).name
        logger.info(f"ğŸ“¥ ä¸‹è½½å‚è€ƒæ ‡ç­¾æ‰“æ ‡æ–‡ä»¶: {actual_filename}")
        
        return send_file(
            custom_output,
            as_attachment=True,
            download_name=actual_filename,
            mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
        )
        
    except Exception as e:
        logger.error(f"âŒ ä¸‹è½½å‚è€ƒæ ‡ç­¾æ‰“æ ‡ç»“æœå¤±è´¥: {e}")
        return jsonify({'error': f'ä¸‹è½½å‚è€ƒæ ‡ç­¾æ‰“æ ‡ç»“æœå¤±è´¥: {str(e)}'}), 500

@app.route('/download-custom-manual-result/<analysis_id>', methods=['GET'])
def download_custom_manual_result(analysis_id):
    """ä¸‹è½½å‚è€ƒæ ‡ç­¾æ‰“æ ‡æ‰‹åŠ¨ç¼–è¾‘åçš„æ–‡ä»¶ (_custom_manual_ æ–‡ä»¶)"""
    try:
        if analysis_id not in analysis_results:
            return jsonify({'error': 'åˆ†æIDä¸å­˜åœ¨'}), 404
        
        analysis_info = analysis_results[analysis_id]
        # æŸ¥æ‰¾å‚è€ƒæ ‡ç­¾æ‰‹åŠ¨ç¼–è¾‘åçš„æ–‡ä»¶
        custom_manual_output = analysis_info.get('manual_custom_output')
        
        if not custom_manual_output or not os.path.exists(custom_manual_output):
            return jsonify({'error': 'å‚è€ƒæ ‡ç­¾æ‰‹åŠ¨ç¼–è¾‘ç»“æœæ–‡ä»¶ä¸å­˜åœ¨'}), 404
        
        actual_filename = Path(custom_manual_output).name
        logger.info(f"ğŸ“¥ ä¸‹è½½å‚è€ƒæ ‡ç­¾æ‰‹åŠ¨ç¼–è¾‘æ–‡ä»¶: {actual_filename}")
        
        return send_file(
            custom_manual_output,
            as_attachment=True,
            download_name=actual_filename,
            mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
        )
        
    except Exception as e:
        logger.error(f"âŒ ä¸‹è½½å‚è€ƒæ ‡ç­¾æ‰‹åŠ¨ç¼–è¾‘ç»“æœå¤±è´¥: {e}")
        return jsonify({'error': f'ä¸‹è½½å‚è€ƒæ ‡ç­¾æ‰‹åŠ¨ç¼–è¾‘ç»“æœå¤±è´¥: {str(e)}'}), 500

# @app.route('/download-retag/<analysis_id>', methods=['GET'])
# def download_retag(analysis_id):
#     """ä¸‹è½½é‡æ–°æ‰“æ ‡å¤„ç†åçš„æ–‡ä»¶"""
#     try:
#         if analysis_id not in analysis_results:
#             return jsonify({'error': 'åˆ†æIDä¸å­˜åœ¨'}), 404
        
#         analysis_info = analysis_results[analysis_id]
#         # ä¼˜å…ˆä½¿ç”¨æ–°çš„å‚è€ƒæ ‡ç­¾æ‰“æ ‡è¾“å‡ºæ–‡ä»¶
#         retag_output = analysis_info.get('custom_labeling_output')
#         if not retag_output or not os.path.exists(retag_output):
#             # å‘åå…¼å®¹ï¼šå°è¯•ä½¿ç”¨æ—§çš„å­—æ®µå
#             retag_output = analysis_info.get('retag_output')
#             if not retag_output or not os.path.exists(retag_output):
#                 return jsonify({'error': 'é‡æ–°æ‰“æ ‡ç»“æœæ–‡ä»¶ä¸å­˜åœ¨'}), 404
        
#         # ä½¿ç”¨å®é™…æ–‡ä»¶åè€Œä¸æ˜¯ç¡¬ç¼–ç çš„åç§°
#         actual_filename = Path(retag_output).name
        
#         return send_file(
#             retag_output,
#             as_attachment=True,
#             download_name=actual_filename,
#             mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
#         )
        
#     except Exception as e:
#         logger.error(f"âŒ ä¸‹è½½é‡æ–°æ‰“æ ‡ç»“æœå¤±è´¥: {e}")
#         return jsonify({'error': f'ä¸‹è½½é‡æ–°æ‰“æ ‡ç»“æœå¤±è´¥: {str(e)}'}), 500

# @app.route('/download-final-result/<analysis_id>', methods=['GET'])
# def download_final_result(analysis_id):
#     """ä¸‹è½½æœ€ç»ˆç»“æœæ–‡ä»¶ï¼ˆæ‰‹åŠ¨ä¿®æ”¹åçš„æ–‡ä»¶ï¼‰"""
#     try:
#         if analysis_id not in analysis_results:
#             return jsonify({'error': 'åˆ†æIDä¸å­˜åœ¨'}), 404
        
#         analysis_info = analysis_results[analysis_id]
        
#         # ä¼˜å…ˆä½¿ç”¨æ‰‹åŠ¨ä¿®æ”¹åçš„æ–‡ä»¶
#         final_output = analysis_info.get('manual_output')
#         if final_output and os.path.exists(final_output):
#             actual_filename = Path(final_output).name
#             logger.info(f"ğŸ“¥ ä¸‹è½½æ‰‹åŠ¨ä¿®æ”¹åçš„æ–‡ä»¶: {actual_filename}")
#         else:
#             # å¦‚æœæ²¡æœ‰æ‰‹åŠ¨ä¿®æ”¹ï¼Œä½¿ç”¨é‡æ–°æ‰“æ ‡çš„æ–‡ä»¶
#             final_output = analysis_info.get('retag_output')
#             if final_output and os.path.exists(final_output):
#                 actual_filename = Path(final_output).name
#                 logger.info(f"ğŸ“¥ ä¸‹è½½é‡æ–°æ‰“æ ‡çš„æ–‡ä»¶: {actual_filename}")
#             else:
#                 return jsonify({'error': 'æœ€ç»ˆç»“æœæ–‡ä»¶ä¸å­˜åœ¨'}), 404
        
#         return send_file(
#             final_output,
#             as_attachment=True,
#             download_name=actual_filename,
#             mimetype='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
#         )
        
#     except Exception as e:
#         logger.error(f"âŒ ä¸‹è½½æœ€ç»ˆç»“æœå¤±è´¥: {e}")
#         return jsonify({'error': f'ä¸‹è½½æœ€ç»ˆç»“æœå¤±è´¥: {str(e)}'}), 500

@app.route('/get-modification-history/<analysis_id>', methods=['GET'])
def get_modification_history(analysis_id):
    """è·å–ä¿®æ”¹å†å²è®°å½•"""
    try:
        if analysis_id not in analysis_results:
            return jsonify({'error': 'åˆ†æIDä¸å­˜åœ¨'}), 404
        
        analysis_info = analysis_results[analysis_id]
        modifications = analysis_info.get('manual_modifications', [])
        
        # æŒ‰æ—¶é—´å€’åºæ’åˆ—
        modifications.sort(key=lambda x: x.get('timestamp', ''), reverse=True)
        
        result = {
            'total_modifications': len(modifications),
            'modifications': modifications,
            'has_manual_changes': 'manual_output' in analysis_info
        }
        
        logger.info(f"âœ… è·å–ä¿®æ”¹å†å²æˆåŠŸï¼Œåˆ†æID: {analysis_id}")
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"âŒ è·å–ä¿®æ”¹å†å²å¤±è´¥: {e}")
        return jsonify({'error': f'è·å–ä¿®æ”¹å†å²å¤±è´¥: {str(e)}'}), 500

@app.route('/test/analysis-history', methods=['GET'])
def get_analysis_history():
    """è·å–åˆ†æå†å²"""
    try:
        history = []
        for analysis_id, info in analysis_results.items():
            history.append({
                'analysisId': analysis_id,
                'filename': info.get('filename', 'Unknown'),
                'uploadTime': info.get('upload_time', ''),
                'hasResult': 'analysis_result' in info
            })
        
        return jsonify(history)
        
    except Exception as e:
        logger.error(f"âŒ è·å–åˆ†æå†å²å¤±è´¥: {e}")
        return jsonify({'error': f'è·å–åˆ†æå†å²å¤±è´¥: {str(e)}'}), 500

@app.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return jsonify({
        'status': 'healthy',
        'timestamp': datetime.now().isoformat(),
        'service': 'questionnaire-analysis-api'
    })

# æ•°æ®åº“ç›¸å…³APIæ¥å£
@app.route('/test-database-connection', methods=['GET'])
def test_db_connection():
    """æµ‹è¯•æ•°æ®åº“è¿æ¥æ¥å£"""
    try:
        success, message = test_database_connection()
        
        if success:
            return jsonify({
                'success': True,
                'message': message,
                'database': 'mkt',
                'timestamp': datetime.now().isoformat()
            })
        else:
            return jsonify({
                'success': False,
                'error': message,
                'timestamp': datetime.now().isoformat()
            }), 500
            
    except Exception as e:
        logger.error(f"âŒ æ•°æ®åº“è¿æ¥æµ‹è¯•å¼‚å¸¸: {e}")
        return jsonify({
            'success': False,
            'error': f'æµ‹è¯•å¼‚å¸¸: {str(e)}',
            'timestamp': datetime.now().isoformat()
        }), 500

@app.route('/import-to-database/<analysis_id>', methods=['POST'])
def import_to_database(analysis_id):
    """å°†é—®å·æ•°æ®å¯¼å…¥åˆ°æ•°æ®åº“"""
    try:
        logger.info(f"ğŸš€ å¼€å§‹å¯¼å…¥æ•°æ®åˆ°æ•°æ®åº“ï¼Œåˆ†æID: {analysis_id}")
        
        # æ£€æŸ¥åˆ†æIDæ˜¯å¦å­˜åœ¨
        if analysis_id not in analysis_results:
            return jsonify({'error': 'åˆ†æIDä¸å­˜åœ¨'}), 404
        
        analysis_info = analysis_results[analysis_id]
        
        # è·å–æ‰‹åŠ¨ä¿®æ”¹åçš„æ–‡ä»¶è·¯å¾„
        manual_output = analysis_info.get('manual_output')
        retag_output = analysis_info.get('retag_output')
        
        # ç¡®å®šè¦è¯»å–çš„æ–‡ä»¶
        file_path = None
        if manual_output and os.path.exists(manual_output):
            file_path = manual_output
            logger.info(f"ğŸ“ ä½¿ç”¨æ‰‹åŠ¨ä¿®æ”¹åçš„æ–‡ä»¶: {file_path}")
        elif retag_output and os.path.exists(retag_output):
            file_path = retag_output
            logger.info(f"ğŸ“ ä½¿ç”¨é‡æ–°æ‰“æ ‡çš„æ–‡ä»¶: {file_path}")
        else:
            return jsonify({'error': 'æœªæ‰¾åˆ°å¯å¯¼å…¥çš„æ•°æ®æ–‡ä»¶'}), 404
        
        # è¯»å–Excelæ–‡ä»¶
        try:
            df = pd.read_excel(file_path)
            logger.info(f"ğŸ“Š æˆåŠŸè¯»å–æ–‡ä»¶ï¼Œå…± {len(df)} è¡Œï¼Œ{len(df.columns)} åˆ—")
        except Exception as e:
            return jsonify({'error': f'è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}'}), 500
        
        # è§£æåˆ—ç»“æ„
        column_info = parse_column_structure(df)
        
        # æå–é—®å·å…ƒæ•°æ®
        filename = os.path.basename(file_path)
        survey_name = extract_survey_name_from_filename(filename)
        
        # å¯é€‰ï¼šä»è¯·æ±‚ä¸­è·å–é—®å·ä¸»é¢˜
        data = request.get_json() or {}
        survey_topic = data.get('survey_topic', '')
        
        # è½¬æ¢æ•°æ®ä¸ºè®°å½•æ ¼å¼
        records = transform_data_to_records(df, column_info, analysis_id, survey_name, survey_topic)
        
        # éªŒè¯æ•°æ®
        validation = validate_records(records)
        if not validation['valid']:
            return jsonify({
                'error': 'æ•°æ®éªŒè¯å¤±è´¥',
                'details': validation['errors'],
                'warnings': validation['warnings']
            }), 400
        
        # è¿æ¥æ•°æ®åº“
        connection, error = get_database_connection()
        if error:
            return jsonify({'error': error}), 500
        
        try:
            cursor = connection.cursor()
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒanalysis_idçš„æ•°æ®
            check_sql = "SELECT COUNT(*) FROM questionnaire_final_results WHERE analysis_id = %s"
            cursor.execute(check_sql, (analysis_id,))
            existing_count = cursor.fetchone()[0]
            
            if existing_count > 0:
                # åˆ é™¤å·²å­˜åœ¨çš„æ•°æ®
                delete_sql = "DELETE FROM questionnaire_final_results WHERE analysis_id = %s"
                cursor.execute(delete_sql, (analysis_id,))
                logger.info(f"ğŸ—‘ï¸ åˆ é™¤äº† {existing_count} æ¡å·²å­˜åœ¨çš„è®°å½•")
            
            # æ‰¹é‡æ’å…¥æ–°æ•°æ®
            insert_sql = """
            INSERT INTO questionnaire_final_results (
                analysis_id, respondent_id, survey_name, survey_topic, question_code,
                question, question_type, respondent_row, user_answer, translation,
                labels, primary_category, secondary_category
            ) VALUES (
                %(analysis_id)s, %(respondent_id)s, %(survey_name)s, %(survey_topic)s, %(question_code)s,
                %(question)s, %(question_type)s, %(respondent_row)s, %(user_answer)s, %(translation)s,
                %(labels)s, %(primary_category)s, %(secondary_category)s
            )
            """
            
            cursor.executemany(insert_sql, records)
            connection.commit()
            
            inserted_count = cursor.rowcount
            logger.info(f"âœ… æˆåŠŸæ’å…¥ {inserted_count} æ¡è®°å½•")
            
            # æ›´æ–°åˆ†æç»“æœä¸­çš„æ•°æ®åº“å¯¼å…¥çŠ¶æ€
            analysis_info['database_imported'] = True
            analysis_info['database_import_time'] = datetime.now().isoformat()
            analysis_info['database_record_count'] = inserted_count
            
            cursor.close()
            
            return jsonify({
                'success': True,
                'message': f'æˆåŠŸå¯¼å…¥ {inserted_count} æ¡è®°å½•åˆ°æ•°æ®åº“',
                'statistics': validation['statistics'],
                'analysis_id': analysis_id,
                'database': 'mkt',
                'table': 'questionnaire_final_results'
            })
            
        except Exception as e:
            connection.rollback()
            error_msg = f"æ•°æ®åº“æ“ä½œå¤±è´¥: {str(e)}"
            logger.error(f"âŒ {error_msg}")
            return jsonify({'error': error_msg}), 500
        finally:
            connection.close()
            
    except Exception as e:
        logger.error(f"âŒ å¯¼å…¥è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}")
        logger.error(traceback.format_exc())
        return jsonify({'error': f'å¯¼å…¥å¼‚å¸¸: {str(e)}'}), 500

@app.route('/database-status/<analysis_id>', methods=['GET'])
def get_database_status(analysis_id):
    """è·å–æ•°æ®åº“å¯¼å…¥çŠ¶æ€"""
    try:
        if analysis_id not in analysis_results:
            return jsonify({'error': 'åˆ†æIDä¸å­˜åœ¨'}), 404
        
        analysis_info = analysis_results[analysis_id]
        
        # æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦æœ‰è¯¥åˆ†æIDçš„æ•°æ®
        connection, error = get_database_connection()
        if error:
            return jsonify({'error': error}), 500
        
        try:
            cursor = connection.cursor()
            cursor.execute("SELECT COUNT(*) FROM questionnaire_final_results WHERE analysis_id = %s", (analysis_id,))
            db_record_count = cursor.fetchone()[0]
            cursor.close()
            
            return jsonify({
                'analysis_id': analysis_id,
                'database_imported': analysis_info.get('database_imported', False),
                'database_import_time': analysis_info.get('database_import_time', ''),
                'database_record_count': db_record_count,
                'local_record_count': analysis_info.get('database_record_count', 0)
            })
            
        finally:
            connection.close()
            
    except Exception as e:
        logger.error(f"âŒ è·å–æ•°æ®åº“çŠ¶æ€å¤±è´¥: {e}")
        return jsonify({'error': f'è·å–çŠ¶æ€å¤±è´¥: {str(e)}'}), 500

@app.route('/save-ai-manual-tags/<analysis_id>', methods=['POST'])
def save_ai_manual_tags(analysis_id):
    """ä¿å­˜AIæ‰“æ ‡æ‰‹åŠ¨ä¿®æ”¹çš„æ ‡ç­¾"""
    try:
        if analysis_id not in analysis_results:
            return jsonify({'error': 'åˆ†æIDä¸å­˜åœ¨'}), 404
        
        data = request.get_json()
        modifications = data.get('modifications', [])
        
        if not modifications:
            return jsonify({'error': 'æ²¡æœ‰æä¾›ä¿®æ”¹æ•°æ®'}), 400
        
        analysis_info = analysis_results[analysis_id]
        
        # ä¸“é—¨å¤„ç†AIæ‰“æ ‡æ–‡ä»¶
        standard_labeling_output = analysis_info.get('standard_labeling_output')
        manual_ai_output = analysis_info.get('manual_ai_output')
        
        # åˆ é™¤ä¹‹å‰çš„AIæ‰‹åŠ¨ç¼–è¾‘æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if manual_ai_output and os.path.exists(manual_ai_output):
            try:
                os.remove(manual_ai_output)
                logger.info(f"ğŸ—‘ï¸ åˆ é™¤ä¹‹å‰çš„AIæ‰‹åŠ¨ç¼–è¾‘æ–‡ä»¶: {manual_ai_output}")
            except Exception as e:
                logger.warning(f"âš ï¸ åˆ é™¤ä¹‹å‰çš„AIæ‰‹åŠ¨ç¼–è¾‘æ–‡ä»¶å¤±è´¥: {e}")
        
        # ç¡®å®šå½“å‰è¯»å–æ–‡ä»¶
        if manual_ai_output and os.path.exists(manual_ai_output):
            current_file = manual_ai_output
            logger.info(f"ğŸ“ AIæ‰“æ ‡æ‰‹åŠ¨ä¿®æ”¹åŸºäºå·²æœ‰æ‰‹åŠ¨ä¿®æ”¹æ–‡ä»¶: {current_file}")
        elif standard_labeling_output and os.path.exists(standard_labeling_output):
            current_file = standard_labeling_output
            logger.info(f"ğŸ“ AIæ‰“æ ‡æ‰‹åŠ¨ä¿®æ”¹åŸºäºæ ‡å‡†AIæ‰“æ ‡æ–‡ä»¶: {current_file}")
        else:
            return jsonify({'error': 'æ²¡æœ‰æ‰¾åˆ°AIæ‰“æ ‡ç»“æœæ–‡ä»¶'}), 404
        
        # ç”Ÿæˆä¿å­˜è·¯å¾„åˆ°translate_ai_manualç›®å½•ï¼Œä½¿ç”¨åŸå§‹æ—¶é—´æˆ³ä¿æŒä¸€è‡´æ€§
        base_name, original_timestamp = extract_file_info(current_file)
        output_file = TRANSLATE_AI_MANUAL_FOLDER / f"{base_name}_ai_manual_{original_timestamp}.xlsx"
        
        return _save_manual_tags_common(analysis_id, current_file, str(output_file), modifications, 'manual_ai_output', "AIæ‰“æ ‡æ‰‹åŠ¨ä¿®æ”¹")
        
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜AIæ‰“æ ‡æ‰‹åŠ¨ä¿®æ”¹å¤±è´¥: {e}")
        return jsonify({'error': f'ä¿å­˜AIæ‰“æ ‡æ‰‹åŠ¨ä¿®æ”¹å¤±è´¥: {str(e)}'}), 500

@app.route('/save-custom-manual-tags/<analysis_id>', methods=['POST'])
def save_custom_manual_tags(analysis_id):
    """ä¿å­˜å‚è€ƒæ ‡ç­¾æ‰“æ ‡æ‰‹åŠ¨ä¿®æ”¹çš„æ ‡ç­¾"""
    try:
        if analysis_id not in analysis_results:
            return jsonify({'error': 'åˆ†æIDä¸å­˜åœ¨'}), 404
        
        data = request.get_json()
        modifications = data.get('modifications', [])
        
        if not modifications:
            return jsonify({'error': 'æ²¡æœ‰æä¾›ä¿®æ”¹æ•°æ®'}), 400
        
        analysis_info = analysis_results[analysis_id]
        
        # ä¸“é—¨å¤„ç†å‚è€ƒæ ‡ç­¾æ‰“æ ‡æ–‡ä»¶
        custom_labeling_output = analysis_info.get('custom_labeling_output')
        manual_custom_output = analysis_info.get('manual_custom_output')
        
        # åˆ é™¤ä¹‹å‰çš„å‚è€ƒæ ‡ç­¾æ‰‹åŠ¨ç¼–è¾‘æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if manual_custom_output and os.path.exists(manual_custom_output):
            try:
                os.remove(manual_custom_output)
                logger.info(f"ğŸ—‘ï¸ åˆ é™¤ä¹‹å‰çš„å‚è€ƒæ ‡ç­¾æ‰‹åŠ¨ç¼–è¾‘æ–‡ä»¶: {manual_custom_output}")
            except Exception as e:
                logger.warning(f"âš ï¸ åˆ é™¤ä¹‹å‰çš„å‚è€ƒæ ‡ç­¾æ‰‹åŠ¨ç¼–è¾‘æ–‡ä»¶å¤±è´¥: {e}")
        
        # ç¡®å®šå½“å‰è¯»å–æ–‡ä»¶
        if manual_custom_output and os.path.exists(manual_custom_output):
            current_file = manual_custom_output
            logger.info(f"ğŸ“ å‚è€ƒæ ‡ç­¾æ‰‹åŠ¨ä¿®æ”¹åŸºäºå·²æœ‰æ‰‹åŠ¨ä¿®æ”¹æ–‡ä»¶: {current_file}")
        elif custom_labeling_output and os.path.exists(custom_labeling_output):
            current_file = custom_labeling_output
            logger.info(f"ğŸ“ å‚è€ƒæ ‡ç­¾æ‰‹åŠ¨ä¿®æ”¹åŸºäºå‚è€ƒæ ‡ç­¾æ‰“æ ‡æ–‡ä»¶: {current_file}")
        else:
            # å°è¯•æœç´¢å¤‡ç”¨æ–‡ä»¶
            try:
                custom_files = list(TRANSLATE_CUSTOM_FOLDER.glob(f"*{analysis_id.split('-')[0]}*_translate_custom_*.xlsx"))
                if custom_files:
                    current_file = str(max(custom_files, key=os.path.getmtime))
                    logger.info(f"ğŸ”§ æ‰¾åˆ°å¤‡ç”¨å‚è€ƒæ ‡ç­¾æ–‡ä»¶: {current_file}")
                else:
                    return jsonify({'error': 'æ²¡æœ‰æ‰¾åˆ°å‚è€ƒæ ‡ç­¾æ‰“æ ‡ç»“æœæ–‡ä»¶'}), 404
            except Exception:
                return jsonify({'error': 'æ²¡æœ‰æ‰¾åˆ°å‚è€ƒæ ‡ç­¾æ‰“æ ‡ç»“æœæ–‡ä»¶'}), 404
        
        # ç”Ÿæˆä¿å­˜è·¯å¾„åˆ°translate_custom_manualç›®å½•ï¼Œä½¿ç”¨åŸå§‹æ—¶é—´æˆ³ä¿æŒä¸€è‡´æ€§
        base_name, original_timestamp = extract_file_info(current_file)
        output_file = TRANSLATE_CUSTOM_MANUAL_FOLDER / f"{base_name}_custom_manual_{original_timestamp}.xlsx"
        
        return _save_manual_tags_common(analysis_id, current_file, str(output_file), modifications, 'manual_custom_output', "å‚è€ƒæ ‡ç­¾æ‰‹åŠ¨ä¿®æ”¹")
        
    except Exception as e:
        logger.error(f"âŒ ä¿å­˜å‚è€ƒæ ‡ç­¾æ‰‹åŠ¨ä¿®æ”¹å¤±è´¥: {e}")
        return jsonify({'error': f'ä¿å­˜å‚è€ƒæ ‡ç­¾æ‰‹åŠ¨ä¿®æ”¹å¤±è´¥: {str(e)}'}), 500

def _save_manual_tags_common(analysis_id, current_file, output_file, modifications, output_key, save_type):
    """ä¿å­˜æ‰‹åŠ¨æ ‡ç­¾çš„é€šç”¨å‡½æ•°"""
    try:
        # è¯»å–å½“å‰æ–‡ä»¶
        df = pd.read_excel(current_file)
        logger.info(f"ğŸ“Š è¯»å–{save_type}æ–‡ä»¶æˆåŠŸï¼Œå…± {len(df)} è¡Œï¼Œ{len(df.columns)} åˆ—")
        
        # åº”ç”¨ä¿®æ”¹
        for mod in modifications:
            row_id = mod.get('rowId')
            field_name = mod.get('fieldName')
            tag_type = mod.get('tagType')  # 'level1_themes', 'level2_tags', 'reference_tags'
            new_values = mod.get('newValues', [])
            
            logger.info(f"ğŸ”§ åº”ç”¨{save_type}ä¿®æ”¹: è¡Œ{row_id}, å­—æ®µ{field_name}, ç±»å‹{tag_type}, æ–°å€¼{new_values}")
            
            # æ ¹æ®tag_typeç¡®å®šè¦ä¿®æ”¹çš„åˆ—
            if tag_type == 'level1_themes':
                target_col = f"{field_name}ä¸€çº§ä¸»é¢˜"
            elif tag_type == 'level2_tags':
                target_col = f"{field_name}äºŒçº§æ ‡ç­¾"
            elif tag_type == 'reference_tags':
                target_col = f"{field_name}æ ‡ç­¾"
            else:
                logger.warning(f"âš ï¸ æœªçŸ¥çš„æ ‡ç­¾ç±»å‹: {tag_type}")
                continue
            
            if target_col in df.columns and row_id < len(df):
                # å°†æ–°å€¼åˆ—è¡¨è½¬æ¢ä¸ºé€—å·åˆ†éš”çš„å­—ç¬¦ä¸²
                new_value_str = ','.join(new_values) if new_values else ''
                df.at[row_id, target_col] = new_value_str
                logger.info(f"âœ… æ›´æ–° {target_col}[{row_id}] = '{new_value_str}'")
            else:
                if target_col not in df.columns:
                    logger.warning(f"âš ï¸ åˆ— {target_col} ä¸å­˜åœ¨äºDataFrameä¸­")
                if row_id >= len(df):
                    logger.warning(f"âš ï¸ è¡Œç´¢å¼• {row_id} è¶…å‡ºDataFrameèŒƒå›´")
        
        # ä¿å­˜ä¿®æ”¹åçš„æ–‡ä»¶
        df.to_excel(output_file, index=False)
        logger.info(f"ğŸ’¾ {save_type}æ–‡ä»¶å·²ä¿å­˜: {output_file}")
        
        # æ›´æ–°analysis_results
        analysis_results[analysis_id][output_key] = output_file
        
        # ç”Ÿæˆæ–‡ä»¶é¢„è§ˆæ•°æ®
        preview_data = []
        for col in df.columns:
            field_data = {
                'field': col,
                'values': df[col].fillna('').astype(str).tolist()
            }
            preview_data.append(field_data)
        
        result = {
            'success': True,
            'message': f'{save_type}ä¿å­˜æˆåŠŸ',
            'output_file': output_file,
            'save_type': save_type,
            'total_rows': len(df),
            'processed_data': preview_data
        }
        
        logger.info(f"âœ… {save_type}ä¿å­˜æˆåŠŸï¼Œåˆ†æID: {analysis_id}")
        return jsonify(convert_pandas_types(result))
        
    except Exception as e:
        logger.error(f"âŒ {save_type}ä¿å­˜å¤±è´¥: {e}")
        import traceback
        logger.error(f"âŒ å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
        return jsonify({'error': f'{save_type}ä¿å­˜å¤±è´¥: {str(e)}'}), 500

if __name__ == '__main__':
    logger.info("ğŸš€ é—®å·åˆ†æFlaskæœåŠ¡å¯åŠ¨")
    logger.info(f"ğŸ“ ä¸Šä¼ æ–‡ä»¶å¤¹: {UPLOAD_FOLDER}")
    logger.info("ğŸŒ æœåŠ¡åœ°å€: http://localhost:9001")
    
    # å¯åŠ¨Flaskåº”ç”¨
    app.run(
        host='0.0.0.0',
        port=9001,
        debug=False  # å…³é—­debugæ¨¡å¼é¿å…watchdogé—®é¢˜
    ) 