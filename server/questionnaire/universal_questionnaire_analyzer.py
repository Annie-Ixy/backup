import pandas as pd
import os
import glob
from collections import defaultdict
import re
import numpy as np
from flask import Flask, request, jsonify
from flask_cors import CORS
from werkzeug.utils import secure_filename
from datetime import datetime
import uuid
import json

# åŠ è½½ç¯å¢ƒå˜é‡ï¼ˆå¯é€‰ï¼‰
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    print("âš ï¸  python-dotenvåº“æœªå®‰è£…ï¼Œå°†è·³è¿‡ç¯å¢ƒå˜é‡åŠ è½½")

# OpenAIé…ç½®ï¼ˆå¯é€‰å¯¼å…¥ï¼‰
try:
    from openai import OpenAI
    openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("âš ï¸  OpenAIåº“æœªå®‰è£…ï¼ŒAIåˆ†æåŠŸèƒ½å°†è¢«ç¦ç”¨")

# Flaskåº”ç”¨é…ç½®
app = Flask(__name__)
CORS(app)  # å…è®¸è·¨åŸŸè¯·æ±‚

# æ–‡ä»¶ä¸Šä¼ é…ç½®
UPLOAD_FOLDER = os.path.join(os.path.dirname(__file__), '../uploads/questionnaire')
ALLOWED_EXTENSIONS = {'csv', 'xlsx', 'xls', 'txt'}

# ç¡®ä¿ä¸Šä¼ ç›®å½•å­˜åœ¨
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max file size

# å­˜å‚¨åˆ†æç»“æœçš„å†…å­˜æ•°æ®ç»“æ„
analysis_results = {}

class UniversalQuestionnaireAnalyzer:
    """é€šç”¨é—®å·åˆ†æå·¥å…·"""
    
    def __init__(self):
        # å•é€‰é¢˜æ¨¡å¼å®šä¹‰
        self.single_choice_patterns = {
            'binary_patterns': {
                'yes_no': {
                    'chinese': ['æ˜¯', 'å¦'],
                    'english': ['yes', 'no', 'y', 'n']
                },
                'true_false': {
                    'chinese': ['å¯¹', 'é”™', 'æ­£ç¡®', 'é”™è¯¯'],
                    'english': ['true', 'false', 't', 'f']
                },
                'selected_unselected': {
                    'chinese': ['å·²é€‰æ‹©', 'æœªé€‰æ‹©', 'é€‰ä¸­', 'æœªé€‰ä¸­'],
                    'english': ['selected', 'not selected', 'unselected', 'checked', 'unchecked']
                },
                'agree_disagree': {
                    'chinese': ['åŒæ„', 'ä¸åŒæ„', 'èµæˆ', 'åå¯¹'],
                    'english': ['agree', 'disagree', 'support', 'oppose']
                }
            },
            'satisfaction_patterns': {
                'satisfaction_basic': {
                    'chinese': ['éå¸¸æ»¡æ„', 'æ»¡æ„', 'ä¸€èˆ¬', 'ä¸æ»¡æ„', 'éå¸¸ä¸æ»¡æ„'],
                    'english': ['very satisfied', 'satisfied', 'neutral', 'dissatisfied', 'very dissatisfied']
                },
                'satisfaction_extended': {
                    'chinese': [
                        'æå…¶æ»¡æ„', 'éå¸¸æ»¡æ„', 'æ¯”è¾ƒæ»¡æ„', 'é€‚åº¦æ»¡æ„', 'æ—¢ä¸æ»¡æ„ä¹Ÿä¸ä¸æ»¡æ„',
                        'é€‚åº¦ä¸æ»¡æ„', 'æ¯”è¾ƒä¸æ»¡æ„', 'éå¸¸ä¸æ»¡æ„', 'æå…¶ä¸æ»¡æ„'
                    ],
                    'english': [
                        'extremely satisfied', 'very satisfied', 'moderately satisfied', 'somewhat satisfied',
                        'neither satisfied nor dissatisfied', 'moderately dissatisfied', 'somewhat dissatisfied',
                        'very dissatisfied', 'extremely dissatisfied'
                    ]
                }
            },
            'rating_patterns': {
                'quality_3': {
                    'chinese': ['å¥½', 'ä¸­', 'å·®', 'ä¼˜', 'è‰¯', 'åŠ£'],
                    'english': ['good', 'fair', 'poor', 'excellent', 'average', 'bad']
                },
                'quality_5': {
                    'chinese': ['å¾ˆå¥½', 'å¥½', 'ä¸€èˆ¬', 'å·®', 'å¾ˆå·®'],
                    'english': ['very good', 'good', 'average', 'poor', 'very poor']
                },
                'frequency': {
                    'chinese': ['ç»å¸¸', 'å¶å°”', 'å¾ˆå°‘', 'ä»ä¸', 'æ€»æ˜¯', 'æœ‰æ—¶'],
                    'english': ['often', 'sometimes', 'rarely', 'never', 'always', 'occasionally']
                }
            }
        }
        
        # æ™ºèƒ½å…³é”®è¯æ£€æµ‹ï¼ˆç”¨äºè¯†åˆ«åŒ…å«å…³é”®è¯çš„é•¿é€‰é¡¹ï¼‰
        self.smart_keywords = {
            'binary_keywords': {
                'chinese': ['æ˜¯', 'å¦', 'æœ‰', 'æ²¡æœ‰', 'ä¼š', 'ä¸ä¼š', 'ç”¨è¿‡', 'æ²¡ç”¨è¿‡'],
                'english': ['yes', 'no', 'have', 'never', 'used', 'not used', 'will', 'won\'t']
            },
            'frequency_keywords': {
                'chinese': ['ç»å¸¸', 'å¶å°”', 'å¾ˆå°‘', 'ä»ä¸', 'æ€»æ˜¯', 'æœ‰æ—¶', 'æ¯å¤©', 'æ¯å‘¨'],
                'english': ['often', 'sometimes', 'rarely', 'never', 'always', 'daily', 'weekly', 'occasionally']
            },
            'satisfaction_keywords': {
                'chinese': ['æ»¡æ„', 'ä¸æ»¡æ„', 'å–œæ¬¢', 'ä¸å–œæ¬¢', 'å¥½', 'ä¸å¥½'],
                'english': ['satisfied', 'dissatisfied', 'like', 'dislike', 'good', 'bad', 'love', 'hate']
            }
        }
        
    def find_excel_files(self, directory=None):
        """æŸ¥æ‰¾ç›®å½•ä¸­çš„Excelæ–‡ä»¶"""
        if directory is None:
            directory = os.path.dirname(os.path.abspath(__file__))
        
        excel_patterns = ['*.xlsx', '*.xls', '*.csv']
        excel_files = []
        
        for pattern in excel_patterns:
            files = glob.glob(os.path.join(directory, pattern))
            excel_files.extend(files)
        
        return excel_files
    
    def select_file_interactive(self):
        """äº¤äº’å¼é€‰æ‹©æ–‡ä»¶"""
        print("=" * 60)
        print("é€šç”¨é—®å·åˆ†æå·¥å…·")
        print("=" * 60)
        
        excel_files = self.find_excel_files()
        
        if not excel_files:
            print("å½“å‰ç›®å½•æœªæ‰¾åˆ°Excelæˆ–CSVæ–‡ä»¶")
            
            # è®©ç”¨æˆ·è¾“å…¥æ–‡ä»¶è·¯å¾„
            file_path = input("è¯·è¾“å…¥æ–‡ä»¶å®Œæ•´è·¯å¾„: ").strip().strip('"')
            if os.path.exists(file_path):
                return file_path
            else:
                print("æ–‡ä»¶ä¸å­˜åœ¨")
                return None
        
        print(f"å‘ç° {len(excel_files)} ä¸ªæ•°æ®æ–‡ä»¶:")
        for i, file in enumerate(excel_files, 1):
            filename = os.path.basename(file)
            print(f"{i:2d}. {filename}")
        
        if len(excel_files) == 1:
            print(f"\nè‡ªåŠ¨é€‰æ‹©å”¯ä¸€æ–‡ä»¶: {os.path.basename(excel_files[0])}")
            return excel_files[0]
        
        choice = input(f"\nè¯·é€‰æ‹©æ–‡ä»¶ (1-{len(excel_files)}): ").strip()
        
        try:
            idx = int(choice) - 1
            if 0 <= idx < len(excel_files):
                return excel_files[idx]
            else:
                print("æ— æ•ˆé€‰æ‹©")
                return None
        except ValueError:
            print("è¾“å…¥æ ¼å¼é”™è¯¯")
            return None
    
    def read_data_file(self, file_path):
        """è¯»å–æ•°æ®æ–‡ä»¶ï¼ˆæ”¯æŒExcelå’ŒCSVï¼‰"""
        try:
            print(f"æ­£åœ¨è¯»å–æ–‡ä»¶: {os.path.basename(file_path)}")
            
            if file_path.endswith('.csv'):
                # å°è¯•ä¸åŒçš„ç¼–ç æ ¼å¼
                for encoding in ['utf-8', 'gbk', 'gb2312', 'latin1']:
                    try:
                        df = pd.read_csv(file_path, encoding=encoding)
                        break
                    except UnicodeDecodeError:
                        continue
                else:
                    raise Exception("æ— æ³•è¯†åˆ«CSVæ–‡ä»¶ç¼–ç æ ¼å¼")
            else:
                df = pd.read_excel(file_path)
            
            print("=" * 50)
            print("æ–‡ä»¶è¯»å–æˆåŠŸï¼")
            print("=" * 50)
            print(f"æ•°æ®è¡Œæ•°: {len(df)}")
            print(f"æ•°æ®åˆ—æ•°: {len(df.columns)}")
            
            return df
            
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶æ—¶å‡ºé”™ï¼š{str(e)}")
            return None
    
    def is_numeric_scale(self, column_data, unique_values):
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ•°å€¼å‹é‡è¡¨é¢˜"""
        # æ–¹æ³•1ï¼špandasæ•°æ®ç±»å‹åˆ¤æ–­
        if column_data.dtype in ['int64', 'float64', 'int32', 'float32']:
            return True
        
        # æ–¹æ³•2ï¼šæ£€æŸ¥å­—ç¬¦ä¸²æ˜¯å¦éƒ½æ˜¯æ•°å­—
        if column_data.dtype == 'object':
            try:
                for value in unique_values:
                    if pd.notna(value):
                        float(str(value))  # å°è¯•è½¬æ¢ä¸ºæ•°å­—
                return True
            except (ValueError, TypeError):
                return False
        
        return False

    def preprocess_option_text(self, option):
        """é¢„å¤„ç†é€‰é¡¹æ–‡æœ¬"""
        if pd.isna(option):
            return ""
        
        text = str(option).lower().strip()
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·å’Œè¿æ¥è¯
        text = re.sub(r'[ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘\s\-_]', '', text)
        text = re.sub(r'\b(nor|and|or)\b', '', text)
        text = re.sub(r'(ä¹Ÿä¸|å’Œ|æˆ–è€…)', '', text)
        
        return text

    def check_pattern_match(self, processed_values, pattern_data):
        """æ£€æŸ¥é€‰é¡¹æ˜¯å¦åŒ¹é…ç‰¹å®šæ¨¡å¼ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰"""
        all_options = []
        
        # åˆå¹¶ä¸­è‹±æ–‡é€‰é¡¹
        if 'chinese' in pattern_data:
            all_options.extend([self.preprocess_option_text(opt) for opt in pattern_data['chinese']])
        if 'english' in pattern_data:
            all_options.extend([self.preprocess_option_text(opt) for opt in pattern_data['english']])
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰é€‰é¡¹éƒ½åœ¨é¢„å®šä¹‰æ¨¡å¼ä¸­
        for value in processed_values:
            if value not in all_options:
                return False
                
        return True

    def check_smart_keywords(self, unique_values):
        """æ™ºèƒ½å…³é”®è¯æ£€æµ‹ - è¯†åˆ«åŒ…å«å…³é”®è¯çš„é•¿é€‰é¡¹"""
        # é¢„å¤„ç†é€‰é¡¹
        processed_values = [self.preprocess_option_text(v) for v in unique_values]
        
        # æ£€æŸ¥æ¯ä¸ªå…³é”®è¯ç±»åˆ«
        for keyword_type, keyword_data in self.smart_keywords.items():
            total_keywords = []
            
            # æ”¶é›†æ‰€æœ‰å…³é”®è¯
            if 'chinese' in keyword_data:
                total_keywords.extend([self.preprocess_option_text(kw) for kw in keyword_data['chinese']])
            if 'english' in keyword_data:
                total_keywords.extend([self.preprocess_option_text(kw) for kw in keyword_data['english']])
            
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰é€‰é¡¹éƒ½ç²¾ç¡®åŒ¹é…å…³é”®è¯
            all_match = True
            for value in processed_values:
                if value not in total_keywords:
                    all_match = False
                    break
            
            if all_match:
                return True, f"æ™ºèƒ½å…³é”®è¯æ£€æµ‹-{keyword_type}"
        
        return False, "æ— å…³é”®è¯åŒ¹é…"

    def match_single_choice_pattern(self, unique_values, unique_count):
        """åŒ¹é…å•é€‰é¢˜æ¨¡å¼ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # æ­¥éª¤1ï¼šé¢„å¤„ç†é€‰é¡¹
        processed_values = [self.preprocess_option_text(v) for v in unique_values]
        
        # æ­¥éª¤2ï¼šä¼˜å…ˆè¿›è¡Œç²¾ç¡®æ¨¡å¼åŒ¹é…
        for pattern_type, patterns in self.single_choice_patterns.items():
            for pattern_name, pattern_data in patterns.items():
                if self.check_pattern_match(processed_values, pattern_data):
                    return True, f"ç²¾ç¡®åŒ¹é…-{pattern_type}-{pattern_name}"
        
        # æ­¥éª¤3ï¼šæ™ºèƒ½å…³é”®è¯æ£€æµ‹
        is_keyword_match, keyword_reason = self.check_smart_keywords(unique_values)
        if is_keyword_match:
            return True, keyword_reason
        
        # æ­¥éª¤4ï¼šæ£€æŸ¥é€‰é¡¹æ•°é‡èŒƒå›´ï¼ˆ1-10ä¸ªå³åˆ¤å®šä¸ºå•é€‰é¢˜ï¼‰
        if 1 <= unique_count <= 10:
            return True, f"é€‰é¡¹æ•°é‡ç‰¹å¾({unique_count}ä¸ªé€‰é¡¹åœ¨1-10èŒƒå›´å†…)"
        
        return False, f"é€‰é¡¹æ•°é‡è¶…å‡ºèŒƒå›´(éœ€è¦1-10ä¸ªï¼Œå®é™…{unique_count}ä¸ª)"
    
    def identify_all_question_types(self, df):
        """è¯†åˆ«æ‰€æœ‰é¢˜å‹ - æ–°çš„ä¸‰æ­¥åˆ¤æ–­æ³•"""
        print("=" * 70)
        print("æ™ºèƒ½é—®å·åˆ†æ - é¢˜å‹è¯†åˆ« (æ–°é€»è¾‘)")
        print("=" * 70)
        
        # åˆå§‹åŒ–ç»“æœæ•°æ®ç»“æ„
        all_fields = df.columns.tolist()
        field_types = []
        
        # åˆå§‹åŒ–å„é¢˜å‹åˆ—è¡¨
        scale_questions = []
        single_choice = []
        open_ended = []
        
        print(f"\nğŸ” å¼€å§‹ä¸‰æ­¥é¢˜å‹åˆ¤æ–­ (å…± {len(all_fields)} ä¸ªå­—æ®µ)...")
        print("ğŸ“Š åˆ¤æ–­é¡ºåºï¼š1.é‡è¡¨é¢˜(æ•°å€¼å‹) â†’ 2.å•é€‰é¢˜(å›ºå®šé€‰é¡¹) â†’ 3.å¼€æ”¾é¢˜(å…¶ä½™)")
        
        # å¯¹æ¯ä¸ªå­—æ®µè¿›è¡Œä¸‰æ­¥åˆ¤æ–­
        for column in all_fields:
            column_data = df[column]
            unique_values = column_data.dropna().unique()
            unique_count = len(unique_values)
            
            # å‡†å¤‡å­—æ®µä¿¡æ¯
            field_info = {
                'column': column,
                'unique_count': unique_count,
                'data_type': str(column_data.dtype)
            }
            
            # ç¬¬ä¸€æ­¥ï¼šé‡è¡¨é¢˜è¯†åˆ«ï¼ˆæ•°å€¼å‹ï¼‰
            if self.is_numeric_scale(column_data, unique_values):
                field_info['type'] = 'scale'
                field_info['unique_values'] = sorted(unique_values) if unique_count <= 20 else unique_values[:10]
                scale_questions.append(field_info)
                print(f"      {column} -> ğŸ“Š é‡è¡¨é¢˜ (æ•°å€¼å‹ï¼Œ{unique_count}ä¸ªä¸åŒå€¼)")
                
            # ç¬¬äºŒæ­¥ï¼šå•é€‰é¢˜è¯†åˆ«ï¼ˆå›ºå®šé€‰é¡¹ï¼‰
            else:
                is_single, reason = self.match_single_choice_pattern(unique_values, unique_count)
                if is_single:
                    field_info['type'] = 'single'
                    field_info['unique_values'] = list(unique_values)
                    field_info['match_reason'] = reason
                    single_choice.append(field_info)
                    print(f"      {column} -> âšª å•é€‰é¢˜ ({reason}ï¼Œ{unique_count}ä¸ªé€‰é¡¹)")
                
                # ç¬¬ä¸‰æ­¥ï¼šå¼€æ”¾é¢˜è¯†åˆ«ï¼ˆå…¶ä½™æ‰€æœ‰ï¼‰
                else:
                    field_info['type'] = 'open'
                    field_info['reject_reason'] = reason
                    open_ended.append(field_info)
                    print(f"      {column} -> âœï¸ å¼€æ”¾é¢˜ ({reason}ï¼Œ{unique_count}ä¸ªä¸åŒå€¼)")
            
            # æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
            field_types.append(field_info)
        
        # è¾“å‡ºè¯†åˆ«ç»“æœ
        self.print_new_question_type_results(scale_questions, single_choice, open_ended)
        
        return {
            'field_types': field_types,
            'scale_questions': scale_questions,
            'single_choice': single_choice,
            'open_ended': open_ended,
            'summary': {
                'total_fields': len(all_fields),
                'scale_questions': len(scale_questions),
                'single_choice': len(single_choice),
                'open_ended': len(open_ended)
            }
        }
    
    def print_new_question_type_results(self, scale_questions, single_choice, open_ended):
        """è¾“å‡ºæ–°çš„é¢˜å‹è¯†åˆ«ç»“æœ"""
        
        print(f"\nğŸ“Š é‡è¡¨é¢˜ ({len(scale_questions)} ä¸ª) - æ•°å€¼å‹æ•°æ®:")
        print("-" * 60)
        if scale_questions:
            for i, q in enumerate(scale_questions[:10], 1):
                print(f"{i:2d}. {q['column']}")
                print(f"     æ•°æ®ç±»å‹: {q['data_type']}, ä¸åŒå€¼: {q['unique_count']}ä¸ª")
                if q['unique_count'] <= 10:
                    print(f"     å–å€¼èŒƒå›´: {q['unique_values']}")
            if len(scale_questions) > 10:
                print(f"     ... è¿˜æœ‰ {len(scale_questions)-10} ä¸ªé‡è¡¨é¢˜")
        else:
            print("   æœªå‘ç°é‡è¡¨é¢˜")
        
        print(f"\nâšª å•é€‰é¢˜ ({len(single_choice)} ä¸ª) - å›ºå®šé€‰é¡¹:")
        print("-" * 60)
        if single_choice:
            for i, q in enumerate(single_choice[:10], 1):
                print(f"{i:2d}. {q['column']}")
                print(f"     åŒ¹é…åŸå› : {q.get('match_reason', 'é€šç”¨ç‰¹å¾')}")
                print(f"     é€‰é¡¹æ•°: {q['unique_count']}ä¸ª")
                if q['unique_count'] <= 5:
                    print(f"     é€‰é¡¹å†…å®¹: {list(q['unique_values'])}")
            if len(single_choice) > 10:
                print(f"     ... è¿˜æœ‰ {len(single_choice)-10} ä¸ªå•é€‰é¢˜")
        else:
            print("   æœªå‘ç°å•é€‰é¢˜")
        
        print(f"\nâœï¸ å¼€æ”¾é¢˜ ({len(open_ended)} ä¸ª) - å…¶ä½™é¢˜å‹:")
        print("-" * 60)
        if open_ended:
            for i, q in enumerate(open_ended[:10], 1):
                print(f"{i:2d}. {q['column']}")
                print(f"     æ’é™¤åŸå› : {q.get('reject_reason', 'æ— æ³•å½’ç±»')}")
                print(f"     ä¸åŒå€¼: {q['unique_count']}ä¸ª")
            if len(open_ended) > 10:
                print(f"     ... è¿˜æœ‰ {len(open_ended)-10} ä¸ªå¼€æ”¾é¢˜")
        else:
            print("   æœªå‘ç°å¼€æ”¾é¢˜")
        
        # ç»Ÿè®¡æ‘˜è¦
        total_fields = len(scale_questions) + len(single_choice) + len(open_ended)
        
        print(f"\nğŸ“ˆ æ–°é€»è¾‘ç»Ÿè®¡æ‘˜è¦:")
        print("-" * 60)
        print(f"   æ€»å­—æ®µæ•°: {total_fields} ä¸ª")
        print(f"   é‡è¡¨é¢˜: {len(scale_questions)} ä¸ª ({len(scale_questions)/total_fields*100:.1f}%) - æ•°å€¼å‹")
        print(f"   å•é€‰é¢˜: {len(single_choice)} ä¸ª ({len(single_choice)/total_fields*100:.1f}%) - å›ºå®šé€‰é¡¹")
        print(f"   å¼€æ”¾é¢˜: {len(open_ended)} ä¸ª ({len(open_ended)/total_fields*100:.1f}%) - å…¶ä½™ç±»å‹")
        print(f"   è¯†åˆ«è¦†ç›–ç‡: 100% (æ–°é€»è¾‘ä¸ä¼šé—æ¼ä»»ä½•å­—æ®µ)")
    
    def analyze_scale_question(self, df, column):
        """åˆ†æé‡è¡¨é¢˜ï¼Œè¿”å›ç»“æ„åŒ–çš„åˆ†æç»“æœ"""
        print(f"\nğŸ“Š é‡è¡¨é¢˜åˆ†æï¼š{column}")
        print("=" * 80)
        
        data = df[column].dropna()
        
        if len(data) == 0:
            print("   æ— æœ‰æ•ˆæ•°æ®")
            return None
        
        # å‡†å¤‡è¿”å›ç»“æœ
        result = {
            'column': column,
            'valid_samples': len(data),
            'stats': {},
            'distribution': [],
            'nps': None
        }
        
        # åŸºç¡€ç»Ÿè®¡
        print(f"ğŸ“ˆ åŸºç¡€ç»Ÿè®¡æŒ‡æ ‡:")
        print(f"   æœ‰æ•ˆæ ·æœ¬æ•°: {len(data)}")
        try:
            mean_value = float(data.mean())
            std_value = float(data.std())
            min_value = float(data.min())
            max_value = float(data.max())
            median_value = float(data.median())
            
            print(f"   å¹³å‡åˆ†: {mean_value:.2f}")
            print(f"   æ ‡å‡†å·®: {std_value:.2f}")
            print(f"   æœ€å°å€¼: {min_value}")
            print(f"   æœ€å¤§å€¼: {max_value}")
            print(f"   ä¸­ä½æ•°: {median_value:.2f}")
            
            result['stats'] = {
                'mean': mean_value,
                'std': std_value,
                'min': min_value,
                'max': max_value,
                'median': median_value
            }
        except:
            print("   æ•°æ®ç±»å‹ä¸æ”¯æŒæ•°å€¼ç»Ÿè®¡")
            return None
        
        # åˆ†å¸ƒåˆ†æ
        print(f"\nğŸ“Š åˆ†æ•°åˆ†å¸ƒ:")
        value_counts = data.value_counts().sort_index()
        total = len(data)
        
        for score, count in value_counts.items():
            percentage = (count / total) * 100
            bar = "â–ˆ" * int(percentage / 2)
            print(f"   {str(score):>8s}: {count:3d}äºº ({percentage:5.1f}%) {bar}")
            
            # æ·»åŠ åˆ°ç»“æœä¸­
            result['distribution'].append({
                'score': float(score) if isinstance(score, (int, float)) else str(score),
                'count': int(count),
                'percentage': float(percentage)
            })
        
        # NPSåˆ†æï¼ˆå¦‚æœæ˜¯0-10é‡è¡¨ï¼‰
        try:
            if data.min() >= 0 and data.max() <= 10:
                print(f"\nğŸ¯ NPSåˆ†æ (Net Promoter Score):")
                promoters = len(data[data >= 9])
                passives = len(data[(data >= 7) & (data <= 8)])
                detractors = len(data[data <= 6])
                
                nps = ((promoters - detractors) / total) * 100
                
                print(f"   æ¨èè€… (9-10åˆ†): {promoters}äºº ({promoters/total*100:.1f}%)")
                print(f"   ä¸­æ€§è€… (7-8åˆ†):  {passives}äºº ({passives/total*100:.1f}%)")
                print(f"   æ‰¹è¯„è€… (0-6åˆ†):  {detractors}äºº ({detractors/total*100:.1f}%)")
                print(f"   NPSå¾—åˆ†: {nps:.1f}")
                
                rating = ""
                if nps >= 50:
                    rating = "ä¼˜ç§€"
                    print(f"   è¯„ä»·: ğŸŒŸ ä¼˜ç§€")
                elif nps >= 0:
                    rating = "è‰¯å¥½"
                    print(f"   è¯„ä»·: ğŸ‘ è‰¯å¥½")
                else:
                    rating = "éœ€è¦æ”¹è¿›"
                    print(f"   è¯„ä»·: ğŸ‘ éœ€è¦æ”¹è¿›")
                    
                result['nps'] = {
                    'score': float(nps),
                    'promoters': int(promoters),
                    'promoters_percentage': float(promoters/total*100),
                    'passives': int(passives),
                    'passives_percentage': float(passives/total*100),
                    'detractors': int(detractors),
                    'detractors_percentage': float(detractors/total*100),
                    'rating': rating
                }
        except:
            pass  # è·³è¿‡éæ•°å€¼å‹æ•°æ®çš„NPSåˆ†æ
            
        return result
    
    def is_option_selected(self, value):
        """åˆ¤æ–­é€‰é¡¹æ˜¯å¦è¢«é€‰æ‹©ï¼ˆæ”¯æŒå¤šç§æ ‡è®°æ–¹å¼ï¼‰"""
        if pd.isna(value):
            return False
        
        value_str = str(value).strip().lower()
        
        # å„ç§"é€‰æ‹©"çš„æ ‡è®°æ–¹å¼
        selected_markers = [
            'selected', 'yes', 'y', '1', 'æ˜¯', 'é€‰ä¸­', 'âˆš', 'true', 
            'é€‰æ‹©', 'å‹¾é€‰', 'checked', 'choose', 'pick'
        ]
        
        return value_str in selected_markers
    
    def analyze_multiple_choice_question(self, df, question_stem, options):
        """åˆ†æå¤šé€‰é¢˜"""
        print(f"\nğŸ”˜ å¤šé€‰é¢˜åˆ†æï¼š{question_stem}")
        print("=" * 80)
        
        total_responses = len(df)
        option_stats = []
        
        print(f"ğŸ“Š å„é€‰é¡¹é€‰æ‹©æƒ…å†µ:")
        
        for opt in options:
            column = opt['full_column']
            option_text = opt['option']
            
            if column in df.columns:
                # è®¡ç®—é€‰æ‹©è¯¥é€‰é¡¹çš„äººæ•°ï¼ˆä½¿ç”¨é€šç”¨åˆ¤æ–­å‡½æ•°ï¼‰
                selected = df[column].apply(self.is_option_selected)
                count = selected.sum()
                percentage = (count / total_responses) * 100
                
                option_stats.append({
                    'option': option_text,
                    'count': count,
                    'percentage': percentage
                })
        
        # æŒ‰é€‰æ‹©äººæ•°æ’åº
        option_stats.sort(key=lambda x: x['count'], reverse=True)
        
        for i, stat in enumerate(option_stats, 1):
            bar = "â–ˆ" * int(stat['percentage'] / 2)  # æ¯2%ä¸€ä¸ªå­—ç¬¦
            print(f"   {i:2d}. {stat['option'][:60]}")
            print(f"       {stat['count']:3d}äºº ({stat['percentage']:5.1f}%) {bar}")
            print()
        
        print(f"ğŸ“ˆ æ€»ç»“:")
        if option_stats:
            most_selected = option_stats[0]
            least_selected = option_stats[-1]
            print(f"   æœ€å—å…³æ³¨: {most_selected['option'][:40]} ({most_selected['percentage']:.1f}%)")
            print(f"   æœ€å°‘é€‰æ‹©: {least_selected['option'][:40]} ({least_selected['percentage']:.1f}%)")
            print(f"   å¹³å‡é€‰æ‹©ç‡: {sum(s['percentage'] for s in option_stats) / len(option_stats):.1f}%")
    
    def filter_questions_interactive(self, df, question_types):
        """äº¤äº’å¼å­—æ®µç­›é€‰åŠŸèƒ½ï¼ˆä¸å†åŒºåˆ†å…ƒæ•°æ®/é—®é¢˜å­—æ®µï¼Œå…¨éƒ¨å­—æ®µéƒ½å¯é€‰ï¼‰"""
        print("\n" + "=" * 70)
        print("æ™ºèƒ½å­—æ®µç­›é€‰åŠŸèƒ½")
        print("=" * 70)
        
        all_fields = df.columns.tolist()
        print(f"ğŸ“‹ å¯ç”¨å­—æ®µ: {len(all_fields)} ä¸ª")
        for field in all_fields[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
            print(f"   â€¢ {field}")
        if len(all_fields) > 10:
            print(f"   â€¢ ... è¿˜æœ‰ {len(all_fields)-10} ä¸ª")
        
        print("\nè¯·é€‰æ‹©ç­›é€‰æ–¹å¼:")
        print("1. æŒ‰é¢˜å‹ç­›é€‰")
        print("2. æŒ‰å­—æ®µåç­›é€‰ (å¦‚ Q1, å§“å, å¹´é¾„)")
        print("3. æ‰‹åŠ¨é€‰æ‹©å­—æ®µ")
        print("4. è·³è¿‡ç­›é€‰ï¼Œåˆ†ææ‰€æœ‰å­—æ®µ")
        
        choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-4): ").strip()
        
        if choice == "1":
            return self.filter_by_question_type(df, question_types)
        elif choice == "2":
            return self.filter_by_field_name(df, question_types)
        elif choice == "3":
            return self.filter_by_manual_selection(df, question_types)
        elif choice == "4":
            print("è·³è¿‡ç­›é€‰ï¼Œå°†åˆ†ææ‰€æœ‰å­—æ®µ")
            return df, question_types
        else:
            print("æ— æ•ˆé€‰æ‹©ï¼Œè·³è¿‡ç­›é€‰")
            return df, question_types
    
    def filter_by_question_type(self, df, question_types):
        """æŒ‰é¢˜å‹ç­›é€‰ - ç®€åŒ–ä¸ºä¸‰ç§é¢˜å‹ï¼šé‡è¡¨é¢˜ã€å•é€‰é¢˜å’Œå¼€æ”¾é¢˜"""
        print("\nå¯é€‰é¢˜å‹:")
        print("1. é‡è¡¨é¢˜") 
        print("2. å•é€‰é¢˜")
        print("3. å¼€æ”¾é¢˜")
        
        type_choice = input("è¯·é€‰æ‹©é¢˜å‹ (1-3): ").strip()
        
        selected_columns = []
        filtered_types = {}
        
        if type_choice == "1" and question_types['scale_questions']:
            selected_columns = [q['column'] for q in question_types['scale_questions']]
            filtered_types['scale_questions'] = question_types['scale_questions']
            print(f"å·²é€‰æ‹© {len(question_types['scale_questions'])} ä¸ªé‡è¡¨é¢˜")
            
        elif type_choice == "2" and question_types['single_choice']:
            selected_columns = [q['column'] for q in question_types['single_choice']]
            filtered_types['single_choice'] = question_types['single_choice']
            print(f"å·²é€‰æ‹© {len(question_types['single_choice'])} ä¸ªå•é€‰é¢˜")
            
        elif type_choice == "3" and question_types['open_ended']:
            selected_columns = [q['column'] for q in question_types['open_ended']]
            filtered_types['open_ended'] = question_types['open_ended']
            print(f"å·²é€‰æ‹© {len(question_types['open_ended'])} ä¸ªå¼€æ”¾é¢˜")
            
        elif type_choice == "4":
            print("æ–°é€»è¾‘ä¸­å·²ç§»é™¤å…¶ä»–é¢˜å‹ï¼Œæ‰€æœ‰å­—æ®µéƒ½ä¼šè¢«åˆ†ç±»ä¸ºé‡è¡¨é¢˜ã€å•é€‰é¢˜æˆ–å¼€æ”¾é¢˜")
            
        else:
            print("æ— æ•ˆé€‰æ‹©æˆ–è¯¥é¢˜å‹æ— é—®é¢˜")
            return df, question_types
        
        # ç­›é€‰åˆ—
        final_columns = [col for col in selected_columns if col in df.columns]
        
        filtered_df = df[final_columns]
        print(f"ç­›é€‰åæ•°æ®: {len(filtered_df)} è¡Œ x {len(filtered_df.columns)} åˆ—")
        print(f"é—®é¢˜å­—æ®µ: {len(selected_columns)} ä¸ª")
        
        # é‡æ–°ç”Ÿæˆfield_types
        filtered_types['field_types'] = [ft for ft in question_types['field_types'] if ft['column'] in final_columns]
        filtered_types['summary'] = {
            'total_fields': len(final_columns),
            'scale_questions': len(filtered_types.get('scale_questions', [])),
            'single_choice': len(filtered_types.get('single_choice', [])),
            'open_ended': len(filtered_types.get('open_ended', []))
        }
        
        return filtered_df, filtered_types
    
    def filter_by_field_name(self, df, question_types):
        """æŒ‰å­—æ®µåç­›é€‰ï¼ˆç”¨æˆ·è¾“å…¥å­—æ®µåï¼Œé€—å·åˆ†éš”ï¼‰"""
        field_names = input("è¯·è¾“å…¥å­—æ®µå (ç”¨é€—å·åˆ†éš”ï¼Œå¦‚ Q1,å§“å,å¹´é¾„): ").strip()
        if not field_names:
            print("æœªè¾“å…¥å­—æ®µå")
            return df, question_types
        names = [q.strip() for q in field_names.split(',')]
        selected_columns = [col for col in df.columns if col in names]
        if not selected_columns:
            print("æœªæ‰¾åˆ°åŒ¹é…çš„å­—æ®µ")
            return df, question_types
        filtered_df = df[selected_columns]
        print(f"\nç­›é€‰ç»“æœ:")
        print(f"  â€¢ æ€»å­—æ®µæ•°: {len(selected_columns)} ä¸ª")
        print(f"  â€¢ ç­›é€‰åæ•°æ®: {len(filtered_df)} è¡Œ x {len(filtered_df.columns)} åˆ—")
        filtered_types = self.identify_all_question_types(filtered_df)
        return filtered_df, filtered_types
    
    def filter_by_manual_selection(self, df, question_types):
        """æ‰‹åŠ¨é€‰æ‹©å­—æ®µï¼ˆå…¨éƒ¨å­—æ®µéƒ½å¯é€‰ï¼‰"""
        all_fields = df.columns.tolist()
        print("\nå¯ç”¨å­—æ®µ:")
        for i, col in enumerate(all_fields, 1):
            print(f"F{i:2d}. {col}")
        print("\né€‰æ‹©æ–¹å¼:")
        print("  â€¢ å­—æ®µä½¿ç”¨ F1,F2... è¡¨ç¤º")
        print("  â€¢ å¯æ··åˆé€‰æ‹©ï¼Œå¦‚: F1,F3,F5")
        selection = input(f"\nè¯·è¾“å…¥é€‰æ‹©å†…å®¹: ").strip()
        if not selection:
            print("æœªé€‰æ‹©å­—æ®µ")
            return df, question_types
        selected_columns = []
        selection_items = [item.strip() for item in selection.split(',')]
        try:
            for item in selection_items:
                item = item.strip().upper()
                if item.startswith('F'):
                    idx = int(item[1:]) - 1
                    if 0 <= idx < len(all_fields):
                        selected_columns.append(all_fields[idx])
        except ValueError:
            print("è¾“å…¥æ ¼å¼é”™è¯¯")
            return df, question_types
        if not selected_columns:
            print("æ— æ•ˆçš„é€‰æ‹©")
            return df, question_types
        final_columns = [col for col in selected_columns if col in df.columns]
        filtered_df = df[final_columns]
        print(f"\nç­›é€‰ç»“æœ:")
        print(f"  â€¢ å­—æ®µæ•°: {len(final_columns)} ä¸ª")
        print(f"  â€¢ ç­›é€‰åæ•°æ®: {len(filtered_df)} è¡Œ x {len(filtered_df.columns)} åˆ—")
        filtered_types = self.identify_all_question_types(filtered_df)
        return filtered_df, filtered_types
    
    def run_analysis(self):
        """è¿è¡Œå®Œæ•´åˆ†ææµç¨‹"""
        # 1. é€‰æ‹©æ–‡ä»¶
        file_path = self.select_file_interactive()
        if not file_path:
            return
        
        # 2. è¯»å–æ•°æ®
        df = self.read_data_file(file_path)
        if df is None:
            return
        
        # 3. é¢˜å‹åˆ†æï¼ˆç›´æ¥å¯¹æ‰€æœ‰å­—æ®µï¼‰
        question_types = self.identify_all_question_types(df)
        
        # 4. å­—æ®µç­›é€‰ï¼ˆå¯é€‰ï¼Œå…¨éƒ¨å­—æ®µéƒ½å¯é€‰ï¼Œä¸å†åŒºåˆ†å…ƒæ•°æ®/é—®é¢˜å­—æ®µï¼‰
        print("\nç¬¬å››æ­¥ï¼šå­—æ®µç­›é€‰")
        filtered_df, filtered_types = self.filter_questions_interactive(df, question_types)
        
        # 5. è¿›è¡ŒåŸºç¡€åˆ†æ
        print(f"\n" + "=" * 80)
        print("æ•°æ®åˆ†æç»“æœ")
        print("=" * 80)
        
        # åˆ†æé‡è¡¨é¢˜ï¼ˆé™åˆ¶å‰2ä¸ªï¼‰
        scale_questions = filtered_types.get('scale_questions', [])
        for q in scale_questions[:2]:
            self.analyze_scale_question(filtered_df, q['column'])
        
        print(f"\nâœ… åˆ†æå®Œæˆï¼")
        print(f"ğŸ“ æ–‡ä»¶: {os.path.basename(file_path)}")
        print(f"ğŸ“Š æ•°æ®: {len(filtered_df)} è¡Œ Ã— {len(filtered_df.columns)} åˆ—")
        print(f"ğŸ¯ è¯†åˆ«: {len(filtered_types.get('scale_questions', []))} é‡è¡¨é¢˜, "
              f"{len(filtered_types.get('single_choice', []))} å•é€‰é¢˜, "
              f"{len(filtered_types.get('open_ended', []))} å¼€æ”¾é¢˜")
        
        return filtered_df, filtered_types

    def analyze_single_field(self, df, field, question_types):
        """åˆ†æå•ä¸ªå­—æ®µ"""
        # è·å–å­—æ®µç±»å‹
        field_type = None
        field_info = None
        
        # æ£€æŸ¥å­—æ®µç±»å‹
        if question_types.get('scale_questions'):
            for q in question_types['scale_questions']:
                if q['column'] == field:
                    field_type = 'scale'
                    field_info = q
                    break
                    
        if not field_type and question_types.get('single_choice'):
            for q in question_types['single_choice']:
                if q['column'] == field:
                    field_type = 'single'
                    field_info = q
                    break
                    
        if not field_type and question_types.get('open_ended'):
            for q in question_types['open_ended']:
                if q['column'] == field:
                    field_type = 'open'
                    field_info = q
                    break
        
        results = {
            'field': field,
            'type': field_type,
            'analysis': {}
        }
        
        # æ ¹æ®å­—æ®µç±»å‹è¿›è¡Œç›¸åº”çš„åˆ†æ
        if field_type == 'scale':
            # é‡è¡¨é¢˜åˆ†æ
            scale_analysis = self.analyze_scale_question(df, field)
            results['analysis'] = {
                'mean': scale_analysis['mean'],
                'median': scale_analysis['median'],
                'mode': scale_analysis['mode'],
                'distribution': scale_analysis['distribution']
            }
            
        elif field_type == 'single':
            # å•é€‰é¢˜åˆ†æ
            choice_analysis = self.analyze_multiple_choice_question(df, field, field_info)
            results['analysis'] = {
                'options': choice_analysis['options'],
                'counts': choice_analysis['counts'],
                'percentages': choice_analysis['percentages']
            }
            
        elif field_type == 'open':
            # å¼€æ”¾é¢˜åˆ†æ
            open_ended_values = df[field].dropna().tolist()
            results['analysis'] = {
                'response_count': len(open_ended_values),
                'sample_responses': open_ended_values[:5] if open_ended_values else []
            }
            
        return results

def main():
    """ä¸»å‡½æ•°"""
    analyzer = UniversalQuestionnaireAnalyzer()
    return analyzer.run_analysis()

# Note: Flaské…ç½®å·²åœ¨æ–‡ä»¶å¼€å¤´å®Œæˆï¼Œé¿å…é‡å¤å®šä¹‰

def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def parse_file_content(file_path, file_name):
    """è§£ææ–‡ä»¶å†…å®¹ä¸ºç»“æ„åŒ–æ•°æ®"""
    ext = os.path.splitext(file_name)[1].lower()
    
    if ext == '.csv':
        # å°è¯•ä¸åŒçš„ç¼–ç æ ¼å¼
        for encoding in ['utf-8', 'gbk', 'gb2312', 'latin1']:
            try:
                df = pd.read_csv(file_path, encoding=encoding)
                break
            except UnicodeDecodeError:
                continue
        else:
            raise Exception("æ— æ³•è¯†åˆ«CSVæ–‡ä»¶ç¼–ç æ ¼å¼")
    elif ext in ['.xlsx', '.xls']:
        df = pd.read_excel(file_path)
    elif ext == '.txt':
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        data = []
        for i, line in enumerate(lines):
            if line.strip():
                data.append({
                    'id': i + 1,
                    'content': line.strip(),
                    'timestamp': datetime.now().isoformat()
                })
        return data
    else:
        raise Exception(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {ext}")
    
    return df.to_dict('records')

def analyze_with_openai(content, prompt):
    """ä½¿ç”¨OpenAIè¿›è¡Œåˆ†æ"""
    if not OPENAI_AVAILABLE:
        return "OpenAIåº“æœªå®‰è£…ï¼Œæ— æ³•è¿›è¡ŒAIåˆ†æ"
    
    try:
        response = openai_client.chat.completions.create(
            model=os.getenv('OPENAI_MODEL', 'gpt-4'),
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆã€‚"},
                {"role": "user", "content": f"{prompt}\n\n{content}"}
            ],
            max_tokens=2000,
            temperature=0.3
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"AIåˆ†æå¤±è´¥: {str(e)}"

# æ³¨æ„ï¼šupload-questionnaire è·¯ç”±å·²åœ¨ main.py ä¸­å®šä¹‰ï¼Œè¿™é‡Œä¸å†é‡å¤å®šä¹‰ä»¥é¿å…å†²çª

@app.route('/analyze-text', methods=['POST'])
def analyze_text():
    """é—®å·æ™ºèƒ½åˆ†æ"""
    try:
        data = request.json
        analysis_id = data.get('analysisId')
        
        if not analysis_id or analysis_id not in analysis_results:
            return jsonify({'error': 'æ— æ•ˆçš„åˆ†æID'}), 400
        
        analysis_data = analysis_results[analysis_id]
        parsed_data = analysis_data['parsedData']
        
        if not parsed_data:
            return jsonify({'error': 'æ–‡ä»¶å†…å®¹ä¸ºç©º'}), 400
        
        # å¼€å§‹åˆ†æ
        analysis_data['status'] = 'analyzing'
        
        analyzer = UniversalQuestionnaireAnalyzer()
        df = pd.DataFrame(parsed_data)
        
        # æ‰§è¡Œè¯¦ç»†åˆ†æ
        analysis_result = {}
        
        # è¯†åˆ«æ‰€æœ‰é¢˜å‹ - å¯¹æ¯ä¸ªå­—æ®µè¿›è¡Œé¢˜å‹åˆ†ç±»
        question_types = analyzer.identify_all_question_types(df)
        
        # åˆ†æé‡è¡¨é¢˜ - å¤„ç†æ‰€æœ‰è¢«è¯†åˆ«ä¸ºé‡è¡¨é¢˜çš„å­—æ®µ
        scale_questions = []
        for q in question_types.get('scale_questions', []):
            scale_analysis = analyzer.analyze_scale_question(df, q['column'])
            if scale_analysis:  # ç¡®ä¿åˆ†æç»“æœä¸ä¸ºç©º
                scale_questions.append(scale_analysis)
        
        # åˆ†æå•é€‰é¢˜ - å¤„ç†æ‰€æœ‰è¢«è¯†åˆ«ä¸ºå•é€‰é¢˜çš„å­—æ®µ
        single_choice = []
        for q in question_types.get('single_choice', []):
            single_analysis = {
                'column': q['column'], 
                'unique_count': q['unique_count'],
                'unique_values': q['unique_values']
            }
            single_choice.append(single_analysis)
        
        # åˆ†æå¼€æ”¾é¢˜ - å¤„ç†æ‰€æœ‰è¢«è¯†åˆ«ä¸ºå¼€æ”¾é¢˜çš„å­—æ®µ
        open_ended = []
        for q in question_types.get('open_ended', []):
            open_analysis = {
                'column': q['column'], 
                'unique_count': q['unique_count'],
                'sample_responses': df[q['column']].dropna().tolist()[:5]
            }
            open_ended.append(open_analysis)
        
        # ä¿å­˜åˆ†æç»“æœ
        analysis_result['question_types'] = question_types
        analysis_result['scale_questions'] = scale_questions
        analysis_result['single_choice'] = single_choice
        analysis_result['open_ended'] = open_ended
        analysis_result['summary'] = {
            'totalFields': len(question_types['field_types']),
            'scaleQuestions': len(scale_questions),
            'singleChoice': len(single_choice),
            'openEnded': len(open_ended)
        }
        
        # æ›´æ–°åˆ†æç»“æœ
        analysis_data['analysis'] = analysis_result
        analysis_data['status'] = 'completed'
        analysis_data['completedTime'] = datetime.now().isoformat()
        analysis_results[analysis_id] = analysis_data
        
        return jsonify({
            'success': True,
            'analysisId': analysis_id,
            'results': analysis_result
        })
    
    except Exception as e:
        return jsonify({'error': f'æ™ºèƒ½åˆ†æå¤±è´¥: {str(e)}'}), 500

@app.route('/statistics', methods=['POST'])
def get_statistics():
    """è·å–ç»Ÿè®¡åˆ†æç»“æœ"""
    try:
        data = request.get_json()
        analysis_id = data.get('analysisId')
        selected_fields = data.get('selectedFields', [])
        question_types = data.get('questionTypes', {})
        
        if not analysis_id or not selected_fields:
            return jsonify({'error': 'ç¼ºå°‘å¿…è¦å‚æ•°'}), 400
            
        # è·å–æ•°æ®æ–‡ä»¶è·¯å¾„
        file_info = analysis_results.get(analysis_id)
        if not file_info:
            return jsonify({'error': 'æ‰¾ä¸åˆ°å¯¹åº”çš„åˆ†ææ•°æ®'}), 404
            
        file_path = file_info['file_path']
        if not os.path.exists(file_path):
            return jsonify({'error': 'æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨'}), 404
            
        # è¯»å–æ•°æ®
        analyzer = UniversalQuestionnaireAnalyzer()
        df = analyzer.read_data_file(file_path)
        if df is None:
            return jsonify({'error': 'è¯»å–æ•°æ®æ–‡ä»¶å¤±è´¥'}), 500
            
        # å¯¹æ¯ä¸ªå­—æ®µè¿›è¡Œåˆ†æ
        results = []
        for field in selected_fields:
            field_result = analyzer.analyze_single_field(df, field, question_types)
            results.append(field_result)
            
        return jsonify({
            'results': results
        })
        
    except Exception as e:
        print(f"ç»Ÿè®¡åˆ†æå‡ºé”™: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/trend-analysis', methods=['POST'])
def trend_analysis():
    """è¶‹åŠ¿åˆ†æ"""
    try:
        data = request.json
        analysis_id = data.get('analysisId')
        
        if not analysis_id or analysis_id not in analysis_results:
            return jsonify({'error': 'æ— æ•ˆçš„åˆ†æID'}), 400
        
        analysis_data = analysis_results[analysis_id]
        parsed_data = analysis_data['parsedData']
        analysis = analysis_data['analysis']
        
        if not parsed_data or not analysis:
            return jsonify({'error': 'è¯·å…ˆå®Œæˆæ–‡æœ¬åˆ†æ'}), 400
        
        # æ‰§è¡Œè¶‹åŠ¿åˆ†æ
        trends = {
            'timeSeries': [],
            'trendSummary': 'è¶‹åŠ¿åˆ†æå®Œæˆ',
            'recommendations': ['å»ºè®®1', 'å»ºè®®2', 'å»ºè®®3']
        }
        
        # æ›´æ–°è¶‹åŠ¿ç»“æœ
        analysis_data['trends'] = trends
        analysis_results[analysis_id] = analysis_data
        
        return jsonify({
            'success': True,
            'analysisId': analysis_id,
            'trends': trends
        })
    
    except Exception as e:
        return jsonify({'error': f'è¶‹åŠ¿åˆ†æå¤±è´¥: {str(e)}'}), 500

@app.route('/analysis-results/<analysis_id>', methods=['GET'])
def get_analysis_results(analysis_id):
    """è·å–åˆ†æç»“æœ"""
    try:
        if analysis_id not in analysis_results:
            return jsonify({'error': 'åˆ†æç»“æœä¸å­˜åœ¨'}), 404
        
        result = analysis_results[analysis_id]
        return jsonify({
            'success': True,
            'result': result
        })
    
    except Exception as e:
        return jsonify({'error': 'è·å–åˆ†æç»“æœå¤±è´¥'}), 500

@app.route('/analysis-history', methods=['GET'])
def get_analysis_history():
    """è·å–åˆ†æå†å²"""
    try:
        history = []
        for result in analysis_results.values():
            history.append({
                'id': result['id'],
                'fileName': result['fileName'],
                'uploadTime': result['uploadTime'],
                'completedTime': result.get('completedTime'),
                'status': result['status'],
                'totalRecords': len(result['parsedData']) if result['parsedData'] else 0,
                'hasAnalysis': bool(result['analysis']),
                'hasStatistics': bool(result['statistics']),
                'hasTrends': bool(result['trends'])
            })
        
        return jsonify({
            'success': True,
            'history': history
        })
    
    except Exception as e:
        return jsonify({'error': 'è·å–å†å²è®°å½•å¤±è´¥'}), 500

@app.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥"""
    return jsonify({
        'status': 'healthy',
        'timestamp': datetime.now().isoformat(),
        'service': 'Universal Questionnaire Analyzer'
    })

def run_server():
    """å¯åŠ¨FlaskæœåŠ¡å™¨"""
    print("=" * 60)
    print("å¯åŠ¨é€šç”¨é—®å·åˆ†æAPIæœåŠ¡å™¨")
    print("=" * 60)
    print(f"æœåŠ¡å™¨åœ°å€: http://localhost:9000")
    print(f"ä¸Šä¼ ç›®å½•: {UPLOAD_FOLDER}")
    print(f"æ”¯æŒæ ¼å¼: {', '.join(ALLOWED_EXTENSIONS)}")
    print("=" * 60)
    
    app.run(host='0.0.0.0', port=9000, debug=True)

if __name__ == "__main__":
    print("é€‰æ‹©è¿è¡Œæ¨¡å¼:")
    print("1. äº¤äº’å¼é—®å·åˆ†æ")
    print("2. å¯åŠ¨APIæœåŠ¡å™¨")
    
    choice = input("è¯·é€‰æ‹© (1-2): ").strip()
    
    if choice == "1":
        questionnaire_data, question_types = main()
    elif choice == "2":
        run_server()
    else:
        print("æ— æ•ˆé€‰æ‹©")