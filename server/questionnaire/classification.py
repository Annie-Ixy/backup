import os
import pandas as pd
import time
from pathlib import Path
import re
import random
import sys
import logging
from collections import defaultdict, Counter
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity

# httpxå¯¼å…¥ï¼ˆå¯é€‰ï¼‰
try:
    import httpx
    HTTPX_AVAILABLE = True
except ImportError:
    HTTPX_AVAILABLE = False

# OpenAIå¯¼å…¥ï¼ˆå¯é€‰ï¼‰
try:
    from openai import OpenAI, RateLimitError, APIConnectionError, APIError, AuthenticationError, PermissionDeniedError
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    # å®šä¹‰æ›¿ä»£å¼‚å¸¸ç±»ï¼Œé¿å…æœªå¯¼å…¥OpenAIæ—¶çš„é”™è¯¯
    class RateLimitError(Exception):
        pass
    class APIConnectionError(Exception):
        pass
    class APIError(Exception):
        pass
    class AuthenticationError(Exception):
        pass
    class PermissionDeniedError(Exception):
        pass

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("api_processing.log", encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger()

# å¯¼å…¥universal_questionnaire_analyzeræ¨¡å—
try:
    from universal_questionnaire_analyzer import UniversalQuestionnaireAnalyzer
    logger.info("âœ… æˆåŠŸå¯¼å…¥ UniversalQuestionnaireAnalyzer")
except ImportError as e:
    logger.error(f"âŒ æ— æ³•å¯¼å…¥ UniversalQuestionnaireAnalyzer: {e}")
    logger.error("è¯·ç¡®ä¿ universal_questionnaire_analyzer.py æ–‡ä»¶åœ¨åŒä¸€ç›®å½•ä¸‹")
    sys.exit(1)

# åŠ è½½.envæ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡ï¼ˆå¯é€‰ï¼‰
def load_env_variables():
    """åŠ è½½ç¯å¢ƒå˜é‡ï¼Œæ”¯æŒå¤šç§æ–¹å¼"""
    try:
        from dotenv import load_dotenv
        load_dotenv()
        logger.info("âœ… æˆåŠŸé€šè¿‡dotenvåŠ è½½ç¯å¢ƒå˜é‡")
        return True
    except ImportError:
        logger.warning("âš ï¸ python-dotenvåº“æœªå®‰è£…ï¼Œå°è¯•æ‰‹åŠ¨åŠ è½½.envæ–‡ä»¶")
        
        # æ‰‹åŠ¨åŠ è½½.envæ–‡ä»¶
        env_file = Path(__file__).parent / '.env'
        if env_file.exists():
            try:
                with open(env_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        if line and not line.startswith('#') and '=' in line:
                            key, value = line.split('=', 1)
                            os.environ[key.strip()] = value.strip()
                logger.info("âœ… æˆåŠŸæ‰‹åŠ¨åŠ è½½.envæ–‡ä»¶")
                return True
            except Exception as e:
                logger.error(f"âŒ æ‰‹åŠ¨åŠ è½½.envæ–‡ä»¶å¤±è´¥: {e}")
        else:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°.envæ–‡ä»¶")
        
        return False

# åŠ è½½ç¯å¢ƒå˜é‡
load_env_variables()


class QuestionnaireTranslationClassifier:
    """é—®å·ç¿»è¯‘åˆ†ç±»å™¨ - ç¿»è¯‘å†…å®¹å¹¶ç”Ÿæˆåˆ†ç±»æ ‡ç­¾ï¼Œä½¿ç”¨UniversalQuestionnaireAnalyzerè¿›è¡Œé—®é¢˜è¯†åˆ«"""
    
    def __init__(self):
        # åˆå§‹åŒ–é—®å·åˆ†æå™¨
        self.analyzer = UniversalQuestionnaireAnalyzer()
        
        # æ–°å¢ï¼šå‚è€ƒæ ‡ç­¾ç›¸å…³å±æ€§
        self.reference_tags = []
        self.use_reference_mode = False
        
        # æ£€æŸ¥OpenAIæ˜¯å¦å¯ç”¨
        if not OPENAI_AVAILABLE:
            logger.warning("âš ï¸ OpenAIåº“æœªå®‰è£…ï¼Œç¿»è¯‘å’Œåˆ†ç±»åŠŸèƒ½å°†è¢«ç¦ç”¨")
            self.client = None
            return
        
        # æ£€æŸ¥å¿…è¦çš„ç¯å¢ƒå˜é‡
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°OPENAI_API_KEYç¯å¢ƒå˜é‡ï¼ŒOpenAIåŠŸèƒ½å°†è¢«ç¦ç”¨")
            self.client = None
            return
        
        # è®¾ç½®ä»£ç†é…ç½®ï¼ˆè‡ªåŠ¨è¯»å–.envï¼‰
        proxy_url = os.getenv("PROXY_URL")
        if proxy_url:
            os.environ['http_proxy'] = proxy_url
            os.environ['https_proxy'] = proxy_url
            logger.info(f"ğŸŒ å·²è®¾ç½®ä»£ç†: {proxy_url}")
        else:
            logger.warning("æœªæ£€æµ‹åˆ° PROXY_URL ç¯å¢ƒå˜é‡ï¼Œæœªè®¾ç½®ä»£ç†")
        
        # åˆ›å»ºè‡ªå®šä¹‰ HTTP å®¢æˆ·ç«¯ - å¢å¼ºè¿æ¥ç¨³å®šæ€§
        if HTTPX_AVAILABLE:
            if proxy_url:
                http_client = httpx.Client(
                    transport=httpx.HTTPTransport(retries=5),
                    timeout=httpx.Timeout(120.0),
                    proxies={"all://": proxy_url}
                )
            else:
                http_client = httpx.Client(
                    transport=httpx.HTTPTransport(retries=5),
                    timeout=httpx.Timeout(120.0)
                )
        else:
            logger.warning("âš ï¸ httpxåº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨é»˜è®¤HTTPå®¢æˆ·ç«¯")
            http_client = None
        
        # è®¾ç½®OpenAI API - æ·»åŠ æ›´å¤šé‡è¯•æœºåˆ¶
        try:
            self.client = OpenAI(
                api_key=os.getenv("OPENAI_API_KEY"),
                base_url=os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1"),
                http_client=http_client,
                max_retries=5  # æ·»åŠ å®¢æˆ·ç«¯çº§åˆ«çš„é‡è¯•
            )
            logger.info("âœ… OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            logger.warning("âš ï¸ OpenAIåŠŸèƒ½å°†è¢«ç¦ç”¨    city")
            self.client = OpenAI(
                api_key=os.getenv("OPENAI_API_KEY"),
                base_url=os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1"),
                http_client=http_client,
                max_retries=5  # æ·»åŠ å®¢æˆ·ç«¯çº§åˆ«çš„é‡è¯•
            )
            # self.client = None
    
    def set_reference_tags(self, reference_tags):
        """
        è®¾ç½®å‚è€ƒæ ‡ç­¾
        Args:
            reference_tags: å‚è€ƒæ ‡ç­¾åˆ—è¡¨
            [
                {
                    'name': 'æœåŠ¡è´¨é‡',
                    'definition': 'æœåŠ¡å“åº”é€Ÿåº¦å’Œæ•ˆç‡ç›¸å…³é—®é¢˜',
                    'examples': ['å“åº”æ…¢', 'æœåŠ¡æ€åº¦', 'å¤„ç†æ•ˆç‡']
                }
            ]
        """
        self.reference_tags = reference_tags
        self.use_reference_mode = True
        logger.info(f"ğŸ“‹ å·²è®¾ç½® {len(reference_tags)} ä¸ªå‚è€ƒæ ‡ç­¾")
        for tag in reference_tags:
            logger.info(f"  - {tag['name']}: {tag['definition']}")
    
    def assign_tags_based_on_reference(self, translated_texts, retry_count=0):
        """
        åŸºäºå‚è€ƒæ ‡ç­¾å¯¹ç¿»è¯‘åçš„æ–‡æœ¬è¿›è¡Œæ ‡ç­¾åˆ†é…
        
        Args:
            translated_texts: å·²ç¿»è¯‘çš„æ–‡æœ¬åˆ—è¡¨
            retry_count: é‡è¯•æ¬¡æ•°
        
        Returns:
            æ ‡ç­¾åˆ†é…ç»“æœåˆ—è¡¨: ["æ ‡ç­¾1,æ ‡ç­¾2", "æ ‡ç­¾3", ...]
        """
        if not translated_texts or all(not text.strip() for text in translated_texts):
            return [""] * len(translated_texts)
        
        if not self.reference_tags:
            logger.warning("âš ï¸ æœªè®¾ç½®å‚è€ƒæ ‡ç­¾ï¼Œä½¿ç”¨åŸæœ‰æ–¹æ³•")
            return self._rule_based_tag_assignment(translated_texts)
        
        if not self.client:
            logger.warning("âš ï¸ OpenAIå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œä½¿ç”¨è§„åˆ™åŒ¹é…æ–¹æ³•")
            return self._rule_based_tag_assignment(translated_texts)
        
        # æ„å»ºå‚è€ƒæ ‡ç­¾ä¿¡æ¯
        reference_info = self._build_reference_tags_prompt()
        
        # æ„å»ºæ‰¹é‡æ–‡æœ¬
        text_list = "\n".join([f"{idx+1}. {text}" for idx, text in enumerate(translated_texts) if text.strip()])
        
        prompt = f"""
        è¯·æ ¹æ®ä»¥ä¸‹å‚è€ƒæ ‡ç­¾ä½“ç³»ï¼Œä¸ºæ¯ä¸ªä¸­æ–‡æ–‡æœ¬åˆ†é…æœ€åˆé€‚çš„æ ‡ç­¾ã€‚
        
        å‚è€ƒæ ‡ç­¾ä½“ç³»ï¼š
        {reference_info}
        
        æ‰“æ ‡è¦æ±‚ï¼š
        1. ä¸¥æ ¼ä¿æŒåŸæ–‡é¡ºåºå’Œç¼–å·
        2. æ¯è¡Œè¾“å‡ºæ ¼å¼ä¸º"ç¼–å·. æ ‡ç­¾åç§°1,æ ‡ç­¾åç§°2,æ ‡ç­¾åç§°3"
        3. åªèƒ½è¾“å‡ºä¸Šè¿°å‚è€ƒæ ‡ç­¾ä½“ç³»ä¸­çš„ã€æ ‡ç­¾åç§°ã€‘ï¼ˆå†’å·å‰é¢çš„éƒ¨åˆ†ï¼‰ï¼Œä¸è¦è¾“å‡ºç¤ºä¾‹å…³é”®è¯
        4. æ¯ä¸ªæ–‡æœ¬åˆ†é…1-3ä¸ªæœ€ç›¸å…³çš„æ ‡ç­¾åç§°
        5. å¦‚æœæ–‡æœ¬ä¸æ‰€æœ‰å‚è€ƒæ ‡ç­¾éƒ½ä¸åŒ¹é…ï¼Œè¾“å‡º"å…¶ä»–"
        6. åªè¾“å‡ºæ ‡ç­¾åˆ†é…ç»“æœï¼Œä¸è¦å…¶ä»–è¯´æ˜
        
        é‡è¦æé†’ï¼šè¯·è¾“å‡ºæ ‡ç­¾åç§°ï¼ˆå¦‚"ç”¨æˆ·æ»¡æ„"ï¼‰ï¼Œä¸è¦è¾“å‡ºç¤ºä¾‹å…³é”®è¯ï¼ˆå¦‚"å¾ˆæ»¡æ„"ï¼‰ï¼
        
        ç¤ºä¾‹è¾“å‡ºï¼š
        1. ç”¨æˆ·æ»¡æ„,äº§å“ä½“éªŒ
        2. æŠ€æœ¯æ”¯æŒ
        3. å…¶ä»–
        
        å¾…åˆ†é…æ ‡ç­¾çš„æ–‡æœ¬ï¼š
        {text_list}
        
        æ ‡ç­¾åˆ†é…ç»“æœï¼š
        """
        
        try:
            model = os.getenv("OPENAI_MODEL")
            if not model:
                logger.error("æœªæ‰¾åˆ°OPENAI_MODELç¯å¢ƒå˜é‡")
                return self._rule_based_tag_assignment(translated_texts)
            
            response = self.client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„æ–‡æœ¬åˆ†ç±»ä¸“å®¶ï¼Œä¸¥æ ¼æŒ‰ç…§ç»™å®šçš„æ ‡ç­¾ä½“ç³»è¿›è¡Œåˆ†ç±»ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=100 * len(translated_texts)
            )
            
            # è§£æç»“æœ
            results = [""] * len(translated_texts)
            content = response.choices[0].message.content
            content = content.strip() if content else ""
            
            if content:
                for line in content.split('\n'):
                    match = re.match(r'(\d+)\.\s*(.+)', line)
                    if match:
                        idx = int(match.group(1)) - 1
                        if idx < len(translated_texts):
                            tags = match.group(2).strip()
                            results[idx] = tags
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ åŸºäºå‚è€ƒæ ‡ç­¾çš„æ‰“æ ‡å¤±è´¥: {e}")
            if retry_count < 3:
                logger.warning(f"ğŸ”„ é‡è¯•ç¬¬ {retry_count + 1} æ¬¡...")
                time.sleep(2 ** retry_count)
                return self.assign_tags_based_on_reference(translated_texts, retry_count + 1)
            else:
                logger.warning("âš ï¸ å¤šæ¬¡é‡è¯•å¤±è´¥ï¼Œä½¿ç”¨è§„åˆ™åŒ¹é…æ–¹æ³•")
                return self._rule_based_tag_assignment(translated_texts)
    
    def _build_reference_tags_prompt(self):
        """
        æ„å»ºå‚è€ƒæ ‡ç­¾çš„æç¤ºä¿¡æ¯
        """
        reference_info = ""
        for i, tag in enumerate(self.reference_tags, 1):
            reference_info += f"{i}. {tag['name']}: {tag['definition']}\n"
            if tag.get('examples'):
                examples = ', '.join(tag['examples'])
                reference_info += f"   ç¤ºä¾‹å…³é”®è¯: {examples}\n"
        
        return reference_info
    
    def _rule_based_tag_assignment(self, translated_texts):
        """
        åŸºäºè§„åˆ™çš„æ ‡ç­¾åˆ†é…ï¼ˆå½“AIä¸å¯ç”¨æ—¶çš„å›é€€æ–¹æ³•ï¼‰
        """
        results = []
        
        # æ„å»ºå…³é”®è¯æ˜ å°„
        tag_keywords = {}
        for tag in self.reference_tags:
            keywords = [tag['name']] + tag.get('examples', [])
            # æ·»åŠ å®šä¹‰ä¸­çš„å…³é”®è¯
            if tag.get('definition'):
                definition_words = tag['definition'].split()
                keywords.extend(definition_words)
            tag_keywords[tag['name']] = keywords
        
        for text in translated_texts:
            if not text.strip():
                results.append("")
                continue
            
            # åŒ¹é…å¾—åˆ†
            tag_scores = {}
            for tag_name, keywords in tag_keywords.items():
                score = 0
                for keyword in keywords:
                    if keyword in text:
                        score += 1
                tag_scores[tag_name] = score
            
            # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„æ ‡ç­¾åç§°
            if tag_scores:
                max_score = max(tag_scores.values())
                if max_score > 0:
                    matched_tags = [tag_name for tag_name, score in tag_scores.items() if score == max_score]
                    results.append(",".join(matched_tags[:3]))  # æœ€å¤š3ä¸ªæ ‡ç­¾åç§°
                else:
                    results.append("å…¶ä»–")
            else:
                results.append("å…¶ä»–")
        
        return results
    
    def _need_translation(self, texts):
        """
        åˆ¤æ–­æ˜¯å¦éœ€è¦ç¿»è¯‘ï¼ˆç®€å•çš„ä¸­è‹±æ–‡æ£€æµ‹ï¼‰
        """
        sample_text = ' '.join(texts[:5])  # å–å‰5ä¸ªæ–‡æœ¬æ ·æœ¬
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', sample_text))
        english_chars = len(re.findall(r'[a-zA-Z]', sample_text))
        
        # å¦‚æœè‹±æ–‡å­—ç¬¦æ¯”ä¸­æ–‡å­—ç¬¦å¤šï¼Œè®¤ä¸ºéœ€è¦ç¿»è¯‘
        return english_chars > chinese_chars
    
    def _batch_translate_texts(self, texts, column_name, batch_size=50):
        """
        çº¯ç¿»è¯‘åŠŸèƒ½ï¼Œä¸æ¶‰åŠæ‰“æ ‡
        """
        translations = []
        
        batches = [texts[i:i+batch_size] for i in range(0, len(texts), batch_size)]
        
        for batch_idx, batch in enumerate(batches):
            logger.info(f"ğŸ”„ ç¿»è¯‘æ‰¹æ¬¡ {batch_idx + 1}/{len(batches)}")
            batch_translations = self._translate_batch(batch, column_name)
            translations.extend(batch_translations)
            
            # æ‰¹æ¬¡é—´ç­‰å¾…
            if batch_idx < len(batches) - 1:
                time.sleep(1.0)
        
        return translations
    
    def _translate_batch(self, texts, column_name):
        """
        æ‰¹é‡ç¿»è¯‘å•ä¸ªæ‰¹æ¬¡
        """
        text_list = "\n".join([f"{idx+1}. {text}" for idx, text in enumerate(texts) if text.strip()])
        
        prompt = f"""
        è¯·å°†ä»¥ä¸‹è‹±æ–‡å†…å®¹ç¿»è¯‘æˆä¸­æ–‡ï¼Œä¿æŒåŸæ–‡é¡ºåºå’Œç¼–å·ã€‚
        
        è¦æ±‚ï¼š
        1. ä¸¥æ ¼ä¿æŒåŸæ–‡é¡ºåºå’Œç¼–å·
        2. æ¯è¡Œè¾“å‡ºæ ¼å¼ä¸º"ç¼–å·. ç¿»è¯‘å†…å®¹"
        3. åªè¾“å‡ºç¿»è¯‘ç»“æœï¼Œä¸è¦å…¶ä»–è¯´æ˜
        
        {column_name}å†…å®¹ï¼š
        {text_list}
        
        ç¿»è¯‘ç»“æœï¼š
        """
        
        try:
            model = os.getenv("OPENAI_MODEL")
            if not model:
                logger.error("æœªæ‰¾åˆ°OPENAI_MODELç¯å¢ƒå˜é‡")
                return texts
            
            response = self.client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„ç¿»è¯‘åŠ©æ‰‹ï¼Œä¸“æ³¨äºå‡†ç¡®ç¿»è¯‘ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=100 * len(texts)
            )
            
            # è§£æç¿»è¯‘ç»“æœ
            results = [""] * len(texts)
            content = response.choices[0].message.content
            content = content.strip() if content else ""
            
            if content:
                for line in content.split('\n'):
                    match = re.match(r'(\d+)\.\s*(.+)', line)
                    if match:
                        idx = int(match.group(1)) - 1
                        if idx < len(texts):
                            translation = match.group(2).strip()
                            results[idx] = translation
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ ç¿»è¯‘å¤±è´¥: {e}")
            return texts  # ç¿»è¯‘å¤±è´¥æ—¶è¿”å›åŸæ–‡
    
    def _assign_to_reference_topics(self, sub_tags_list, main_topics):
        """
        å°†æ ‡ç­¾åˆ†é…åˆ°å‚è€ƒä¸»é¢˜
        """
        topic_assignments = []
        
        for tags_str in sub_tags_list:
            if not tags_str or tags_str.strip() == "å…¶ä»–":
                topic_assignments.append("å…¶ä»–")
                continue
            
            # åˆ†å‰²æ ‡ç­¾
            tags = [tag.strip() for tag in tags_str.split(',') if tag.strip()]
            
            # ç›´æ¥ä½¿ç”¨æ ‡ç­¾ä½œä¸ºä¸»é¢˜ï¼ˆå› ä¸ºæ ‡ç­¾å°±æ˜¯ä»å‚è€ƒæ ‡ç­¾ä¸­é€‰æ‹©çš„ï¼‰
            matched_topics = [tag for tag in tags if tag in main_topics]
            
            if matched_topics:
                topic_assignments.append("ã€".join(matched_topics))
            else:
                topic_assignments.append("å…¶ä»–")
        
        return topic_assignments
    
    def find_column(self, df, keywords):
        """
        æ ¹æ®å…³é”®è¯åˆ—è¡¨æŸ¥æ‰¾åŒ¹é…çš„åˆ—å
        """
        for col in df.columns:
            for keyword in keywords:
                if re.search(keyword, col, re.IGNORECASE):
                    return col
        return None
    
    def batch_translate_and_tag(self, texts, column_name, source_lang="è‹±æ–‡", target_lang="ä¸­æ–‡", batch_size=15, retry_count=0):
        """
        æ‰¹é‡ç¿»è¯‘å¹¶ç”Ÿæˆå¤šä¸ªæ ‡ç­¾ï¼Œä¸€æ¬¡APIè°ƒç”¨å®Œæˆä¸¤ä¸ªä»»åŠ¡
        è¿”å›æ ¼å¼ï¼š[("ç¿»è¯‘å†…å®¹", "æ ‡ç­¾1,æ ‡ç­¾2,æ ‡ç­¾3"), ...]
        """
        if not texts or all(not text.strip() for text in texts):
            return [("", "")] * len(texts)
        
        # æ£€æŸ¥OpenAIå®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨
        if not self.client:
            logger.warning("âš ï¸ OpenAIå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œè¿”å›åŸå§‹æ–‡æœ¬")
            return [(text, "") for text in texts]
        
        # å¦‚æœå¯ç”¨äº†å‚è€ƒæ ‡ç­¾æ¨¡å¼ï¼Œåˆ†åˆ«å¤„ç†ç¿»è¯‘å’Œæ‰“æ ‡
        if self.use_reference_mode and self.reference_tags:
            logger.info("ğŸ·ï¸  ä½¿ç”¨å‚è€ƒæ ‡ç­¾æ¨¡å¼è¿›è¡Œåˆ†åˆ«å¤„ç†")
            
            # ç¬¬ä¸€æ­¥ï¼šç¿»è¯‘
            if self._need_translation(texts):
                logger.info("ğŸ“ å¼€å§‹ç¿»è¯‘...")
                translations = self._batch_translate_texts(texts, column_name, batch_size)
            else:
                translations = texts  # å¦‚æœæ˜¯ä¸­æ–‡ï¼Œç›´æ¥ä½¿ç”¨åŸæ–‡
            
            # ç¬¬äºŒæ­¥ï¼šåŸºäºå‚è€ƒæ ‡ç­¾æ‰“æ ‡
            logger.info("ğŸ·ï¸  åŸºäºå‚è€ƒæ ‡ç­¾æ‰“æ ‡...")
            tags = self.assign_tags_based_on_reference(translations, retry_count)
            
            # ç»„åˆç»“æœ
            results = []
            for translation, tag in zip(translations, tags):
                results.append((translation, tag))
            
            return results
        
        # åŸæœ‰çš„ä¸€ä½“åŒ–å¤„ç†é€»è¾‘
        # æ„å»ºæ‰¹é‡æç¤º
        text_list = "\n".join([f"{idx+1}. {text}" for idx, text in enumerate(texts) if text.strip()])
        
        prompt = f"""
        è¯·å°†ä»¥ä¸‹{source_lang}å†…å®¹åˆ—è¡¨ç¿»è¯‘æˆ{target_lang}å¹¶ç”Ÿæˆå¤šä¸ªç›¸å…³çš„ä¸»é¢˜æ ‡ç­¾ã€‚
        è¦æ±‚ï¼š
        1. ä¸¥æ ¼ä¿æŒåŸæ–‡é¡ºåºå’Œç¼–å·
        2. æ¯è¡Œè¾“å‡ºæ ¼å¼ä¸º"ç¼–å·. ç¿»è¯‘å†…å®¹ | æ ‡ç­¾1,æ ‡ç­¾2,æ ‡ç­¾3"
        3. æ¯ä¸ªå›ç­”ç”Ÿæˆ2-3ä¸ªç›¸å…³æ ‡ç­¾ï¼Œç”¨é€—å·åˆ†éš”
        4. æ ‡ç­¾è¦å…·ä½“ã€ç»†åˆ†ï¼Œæ¶µç›–ä¸åŒè§’åº¦
        5. åªè¾“å‡ºç»“æœåˆ—è¡¨ï¼Œä¸è¦åŒ…å«å…¶ä»–è¯´æ˜
        
        ç¤ºä¾‹æ ¼å¼ï¼š
        1. æœåŠ¡å¾ˆæ…¢ | æœåŠ¡é€Ÿåº¦,å“åº”æ—¶é—´,ç”¨æˆ·ä½“éªŒ
        2. ç•Œé¢å¤æ‚ | ç•Œé¢è®¾è®¡,æ“ä½œå¤æ‚,æ˜“ç”¨æ€§
        3. ä»·æ ¼å¤ªé«˜ | ä»·æ ¼é—®é¢˜,æ€§ä»·æ¯”,æˆæœ¬æ§åˆ¶
        
        {column_name}å†…å®¹åˆ—è¡¨ï¼š
        {text_list}
        
        ç¿»è¯‘å’Œæ ‡ç­¾ç»“æœåˆ—è¡¨ï¼š
        """
        
        try:
            logger.debug(f"å‘é€ç¿»è¯‘+æ ‡ç­¾è¯·æ±‚ï¼Œæ‰¹æ¬¡å¤§å°: {len(texts)}")
            model = os.getenv("OPENAI_MODEL")
            if not model:
                logger.error("æœªæ‰¾åˆ°OPENAI_MODELç¯å¢ƒå˜é‡")
                return [("MODEL_ERROR", "")] * len(texts)
                
            response = self.client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯é«˜æ•ˆçš„æ‰¹é‡ç¿»è¯‘æ ‡ç­¾åŠ©æ‰‹ï¼Œä¸“æ³¨äºä¿æŒé¡ºåºçš„å‡†ç¡®ç¿»è¯‘å’Œåˆ†ç±»ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=150 * len(texts)  # åŠ¨æ€tokenåˆ†é…
            )
            logger.info(f"response: {response}")
            
            # è§£ææ‰¹é‡ç»“æœ
            results = [("", "")] * len(texts)
            content = response.choices[0].message.content
            content = content.strip() if content else ""
            
            if content:
                # è§£ææ ¼å¼ï¼š1. ç¿»è¯‘å†…å®¹ | æ ‡ç­¾\n2. ç¿»è¯‘å†…å®¹ | æ ‡ç­¾...
                for line in content.split('\n'):
                    match = re.match(r'(\d+)\.\s*(.*?)(?:\s*\|\s*(.*))?$', line)
                    if match:
                        idx = int(match.group(1)) - 1
                        if idx < len(texts):
                            translation = match.group(2).strip() if match.group(2) else ""
                            tag = match.group(3).strip() if match.group(3) else ""
                            results[idx] = (translation, tag)
            
            return results
        
        except RateLimitError as e:
            # é€Ÿç‡é™åˆ¶é”™è¯¯å¤„ç†
            retry_count += 1
            if retry_count > 3:
                logger.error(f"âš ï¸ æ‰¹é‡ç¿»è¯‘æ ‡ç­¾è¿ç»­3æ¬¡é‡åˆ°é€Ÿç‡é™åˆ¶é”™è¯¯ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                return [("RATE_LIMIT_ERROR", "")] * len(texts)
            
            wait_time = min(60, 10 * retry_count) + random.uniform(0, 5)  # é™åˆ¶æœ€å¤§ç­‰å¾…æ—¶é—´
            logger.warning(f"â³ æ‰¹é‡ç¿»è¯‘æ ‡ç­¾é‡åˆ°é€Ÿç‡é™åˆ¶é”™è¯¯: {e}, å°†åœ¨ {wait_time:.1f} ç§’åé‡è¯• (å°è¯• #{retry_count})")
            time.sleep(wait_time)
            return self.batch_translate_and_tag(texts, column_name, source_lang, target_lang, batch_size, retry_count)
        
        except APIConnectionError as e:
            # ç½‘ç»œè¿æ¥é”™è¯¯å¤„ç†
            retry_count += 1
            if retry_count > 3:
                logger.error(f"âš ï¸ æ‰¹é‡ç¿»è¯‘æ ‡ç­¾è¿ç»­3æ¬¡é‡åˆ°ç½‘ç»œè¿æ¥é”™è¯¯ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                return [("NETWORK_ERROR", "")] * len(texts)
            
            wait_time = min(120, 15 * retry_count) + random.uniform(0, 10)  # é™åˆ¶æœ€å¤§ç­‰å¾…æ—¶é—´
            logger.warning(f"ğŸŒ æ‰¹é‡ç¿»è¯‘æ ‡ç­¾é‡åˆ°ç½‘ç»œè¿æ¥é—®é¢˜: {e}, å°†åœ¨ {wait_time:.1f} ç§’åé‡è¯• (å°è¯• #{retry_count})")
            time.sleep(wait_time)
            return self.batch_translate_and_tag(texts, column_name, source_lang, target_lang, batch_size, retry_count)
        
        except Exception as network_error:
            # å¤„ç†å¯èƒ½çš„httpxç›¸å…³å¼‚å¸¸
            if 'httpx' in str(type(network_error)).lower() or 'connect' in str(network_error).lower():
                retry_count += 1
                if retry_count > 3:
                    logger.error(f"âš ï¸ æ‰¹é‡ç¿»è¯‘æ ‡ç­¾è¿ç»­3æ¬¡é‡åˆ°ç½‘ç»œè¿æ¥é”™è¯¯ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                    return [("NETWORK_ERROR", "")] * len(texts)
                
                wait_time = min(120, 15 * retry_count) + random.uniform(0, 10)
                logger.warning(f"ğŸŒ æ‰¹é‡ç¿»è¯‘æ ‡ç­¾é‡åˆ°ç½‘ç»œè¿æ¥é—®é¢˜: {network_error}, å°†åœ¨ {wait_time:.1f} ç§’åé‡è¯• (å°è¯• #{retry_count})")
                time.sleep(wait_time)
                return self.batch_translate_and_tag(texts, column_name, source_lang, target_lang, batch_size, retry_count)
            else:
                # å…¶ä»–æœªçŸ¥å¼‚å¸¸
                raise
        
        except (AuthenticationError, PermissionDeniedError) as e:
            logger.error(f"âŒ APIè®¤è¯å¤±è´¥: {e}")
            return [("AUTH_ERROR", "")] * len(texts)
        
        except APIError as e:
            # å…¶ä»–APIé”™è¯¯
            logger.error(f"âš ï¸ æ‰¹é‡ç¿»è¯‘æ ‡ç­¾é‡åˆ°APIé”™è¯¯: {e}")
            return [("API_ERROR", "")] * len(texts)
        
        except Exception as e:
            # è·³è¿‡ä¹‹å‰å·²å¤„ç†çš„ç½‘ç»œå¼‚å¸¸
            if 'httpx' in str(type(e)).lower() or 'connect' in str(e).lower():
                pass  # å·²åœ¨ä¸Šé¢å¤„ç†
            else:
                logger.error(f"âš ï¸ æ‰¹é‡ç¿»è¯‘æ ‡ç­¾é‡åˆ°æœªçŸ¥é”™è¯¯: {e}")
                return [("UNKNOWN_ERROR", "")] * len(texts)
    
    def extract_all_tags(self, all_tags_list):
        """
        ä»æ‰€æœ‰æ ‡ç­¾ä¸­æå–å”¯ä¸€çš„æ ‡ç­¾åˆ—è¡¨
        """
        all_tags = []
        for tags_str in all_tags_list:
            if tags_str and tags_str.strip():
                # åˆ†å‰²å¤šä¸ªæ ‡ç­¾
                tags = [tag.strip() for tag in tags_str.split(',') if tag.strip()]
                all_tags.extend(tags)
        
        # ç»Ÿè®¡æ ‡ç­¾é¢‘ç‡
        tag_counts = Counter(all_tags)
        unique_tags = list(tag_counts.keys())
        
        logger.info(f"ğŸ“Š æå–åˆ° {len(unique_tags)} ä¸ªå”¯ä¸€æ ‡ç­¾")
        logger.info(f"ğŸ·ï¸  æ ‡ç­¾é¢‘ç‡ç»Ÿè®¡ï¼ˆå‰10ä¸ªï¼‰: {dict(tag_counts.most_common(10))}")
        
        return unique_tags, tag_counts
    
    def generate_main_topics(self, unique_tags, tag_counts, num_topics=5):
        """
        ä½¿ç”¨AIç”Ÿæˆä¸€çº§ä¸»é¢˜æ ‡ç­¾
        """
        if not unique_tags:
            return []
        
        # æ£€æŸ¥OpenAIå®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨
        if not self.client:
            logger.warning("âš ï¸ OpenAIå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œä½¿ç”¨å›é€€æ–¹æ³•ç”Ÿæˆä¸»é¢˜")
            return self.fallback_topic_generation(unique_tags, num_topics)
        
        # é€‰æ‹©é¢‘ç‡æœ€é«˜çš„æ ‡ç­¾ä½œä¸ºè¾“å…¥
        top_tags = [tag for tag, count in tag_counts.most_common(min(20, len(tag_counts)))]
        tags_text = ", ".join(top_tags)
        
        prompt = f"""
        è¯·æ ¹æ®ä»¥ä¸‹äºŒçº§æ ‡ç­¾ï¼Œç”Ÿæˆ{num_topics}ä¸ªä¸€çº§ä¸»é¢˜æ ‡ç­¾ï¼ˆå¤§åˆ†ç±»ï¼‰ã€‚
        
        äºŒçº§æ ‡ç­¾åˆ—è¡¨ï¼š
        {tags_text}
        
        è¦æ±‚ï¼š
        1. ç”Ÿæˆ{num_topics}ä¸ªä¸€çº§ä¸»é¢˜ï¼Œæ¯ä¸ªä¸»é¢˜ç”¨2-4ä¸ªè¯æè¿°
        2. ä¸€çº§ä¸»é¢˜è¦èƒ½æ¶µç›–ç›¸å…³çš„äºŒçº§æ ‡ç­¾
        3. ä¸»é¢˜ä¹‹é—´è¦æœ‰åŒºåˆ†åº¦ï¼Œé¿å…é‡å¤
        4. åªè¾“å‡ºä¸»é¢˜åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ªä¸»é¢˜
        
        ç¤ºä¾‹ï¼š
        æœåŠ¡è´¨é‡
        äº§å“è®¾è®¡
        ä»·æ ¼ç­–ç•¥
        ç”¨æˆ·ä½“éªŒ
        æŠ€æœ¯æ”¯æŒ
        
        ä¸€çº§ä¸»é¢˜ï¼š
        """
        
        try:
            model = os.getenv("OPENAI_MODEL")
            if not model:
                logger.error("æœªæ‰¾åˆ°OPENAI_MODELç¯å¢ƒå˜é‡")
                return self.fallback_topic_generation(unique_tags, num_topics)
                
            response = self.client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„ä¸»é¢˜åˆ†ç±»ä¸“å®¶ï¼Œæ“…é•¿å°†ç»†åˆ†ç±»åˆ«å½’çº³ä¸ºæ›´é«˜å±‚æ¬¡çš„ä¸»é¢˜ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=200
            )
            
            content = response.choices[0].message.content
            content = content.strip() if content else ""
            topics = [topic.strip() for topic in content.split('\n') if topic.strip()]
            
            # æ¸…ç†å’ŒéªŒè¯ä¸»é¢˜
            cleaned_topics = []
            for topic in topics:
                # ç§»é™¤å¯èƒ½çš„ç¼–å·å’Œå¤šä½™å­—ç¬¦
                topic = re.sub(r'^\d+[\.\)]?\s*', '', topic)
                topic = topic.strip()
                if topic and len(topic) <= 10:  # é™åˆ¶ä¸»é¢˜é•¿åº¦
                    cleaned_topics.append(topic)
            
            logger.info(f"ğŸ¯ ç”Ÿæˆ {len(cleaned_topics)} ä¸ªä¸€çº§ä¸»é¢˜: {cleaned_topics}")
            
            # æ‰“å°ç”Ÿæˆçš„ä¸»é¢˜æ ‡ç­¾
            logger.info("ğŸ“‹ ç”Ÿæˆçš„ä¸€çº§ä¸»é¢˜æ ‡ç­¾:")
            for i, topic in enumerate(cleaned_topics[:num_topics], 1):
                logger.info(f"  {i}. {topic}")
            
            return cleaned_topics[:num_topics]
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆä¸€çº§ä¸»é¢˜å¤±è´¥: {e}")
            # å›é€€åˆ°ç®€å•çš„èšç±»æ–¹æ³•
            return self.fallback_topic_generation(unique_tags, num_topics)
    
    def fallback_topic_generation(self, unique_tags, num_topics):
        """
        å›é€€çš„ä¸»é¢˜ç”Ÿæˆæ–¹æ³•ï¼ˆåŸºäºå…³é”®è¯èšç±»ï¼‰
        """
        if len(unique_tags) < num_topics:
            return unique_tags[:num_topics]
        
        try:
            # ä½¿ç”¨TF-IDFå‘é‡åŒ–æ ‡ç­¾
            vectorizer = TfidfVectorizer(max_features=100, stop_words=None)
            tag_vectors = vectorizer.fit_transform(unique_tags)
            
            # K-meansèšç±»
            kmeans = KMeans(n_clusters=num_topics, random_state=42, n_init="auto")
            clusters = kmeans.fit_predict(tag_vectors)
            
            # ä¸ºæ¯ä¸ªèšç±»é€‰æ‹©ä»£è¡¨æ€§æ ‡ç­¾
            topics = []
            for i in range(num_topics):
                cluster_tags = [tag for tag, cluster_id in zip(unique_tags, clusters) if cluster_id == i]
                if cluster_tags:
                    # é€‰æ‹©æœ€é•¿çš„æ ‡ç­¾ä½œä¸ºä¸»é¢˜ï¼ˆé€šå¸¸æ›´å…·ä½“ï¼‰
                    topic = max(cluster_tags, key=len)
                    topics.append(topic)
            
            logger.info(f"ğŸ”„ ä½¿ç”¨èšç±»æ–¹æ³•ç”Ÿæˆä¸»é¢˜: {topics}")
            return topics
            
        except Exception as e:
            logger.error(f"âŒ èšç±»æ–¹æ³•ä¹Ÿå¤±è´¥: {e}")
            # æœ€åçš„å›é€€ï¼šé€‰æ‹©æœ€é¢‘ç¹çš„æ ‡ç­¾
            return unique_tags[:num_topics]
    
    def assign_tags_to_topics(self, all_tags_list, main_topics):
        """
        å°†äºŒçº§æ ‡ç­¾åˆ†é…åˆ°ä¸€çº§ä¸»é¢˜
        è¿”å›ï¼šæ¯ä¸ªå›ç­”çš„ä¸»é¢˜åˆ†é…ç»“æœï¼ˆæ”¯æŒå¤šä¸ªä¸»é¢˜ç”¨"ã€"åˆ†éš”ï¼‰
        """
        if not main_topics:
            return [""] * len(all_tags_list)
        
        # ä¸ºæ¯ä¸ªå›ç­”åˆ†é…ä¸»é¢˜
        topic_assignments = []
        
        for tags_str in all_tags_list:
            if not tags_str or not tags_str.strip():
                topic_assignments.append("")
                continue
            
            # åˆ†å‰²äºŒçº§æ ‡ç­¾
            sub_tags = [tag.strip() for tag in tags_str.split(',') if tag.strip()]
            
            # è®¡ç®—ä¸æ¯ä¸ªä¸»é¢˜çš„ç›¸ä¼¼åº¦ï¼Œæ”¯æŒå¤šä¸ªä¸»é¢˜
            matched_topics = []
            topic_scores = {}
            
            for topic in main_topics:
                score = 0
                for sub_tag in sub_tags:
                    # æ£€æŸ¥æ ‡ç­¾æ˜¯å¦åŒ…å«ä¸»é¢˜å…³é”®è¯
                    if any(keyword in sub_tag for keyword in topic.split()):
                        score += 1
                    # æ£€æŸ¥ä¸»é¢˜æ˜¯å¦åŒ…å«æ ‡ç­¾å…³é”®è¯
                    if any(keyword in topic for keyword in sub_tag.split()):
                        score += 1
                
                topic_scores[topic] = score
            
            # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„ä¸»é¢˜ï¼ˆæ”¯æŒå¤šä¸ªï¼‰
            if topic_scores:
                max_score = max(topic_scores.values())
                if max_score > 0:
                    # é€‰æ‹©æ‰€æœ‰å¾—åˆ†ç­‰äºæœ€é«˜åˆ†çš„ä¸»é¢˜
                    matched_topics = [topic for topic, score in topic_scores.items() if score == max_score]
                else:
                    # å¦‚æœæ²¡æœ‰åŒ¹é…ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªä¸»é¢˜
                    matched_topics = [main_topics[0]] if main_topics else []
            
            # ç”¨"ã€"è¿æ¥å¤šä¸ªä¸»é¢˜
            topic_assignments.append("ã€".join(matched_topics))
        
        return topic_assignments
    
    def validate_api_connection(self):
        """éªŒè¯APIè¿æ¥æ˜¯å¦æ­£å¸¸"""
        logger.info("éªŒè¯APIè¿æ¥...")
        
        # æ£€æŸ¥OpenAIå®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨
        if not self.client:
            logger.warning("âš ï¸ OpenAIå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œè·³è¿‡APIè¿æ¥æµ‹è¯•")
            return True  # æ— éœ€éªŒè¯æ—¶è¿”å›True
        
        # æ£€æŸ¥APIå¯†é’¥
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            logger.error("æœªæ‰¾åˆ°OPENAI_API_KEYç¯å¢ƒå˜é‡")
            return False
        
        logger.info(f"APIå¯†é’¥: {api_key[:10]}...{api_key[-4:] if len(api_key) > 14 else '***'}")
        
        # æ£€æŸ¥æ¨¡å‹è®¾ç½®
        model = os.getenv("OPENAI_MODEL")
        if not model:
            logger.error("æœªæ‰¾åˆ°OPENAI_MODELç¯å¢ƒå˜é‡")
            return False
        logger.info(f"ä½¿ç”¨æ¨¡å‹: {model}")
        
        # æ£€æŸ¥ä»£ç†è®¾ç½®
        http_proxy = os.getenv("http_proxy")
        https_proxy = os.getenv("https_proxy")
        logger.info(f"HTTPä»£ç†: {http_proxy}")
        logger.info(f"HTTPSä»£ç†: {https_proxy}")
        
        # æµ‹è¯•ç½‘ç»œè¿æ¥ï¼ˆç®€åŒ–ç‰ˆï¼Œåªæµ‹è¯•åŸºæœ¬è¿é€šæ€§ï¼‰
        logger.info("æµ‹è¯•ç½‘ç»œè¿æ¥...")
        try:
            import urllib.request
            import urllib.error
            
            # æµ‹è¯•ä»£ç†è¿æ¥
            if http_proxy:
                proxies = {}
                if http_proxy:
                    proxies['http'] = http_proxy
                if https_proxy:
                    proxies['https'] = https_proxy
                proxy_handler = urllib.request.ProxyHandler(proxies)
                opener = urllib.request.build_opener(proxy_handler)
                urllib.request.install_opener(opener)
            
            # æµ‹è¯•è®¿é—®Googleï¼ˆä¸éœ€è¦è®¤è¯ï¼‰
            response = urllib.request.urlopen('https://www.google.com', timeout=10)
            logger.info(f"ç½‘ç»œè¿æ¥æµ‹è¯•æˆåŠŸ: HTTP {response.getcode()}")
        except Exception as e:
            logger.warning(f"ç½‘ç»œè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            logger.warning("ç»§ç»­å°è¯•APIè¿æ¥æµ‹è¯•...")
            # ä¸è¿”å›Falseï¼Œç»§ç»­å°è¯•APIæµ‹è¯•
        
        try:
            test_text = "Hello, world"
            logger.info(f"å‘é€æµ‹è¯•è¯·æ±‚: '{test_text}'")
            
            model = os.getenv("OPENAI_MODEL")
            if not model:
                logger.error("æœªæ‰¾åˆ°OPENAI_MODELç¯å¢ƒå˜é‡")
                return False
                
            response = self.client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": f"Translate to Chinese: {test_text}"}
                ],
                max_tokens=20,
                timeout=30  # è®¾ç½®30ç§’è¶…æ—¶
            )
            
            result = response.choices[0].message.content
            result = result.strip() if result else ""
            logger.info(f"APIè¿æ¥æµ‹è¯•æˆåŠŸ: è¾“å…¥ '{test_text}' -> è¾“å‡º '{result}'")
            return True
            
        except RateLimitError as e:
            logger.error(f"APIé€Ÿç‡é™åˆ¶: {e}")
            return False
            
        except APIConnectionError as e:
            logger.error(f"APIè¿æ¥é”™è¯¯: {e}")
            logger.error("è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œä»£ç†è®¾ç½®")
            return False
            
        except AuthenticationError as e:
            logger.error(f"APIè®¤è¯å¤±è´¥: {e}")
            logger.error("è¯·æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æ­£ç¡®")
            return False
            
        except Exception as e:
            logger.error(f"APIè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            logger.error("å¯èƒ½çš„åŸå› ï¼š")
            logger.error("1. APIå¯†é’¥æ— æ•ˆæˆ–è¿‡æœŸ")
            logger.error("2. ç½‘ç»œè¿æ¥é—®é¢˜")
            logger.error("3. OpenAIæœåŠ¡ä¸­æ–­")
            logger.error("4. ä½¿ç”¨äº†é”™è¯¯çš„APIç«¯ç‚¹")
            return False
    
    def process_table(self, input_path=None, output_path=None):
        """å¤„ç†è¡¨æ ¼ï¼šç¿»è¯‘å†…å®¹å¹¶ç”Ÿæˆåˆ†ç±»æ ‡ç­¾ï¼Œæ”¯æŒCSVå’ŒExcelæ ¼å¼"""
        
        # å¦‚æœæ²¡æœ‰æä¾›è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼Œä½¿ç”¨äº¤äº’å¼é€‰æ‹©
        if input_path is None:
            input_path = self.analyzer.select_file_interactive()
            if not input_path:
                logger.error("âŒ æœªé€‰æ‹©è¾“å…¥æ–‡ä»¶")
                return False
        
        # è·å–æ–‡ä»¶æ‰©å±•å
        input_ext = Path(input_path).suffix.lower()
        
        # éªŒè¯APIè¿æ¥ï¼ˆå¦‚æœOpenAIå¯ç”¨ï¼‰
        if self.client:
            if not self.validate_api_connection():
                logger.error("âŒ APIè¿æ¥éªŒè¯å¤±è´¥ï¼Œæ— æ³•ç»§ç»­å¤„ç†")
                return False
        else:
            logger.warning("âš ï¸ OpenAIå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œå°†è·³è¿‡ç¿»è¯‘å’Œæ ‡ç­¾åŠŸèƒ½ï¼Œç›´æ¥å¤„ç†æ–‡ä»¶")
        
        # è¯»å–è¾“å…¥æ–‡ä»¶
        df = self.analyzer.read_data_file(input_path)
        if df is None:
            logger.error("âŒ æ–‡ä»¶è¯»å–å¤±è´¥")
            return False
        
        # æ™ºèƒ½è¯†åˆ«é—®é¢˜ç±»å‹
        question_types = self.analyzer.identify_all_question_types(df)
        
        # æŸ¥æ‰¾å¼€æ”¾é¢˜ï¼ˆç”¨äºç¿»è¯‘å’Œæ ‡ç­¾ç”Ÿæˆï¼‰
        open_ended_questions = question_types.get('open_ended', [])
        
        logger.info(f"ğŸ” è¯†åˆ«åˆ°çš„å¼€æ”¾é¢˜æ•°é‡: {len(open_ended_questions)}")
        logger.info(f"ğŸ” å¼€æ”¾é¢˜è¯¦æƒ…: {open_ended_questions}")
        
        if not open_ended_questions:
            logger.warning("âš ï¸ æœªå‘ç°å¼€æ”¾é¢˜ï¼Œå°è¯•ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•æŸ¥æ‰¾åˆ—")
            # å›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•
            reason_keywords = ['åŸå› ', 'reason', 'è¯„åˆ†åŸå› ', 'rating reason', 'feedback', 'æ„è§åŸå› ']
            suggestion_keywords = ['å»ºè®®', 'suggestion', 'advice', 'recommendation', 'æè®®', 'æ”¹è¿›å»ºè®®']
            
            reason_col = self.find_column(df, reason_keywords)
            suggestion_col = self.find_column(df, suggestion_keywords)
            
            logger.info(f"ğŸ” ä¼ ç»Ÿæ–¹æ³•æ‰¾åˆ°çš„åˆ—: reason_col={reason_col}, suggestion_col={suggestion_col}")
            
            if not reason_col and not suggestion_col:
                logger.error("âŒ æœªæ‰¾åˆ°å¯å¤„ç†çš„æ–‡æœ¬åˆ—")
                return False
            
            # åªå¤„ç†æ‰¾åˆ°çš„åˆ—
            columns_to_process = []
            if reason_col:
                columns_to_process.append(reason_col)
            if suggestion_col:
                columns_to_process.append(suggestion_col)
        else:
            # ä½¿ç”¨è¯†åˆ«åˆ°çš„å¼€æ”¾é¢˜
            logger.info(f"âœ… å‘ç° {len(open_ended_questions)} ä¸ªå¼€æ”¾é¢˜")
            columns_to_process = [q['column'] for q in open_ended_questions]
            
            # å¤„ç†æ‰€æœ‰å¼€æ”¾é¢˜
            logger.info(f"ğŸ¯ å°†å¤„ç†æ‰€æœ‰ {len(columns_to_process)} ä¸ªå¼€æ”¾é¢˜")
        
        logger.info(f"ğŸ” æœ€ç»ˆå°†å¤„ç†ä»¥ä¸‹åˆ—: {', '.join(columns_to_process)}")
        
        # å¦‚æœæ²¡æœ‰æä¾›è¾“å‡ºè·¯å¾„ï¼Œè‡ªåŠ¨ç”Ÿæˆ
        if output_path is None:
            input_name = Path(input_path).stem
            output_path = str(Path(input_path).parent / f"{input_name}_processed.xlsx")
            logger.info(f"ğŸ’¾ è‡ªåŠ¨ç”Ÿæˆè¾“å‡ºè·¯å¾„: {output_path}")
        
        # è·å–è¾“å‡ºæ–‡ä»¶æ‰©å±•å
        output_ext = Path(output_path).suffix.lower()
        
        # ä¸ºæ¯ä¸ªè¦å¤„ç†çš„åˆ—åˆ›å»ºæ–°åˆ—ï¼ˆä»…åœ¨OpenAIå¯ç”¨æ—¶ï¼‰
        column_indices = {}
        new_columns = {}
        
        if self.client:
            for col in columns_to_process:
                col_idx = list(df.columns).index(col)
                column_indices[col] = col_idx
                
                # åˆ›å»ºæ–°åˆ—åï¼ˆæ”¯æŒåŒå±‚æ ‡ç­¾ä½“ç³»ï¼‰
                cn_col = f"{col}-CN"
                sub_tags_col = f"{col}äºŒçº§æ ‡ç­¾"  # å¤šä¸ªç»†åˆ†æ ‡ç­¾
                main_topic_col = f"{col}ä¸€çº§ä¸»é¢˜"  # å¤§ä¸»é¢˜åˆ†ç±»
                
                new_columns[col] = {
                    'cn_col': cn_col,
                    'sub_tags_col': sub_tags_col,
                    'main_topic_col': main_topic_col,
                    'index': col_idx
                }
                
                # æ’å…¥æ–°åˆ—
                df.insert(col_idx + 1, cn_col, "")
                df.insert(col_idx + 2, sub_tags_col, "")
                df.insert(col_idx + 3, main_topic_col, "")
                
                logger.info(f"ğŸ“ {cn_col} æ’å…¥åœ¨ {col} åé¢ (ç´¢å¼• {col_idx+1})")
                logger.info(f"ğŸ“ {sub_tags_col} æ’å…¥åœ¨ {cn_col} åé¢ (ç´¢å¼• {col_idx+2})")
                logger.info(f"ğŸ“ {main_topic_col} æ’å…¥åœ¨ {sub_tags_col} åé¢ (ç´¢å¼• {col_idx+3})")
        
        # åˆå§‹åŒ–å¤„ç†æ—¶é—´ç»Ÿè®¡
        start_time = time.time()
        total_rows = len(df)
        
        # å¦‚æœOpenAIä¸å¯ç”¨ï¼Œåˆ›å»ºç®€åŒ–çš„è¾“å‡ºæ–‡ä»¶
        if not self.client:
            logger.info("ğŸ”„ OpenAIä¸å¯ç”¨ï¼Œåˆ›å»ºç®€åŒ–çš„è¾“å‡ºæ–‡ä»¶...")
            
            # ç›´æ¥ä¿å­˜åŸå§‹æ•°æ®
            try:
                if output_ext == '.csv':
                    df.to_csv(output_path, index=False, encoding='utf-8-sig')
                elif output_ext in ['.xlsx', '.xls']:
                    df.to_excel(output_path, index=False)
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„è¾“å‡ºæ–‡ä»¶æ ¼å¼: {output_ext}ã€‚è¯·ä½¿ç”¨.csvæˆ–.xlsxæ–‡ä»¶")
                
                total_time = time.time() - start_time
                logger.info(f"âœ… ç®€åŒ–å¤„ç†å®Œæˆï¼å…±å¤„ç† {len(df)} è¡Œï¼Œè€—æ—¶ {total_time:.2f} ç§’")
                logger.info(f"ğŸ’¾ ç»“æœå·²ä¿å­˜è‡³: {output_path}")
                logger.info("ğŸ’¡ ç”±äºOpenAIä¸å¯ç”¨ï¼Œæœªè¿›è¡Œç¿»è¯‘å’Œåˆ†ç±»å¤„ç†")
                return True
                
            except Exception as e:
                logger.error(f"âŒ ä¿å­˜ç®€åŒ–ç»“æœå¤±è´¥: {e}")
                return False
        
        # æ‰¹é‡å¤„ç†å‚æ•°ï¼ˆå¤§å¹…å¢åŠ æ‰¹å¤„ç†å¤§å°ä»¥æé«˜æ•ˆç‡ï¼‰
        BATCH_SIZE = 50  # å¢åŠ æ‰¹é‡å¤§å°
        
        # å¤„ç†æ‰€æœ‰åˆ—
        all_results = {}
        
        logger.info(f"ğŸ“Š å¼€å§‹å¤„ç† {len(columns_to_process)} ä¸ªåˆ—")
        
        for col in columns_to_process:
            logger.info(f"\nğŸš€ å¼€å§‹æ‰¹é‡å¤„ç†åˆ—: {col}")
            
            # å‡†å¤‡æ–‡æœ¬æ•°æ®
            texts = df[col].fillna('').astype(str).tolist()
            logger.info(f"ğŸ“‹ åˆ— {col} çš„æ–‡æœ¬æ•°æ®é‡: {len(texts)}")
            logger.info(f"ğŸ“‹ å‰5ä¸ªæ–‡æœ¬ç¤ºä¾‹: {texts[:5]}")
            
            translations = []
            sub_tags = []
            
            # æ‰¹é‡å¤„ç†ï¼ˆç¿»è¯‘+äºŒçº§æ ‡ç­¾ï¼‰
            batches = [texts[i:i+BATCH_SIZE] 
                       for i in range(0, len(texts), BATCH_SIZE)]
            logger.info(f"ğŸ“Š åˆ†æˆ {len(batches)} ä¸ªæ‰¹æ¬¡å¤„ç†ï¼Œæ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
            
            for batch_idx, batch in enumerate(batches):
                # åŠ¨æ€è°ƒæ•´æ‰¹å¤„ç†å¤§å°ï¼ˆå¦‚æœæ–‡æœ¬è¿‡é•¿ï¼‰
                max_text_len = max(len(text) for text in batch)
                current_batch_size = min(BATCH_SIZE, 5) if max_text_len > 500 else BATCH_SIZE
                
                logger.info(f"ğŸ”„ å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{len(batches)}")
                logger.info(f"ğŸ“‹ æ‰¹æ¬¡å¤§å°: {len(batch)}, æœ€å¤§æ–‡æœ¬é•¿åº¦: {max_text_len}")
                logger.info(f"ğŸ“‹ å½“å‰æ‰¹æ¬¡å¤§å°: {current_batch_size}")
                
                # å¤„ç†å½“å‰æ‰¹æ¬¡ï¼ˆç¿»è¯‘+äºŒçº§æ ‡ç­¾ï¼‰
                logger.info(f"ğŸš€ å³å°†è°ƒç”¨ batch_translate_and_tag æ–¹æ³•")
                results = self.batch_translate_and_tag(batch, col, batch_size=current_batch_size)
                logger.info(f"âœ… batch_translate_and_tag è°ƒç”¨å®Œæˆï¼Œè¿”å›ç»“æœæ•°é‡: {len(results)}")
                
                # åˆ†ç¦»ç¿»è¯‘å’ŒäºŒçº§æ ‡ç­¾
                batch_translations = [result[0] for result in results]
                batch_sub_tags = [result[1] for result in results]
                
                translations.extend(batch_translations)
                sub_tags.extend(batch_sub_tags)
                
                # è¿›åº¦æŠ¥å‘Š
                processed = min((batch_idx + 1) * BATCH_SIZE, total_rows)
                elapsed = time.time() - start_time
                rows_per_min = processed / (elapsed / 60) if elapsed > 0 else 0
                remaining = (len(batches) - batch_idx - 1) * (elapsed / (batch_idx + 1)) / 60 if batch_idx > 0 else 0
                
                logger.info(f"ğŸ”„ {col}å¤„ç†è¿›åº¦: {processed}/{total_rows} è¡Œ ({processed/total_rows*100:.1f}%) | "
                            f"é€Ÿåº¦: {rows_per_min:.1f} è¡Œ/åˆ†é’Ÿ | é¢„è®¡å‰©ä½™: {remaining:.1f} åˆ†é’Ÿ")
                
                # æ‰¹æ¬¡é—´ç­‰å¾…
                wait_time = 2.0 + random.uniform(0, 1.0)
                time.sleep(wait_time)
            
            # ä¿å­˜ç»“æœ
            all_results[col] = {
                'translations': translations,
                'sub_tags': sub_tags
            }
            
            # æ›´æ–°DataFrameï¼ˆç¿»è¯‘å’ŒäºŒçº§æ ‡ç­¾ï¼‰
            col_info = new_columns[col]
            df.iloc[:, col_info['index'] + 1] = translations
            df.iloc[:, col_info['index'] + 2] = sub_tags
        
        # ç”Ÿæˆä¸€çº§ä¸»é¢˜å¹¶åˆ†é…
        logger.info(f"\nğŸ¯ å¼€å§‹ç”Ÿæˆä¸€çº§ä¸»é¢˜æ ‡ç­¾...")
        
        for col in columns_to_process:
            logger.info(f"\nğŸ“Š å¤„ç†åˆ— {col} çš„ä¸»é¢˜ç”Ÿæˆ...")
            
            # æå–æ‰€æœ‰äºŒçº§æ ‡ç­¾
            sub_tags_list = all_results[col]['sub_tags']
            unique_tags, tag_counts = self.extract_all_tags(sub_tags_list)
            
            if not unique_tags:
                logger.warning(f"âš ï¸ åˆ— {col} æ²¡æœ‰æœ‰æ•ˆçš„äºŒçº§æ ‡ç­¾ï¼Œè·³è¿‡ä¸»é¢˜ç”Ÿæˆ")
                continue
            
            # ç”Ÿæˆä¸€çº§ä¸»é¢˜
            if self.use_reference_mode and self.reference_tags:
                # ä½¿ç”¨å‚è€ƒæ ‡ç­¾ä½œä¸ºä¸»é¢˜
                main_topics = [tag['name'] for tag in self.reference_tags]
                logger.info(f"ğŸ·ï¸  ä½¿ç”¨å‚è€ƒæ ‡ç­¾ä½œä¸ºä¸»é¢˜: {main_topics}")
                
                # å°†äºŒçº§æ ‡ç­¾åˆ†é…åˆ°å‚è€ƒä¸»é¢˜
                topic_assignments = self._assign_to_reference_topics(sub_tags_list, main_topics)
            else:
                # ä½¿ç”¨AIç”Ÿæˆä¸»é¢˜
                num_topics = min(5, len(unique_tags))  # æ ¹æ®æ ‡ç­¾æ•°é‡åŠ¨æ€è°ƒæ•´ä¸»é¢˜æ•°
                main_topics = self.generate_main_topics(unique_tags, tag_counts, num_topics)
                
                if not main_topics:
                    logger.warning(f"âš ï¸ åˆ— {col} ä¸»é¢˜ç”Ÿæˆå¤±è´¥")
                    continue
                
                # å°†äºŒçº§æ ‡ç­¾åˆ†é…åˆ°ä¸€çº§ä¸»é¢˜
                topic_assignments = self.assign_tags_to_topics(sub_tags_list, main_topics)
            
            # æ›´æ–°DataFrameï¼ˆä¸€çº§ä¸»é¢˜ï¼‰
            col_info = new_columns[col]
            df.iloc[:, col_info['index'] + 3] = topic_assignments
            
            # æ˜¾ç¤ºä¸»é¢˜åˆ†é…ç»Ÿè®¡
            topic_counts = Counter(topic_assignments)
            logger.info(f"ğŸ“ˆ ä¸»é¢˜åˆ†é…ç»Ÿè®¡: {dict(topic_counts)}")
            
            # ä¿å­˜ä¸»é¢˜ä¿¡æ¯åˆ°ç»“æœä¸­
            all_results[col]['main_topics'] = main_topics
            all_results[col]['topic_assignments'] = topic_assignments
        
        # ä¿å­˜ç»“æœ
        try:
            if output_ext == '.csv':
                df.to_csv(output_path, index=False, encoding='utf-8-sig')
            elif output_ext in ['.xlsx', '.xls']:
                df.to_excel(output_path, index=False)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„è¾“å‡ºæ–‡ä»¶æ ¼å¼: {output_ext}ã€‚è¯·ä½¿ç”¨.csvæˆ–.xlsxæ–‡ä»¶")
            
            # è®¡ç®—æ€»è€—æ—¶
            total_time = time.time() - start_time
            logger.info(f"\nâœ… å¤„ç†å®Œæˆï¼å…±å¤„ç† {total_rows} è¡Œï¼Œè€—æ—¶ {total_time/60:.1f} åˆ†é’Ÿ")
            logger.info(f"ğŸ’¾ ç»“æœå·²ä¿å­˜è‡³: {output_path}")
            logger.info(f"âœ¨ æ–°å¢åˆ—ä½ç½®: ")
            
            for col in columns_to_process:
                col_info = new_columns[col]
                logger.info(f"  - {col_info['cn_col']} åœ¨ {col} åé¢")
                logger.info(f"  - {col_info['sub_tags_col']} åœ¨ {col_info['cn_col']} åé¢")
                logger.info(f"  - {col_info['main_topic_col']} åœ¨ {col_info['sub_tags_col']} åé¢")
            
            # æ˜¾ç¤ºAPIä½¿ç”¨ç»Ÿè®¡
            total_api_calls = sum(len(all_results[col]['translations']) // BATCH_SIZE for col in columns_to_process)
            logger.info(f"ğŸ“Š APIè°ƒç”¨æ¬¡æ•°: {total_api_calls} (ç›¸æ¯”é€è¡Œå¤„ç†å‡å°‘çº¦ {100 * (1 - total_api_calls/(total_rows*len(columns_to_process))):.0f}%)")
            
            # é”™è¯¯ç»Ÿè®¡
            all_translations = []
            all_sub_tags = []
            for col in columns_to_process:
                all_translations.extend(all_results[col]['translations'])
                all_sub_tags.extend(all_results[col]['sub_tags'])
            
            error_stats = {
                "RATE_LIMIT_ERROR": sum(1 for x in all_translations + all_sub_tags if "RATE_LIMIT" in x),
                "NETWORK_ERROR": sum(1 for x in all_translations + all_sub_tags if "NETWORK" in x),
                "API_ERROR": sum(1 for x in all_translations + all_sub_tags if "API_ERROR" in x),
                "UNKNOWN_ERROR": sum(1 for x in all_translations + all_sub_tags if "UNKNOWN" in x),
                "AUTH_ERROR": sum(1 for x in all_translations + all_sub_tags if "AUTH" in x)
            }
            
            if any(error_stats.values()):
                logger.warning("âš ï¸ å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯:")
                for error_type, count in error_stats.items():
                    if count > 0:
                        logger.warning(f"  - {error_type}: {count} å¤„")
                logger.warning("å»ºè®®æ£€æŸ¥æ—¥å¿—æ–‡ä»¶ 'api_processing.log' è·å–è¯¦ç»†é”™è¯¯ä¿¡æ¯")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
            return False
        
        # å¤„ç†æˆåŠŸï¼Œè¿”å›True
        return True

    def process_table_with_reference_tags(self, input_path, reference_tags, output_path=None):
        """
        åŸºäºå‚è€ƒæ ‡ç­¾é‡æ–°æ‰“æ ‡çš„ä¸»æ–¹æ³•
        """
        # è®¾ç½®å‚è€ƒæ ‡ç­¾
        self.set_reference_tags(reference_tags)
        
        # è°ƒç”¨åŸæœ‰çš„å¤„ç†æ–¹æ³•ï¼Œä½†ä½¿ç”¨å‚è€ƒæ ‡ç­¾æ¨¡å¼
        return self.process_table(input_path, output_path)

    def translate_only(self, input_path, output_path, open_ended_fields):
        """åªè¿›è¡Œç¿»è¯‘ï¼Œä¸è¿›è¡ŒAIåˆ†ç±» - ä¸ºåç»­çš„æ ‡å‡†æ‰“æ ‡æˆ–å‚è€ƒæ ‡ç­¾æ‰“æ ‡åšå‡†å¤‡"""
        try:
            logger.info(f"ğŸ”§ å¼€å§‹åªç¿»è¯‘å¤„ç†: {input_path} -> {output_path}")
            logger.info(f"ğŸ“‹ å¾…ç¿»è¯‘çš„å¼€æ”¾é¢˜å­—æ®µ: {open_ended_fields}")
            
            # è¯»å–è¾“å…¥æ–‡ä»¶
            df = self.analyzer.read_data_file(input_path)
            if df is None:
                logger.error("âŒ æ–‡ä»¶è¯»å–å¤±è´¥")
                return False
            
            # éªŒè¯APIè¿æ¥ï¼ˆå¦‚æœOpenAIå¯ç”¨ï¼‰
            if self.client:
                if not self.validate_api_connection():
                    logger.error("âŒ APIè¿æ¥éªŒè¯å¤±è´¥ï¼Œæ— æ³•ç»§ç»­å¤„ç†")
                    return False
            else:
                logger.warning("âš ï¸ OpenAIå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œå°†ç›´æ¥å¤åˆ¶æ–‡ä»¶")
                # å¦‚æœæ²¡æœ‰OpenAIï¼Œç›´æ¥å¤åˆ¶æ–‡ä»¶
                df.to_excel(output_path, index=False)
                return True
            
            # ä¸ºæ¯ä¸ªå¼€æ”¾é¢˜å­—æ®µåˆ›å»ºç¿»è¯‘åˆ—
            for col in open_ended_fields:
                if col in df.columns:
                    col_idx = list(df.columns).index(col)
                    cn_col = f"{col}-CN"
                    
                    # æ’å…¥æ–°åˆ—
                    df.insert(col_idx + 1, cn_col, "")
                    logger.info(f"ğŸ“ {cn_col} æ’å…¥åœ¨ {col} åé¢ (ç´¢å¼• {col_idx+1})")
            
            # æ‰¹é‡ç¿»è¯‘å¤„ç†
            BATCH_SIZE = 50
            total_rows = len(df)
            
            for col in open_ended_fields:
                if col not in df.columns:
                    continue
                    
                logger.info(f"\nğŸš€ å¼€å§‹ç¿»è¯‘åˆ—: {col}")
                cn_col = f"{col}-CN"
                
                # å‡†å¤‡æ–‡æœ¬æ•°æ®
                texts = df[col].fillna('').astype(str).tolist()
                logger.info(f"ğŸ“‹ åˆ— {col} çš„æ–‡æœ¬æ•°æ®é‡: {len(texts)}")
                
                translations = []
                
                # ä½¿ç”¨å·²æœ‰çš„æ‰¹é‡ç¿»è¯‘æ–¹æ³•
                translations = self._batch_translate_texts(texts, col, BATCH_SIZE)
                
                # æ›´æ–°DataFrame
                df[cn_col] = translations
                logger.info(f"âœ… åˆ— {col} ç¿»è¯‘å®Œæˆ")
            
            # ä¿å­˜ç»“æœ
            df.to_excel(output_path, index=False)
            logger.info(f"âœ… ç¿»è¯‘ç»“æœå·²ä¿å­˜: {output_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ç¿»è¯‘å¤„ç†å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False
    
    def standard_labeling_only(self, input_path, output_path):
        """åªè¿›è¡Œæ ‡å‡†AIåˆ†ç±»ï¼ŒåŸºäºå·²ç¿»è¯‘çš„æ•°æ®"""
        try:
            logger.info(f"ğŸ”§ å¼€å§‹æ ‡å‡†AIåˆ†ç±»å¤„ç†: {input_path} -> {output_path}")
            
            # è¯»å–å·²ç¿»è¯‘çš„æ–‡ä»¶
            df = self.analyzer.read_data_file(input_path)
            if df is None:
                logger.error("âŒ æ–‡ä»¶è¯»å–å¤±è´¥")
                return False
            
            # éªŒè¯APIè¿æ¥ï¼ˆå¦‚æœOpenAIå¯ç”¨ï¼‰
            if self.client:
                if not self.validate_api_connection():
                    logger.error("âŒ APIè¿æ¥éªŒè¯å¤±è´¥ï¼Œæ— æ³•ç»§ç»­å¤„ç†")
                    return False
            else:
                logger.warning("âš ï¸ OpenAIå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œå°†ç›´æ¥å¤åˆ¶æ–‡ä»¶")
                df.to_excel(output_path, index=False)
                return True
            
            # è¯†åˆ«å·²ç¿»è¯‘çš„-CNå­—æ®µ
            cn_columns = [col for col in df.columns if col.endswith('-CN')]
            logger.info(f"ğŸ” å‘ç° {len(cn_columns)} ä¸ªå·²ç¿»è¯‘çš„-CNå­—æ®µ: {cn_columns}")
            
            if not cn_columns:
                logger.error("âŒ æœªæ‰¾åˆ°å·²ç¿»è¯‘çš„-CNå­—æ®µ")
                return False
            
            # ä¸ºæ¯ä¸ª-CNå­—æ®µæ·»åŠ åˆ†ç±»åˆ—
            for cn_col in cn_columns:
                original_col = cn_col.replace('-CN', '')
                
                # æ‰¾åˆ°-CNåˆ—çš„ä½ç½®
                cn_col_idx = list(df.columns).index(cn_col)
                
                # åˆ›å»ºåˆ†ç±»åˆ—
                sub_tags_col = f"{original_col}äºŒçº§æ ‡ç­¾"
                main_topic_col = f"{original_col}ä¸€çº§ä¸»é¢˜"
                
                # æ’å…¥æ–°åˆ—
                df.insert(cn_col_idx + 1, sub_tags_col, "")
                df.insert(cn_col_idx + 2, main_topic_col, "")
                
                logger.info(f"ğŸ“ ä¸º {cn_col} æ·»åŠ åˆ†ç±»åˆ—: {sub_tags_col}, {main_topic_col}")
            
            # æ‰¹é‡åˆ†ç±»å¤„ç†
            BATCH_SIZE = 50
            
            for cn_col in cn_columns:
                original_col = cn_col.replace('-CN', '')
                sub_tags_col = f"{original_col}äºŒçº§æ ‡ç­¾"
                main_topic_col = f"{original_col}ä¸€çº§ä¸»é¢˜"
                
                logger.info(f"\nğŸš€ å¼€å§‹ä¸º {cn_col} è¿›è¡ŒAIåˆ†ç±»")
                
                # å‡†å¤‡å·²ç¿»è¯‘çš„æ–‡æœ¬æ•°æ®
                texts = df[cn_col].fillna('').astype(str).tolist()
                logger.info(f"ğŸ“‹ å·²ç¿»è¯‘æ–‡æœ¬æ•°æ®é‡: {len(texts)}")
                
                sub_tags = []
                main_topics = []
                
                # æ‰¹é‡åˆ†ç±»
                batches = [texts[i:i+BATCH_SIZE] for i in range(0, len(texts), BATCH_SIZE)]
                logger.info(f"ğŸ“Š åˆ†æˆ {len(batches)} ä¸ªæ‰¹æ¬¡åˆ†ç±»ï¼Œæ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
                
                for batch_idx, batch in enumerate(batches):
                    logger.info(f"ğŸ”„ åˆ†ç±»æ‰¹æ¬¡ {batch_idx + 1}/{len(batches)}")
                    
                    # æ‰¹é‡ç¿»è¯‘+åˆ†ç±» (ä½¿ç”¨å·²æœ‰æ–¹æ³•)
                    batch_results = self.batch_translate_and_tag(batch, original_col, "ä¸­æ–‡", "ä¸­æ–‡", 15)
                    
                    # æå–ç»“æœ - batch_translate_and_tag è¿”å› [(translation, tags), ...]
                    for translation, tags in batch_results:
                        # å°†æ ‡ç­¾åˆ†å‰²ä¸ºäºŒçº§æ ‡ç­¾ï¼Œå¹¶ç”Ÿæˆç®€åŒ–çš„ä¸€çº§ä¸»é¢˜
                        tag_list = [tag.strip() for tag in tags.split(',') if tag.strip()] if tags else []
                        
                        # äºŒçº§æ ‡ç­¾ï¼šä½¿ç”¨æ‰€æœ‰ç”Ÿæˆçš„æ ‡ç­¾
                        sub_tags.append(','.join(tag_list))
                        
                        # ä¸€çº§ä¸»é¢˜ï¼šä½¿ç”¨ç¬¬ä¸€ä¸ªæ ‡ç­¾ä½œä¸ºä¸»è¦ä¸»é¢˜ï¼Œå¦‚æœæ²¡æœ‰æ ‡ç­¾åˆ™ä¸ºç©º
                        main_topic = tag_list[0] if tag_list else ''
                        main_topics.append(main_topic)
                    
                    # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
                    if batch_idx < len(batches) - 1:
                        time.sleep(1)
                
                # æ›´æ–°DataFrame
                df[sub_tags_col] = sub_tags
                df[main_topic_col] = main_topics
                logger.info(f"âœ… {cn_col} AIåˆ†ç±»å®Œæˆ")
            
            # ä¿å­˜ç»“æœ
            df.to_excel(output_path, index=False)
            logger.info(f"âœ… æ ‡å‡†AIåˆ†ç±»ç»“æœå·²ä¿å­˜: {output_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ ‡å‡†AIåˆ†ç±»å¤„ç†å¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False


if __name__ == "__main__":
    # ä¸»ç¨‹åºå…¥å£
    logger.info("=" * 80)
    logger.info("ğŸš€ é—®å·ç¿»è¯‘åˆ†ç±»å™¨å¯åŠ¨")
    logger.info("=" * 80)
    
    try:
        # åˆ›å»ºç¿»è¯‘åˆ†ç±»å™¨å®ä¾‹
        classifier = QuestionnaireTranslationClassifier()
        
        # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
        if len(sys.argv) > 1:
            input_file = sys.argv[1]
            output_file = sys.argv[2] if len(sys.argv) > 2 else None
            logger.info(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {input_file}")
            if output_file:
                logger.info(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶: {output_file}")
            classifier.process_table(input_file, output_file)
        else:
            # äº¤äº’å¼æ¨¡å¼
            logger.info("ğŸ¯ äº¤äº’å¼æ¨¡å¼ - è¯·é€‰æ‹©è¦å¤„ç†çš„æ–‡ä»¶")
            classifier.process_table()
        
        logger.info("=" * 80)
        logger.info("âœ¨ ç¨‹åºæ‰§è¡Œå®Œæˆ")
        logger.info("=" * 80)
        
    except Exception as e:
        logger.error(f"\nâŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        logger.error("ğŸ’¡ å»ºè®®æ£€æŸ¥:")
        logger.error("1. OpenAI APIå¯†é’¥æ˜¯å¦æ­£ç¡®")
        logger.error("2. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
        logger.error("3. è¾“å…¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®")
        logger.error("4. universal_questionnaire_analyzer.py æ–‡ä»¶æ˜¯å¦å­˜åœ¨")
        import traceback
        logger.error(traceback.format_exc())
        sys.exit(1) 