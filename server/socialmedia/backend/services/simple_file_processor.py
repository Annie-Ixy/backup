# -*- coding: utf-8 -*-
"""
ÁÆÄÂåñÁöÑÊñá‰ª∂Â§ÑÁêÜÂô®
‰∏ìÊ≥®‰∫éODSÂ±ÇÊï∞ÊçÆÊé•Êî∂Ôºå‰∏çÂÅöÂéªÈáçÂíåAIÂàÜÊûê
"""

import os
import logging
import pandas as pd
import uuid
from datetime import datetime
from typing import Tuple, Dict, Any, List
from pathlib import Path

from models.new_dash_social_model import ODSDashSocialComment, FileUploadLog
from config.database_config import get_db_config
from utils.csv_helper import CSVHelper

logger = logging.getLogger(__name__)

class SimpleFileProcessor:
    """
    ODSÂ±ÇÊñá‰ª∂Â§ÑÁêÜÂô®
    
    ËÅåË¥£Ôºö
    1. Êé•Êî∂ÊâÄÊúâÂéüÂßãÊñá‰ª∂Êï∞ÊçÆÔºå‰∏çÂÅö‰ªª‰ΩïËøáÊª§
    2. Âü∫Êú¨ÁöÑÂàóÂêçÊ†áÂáÜÂåñÂíåÊï∞ÊçÆÁ±ªÂûãËΩ¨Êç¢
    3. ‰øùÁïôÊâÄÊúâÂéüÂßãÊï∞ÊçÆÔºåÂåÖÊã¨Êó†ÊïàÂíåÁ©∫ÂÄº
    4. Êï∞ÊçÆÈ™åËØÅÂíåÂéªÈáçÁî±ETLÂ±ÇË¥üË¥£
    """
    
    def __init__(self):
        self.db_config = get_db_config()
        self.supported_formats = ['.xlsx', '.xls', '.csv']
        
        # È¢ÑÊúüÁöÑÂàóÂêçÊò†Â∞ÑÔºàÊîØÊåÅ‰∏≠Ëã±ÊñáÔºâ
        self.column_mapping = {
            # Ëã±ÊñáÂàóÂêç
            'last_update': 'last_update',
            'brand_label': 'brand_label', 
            'author_name': 'author_name',
            'channel': 'channel',
            'message_type': 'message_type',
            'text': 'text',
            'tags': 'tags',
            'post_link': 'post_link',
            'sentiment': 'sentiment',
            'caption': 'caption',
            
            # ‰∏≠ÊñáÂàóÂêçÊò†Â∞Ñ
            'ÊúÄÂêéÊõ¥Êñ∞Êó∂Èó¥': 'last_update',
            'ÂìÅÁâåÊ†áÁ≠æ': 'brand_label',
            '‰ΩúËÄÖÂêçÁß∞': 'author_name',
            'Ê∏†ÈÅì': 'channel',
            'Âπ≥Âè∞': 'channel',
            'Ê∂àÊÅØÁ±ªÂûã': 'message_type',
            'ÊñáÊú¨ÂÜÖÂÆπ': 'text',
            'ÂÜÖÂÆπ': 'text',
            'ËØÑËÆ∫ÂÜÖÂÆπ': 'text',
            'Ê†áÁ≠æ': 'tags',
            'Â∏ñÂ≠êÈìæÊé•': 'post_link',
            'ÈìæÊé•': 'post_link',
            'ÊÉÖÊÑüÊ†áÁ≠æ': 'sentiment',
            'ÊÉÖÊÑü': 'sentiment',
            'Ê†áÈ¢ò': 'caption',
            'ËØ¥Êòé': 'caption',
            'Ê†áÈ¢òËØ¥Êòé': 'caption'
        }
        
        # Ê∑ªÂä†‰ΩçÁΩÆÊò†Â∞ÑÔºàÁî®‰∫éÂ§ÑÁêÜÊó†Ê†áÈ¢òÊàñÊ†áÈ¢ò‰∏çÂåπÈÖçÁöÑÊÉÖÂÜµÔºâ
        self.position_mapping = {
            0: 'last_update',      # Á¨¨‰∏ÄÂàóÈÄöÂ∏∏ÊòØÊó∂Èó¥
            1: 'brand_label',      # Á¨¨‰∫åÂàóÈÄöÂ∏∏ÊòØÂìÅÁâå
            2: 'author_name',      # Á¨¨‰∏âÂàóÈÄöÂ∏∏ÊòØ‰ΩúËÄÖ
            3: 'channel',          # Á¨¨ÂõõÂàóÈÄöÂ∏∏ÊòØÊ∏†ÈÅì
            4: 'message_type',     # Á¨¨‰∫îÂàóÈÄöÂ∏∏ÊòØÊ∂àÊÅØÁ±ªÂûã
            5: 'text',             # Á¨¨ÂÖ≠ÂàóÈÄöÂ∏∏ÊòØÊñáÊú¨ÂÜÖÂÆπ
        }
    
    def validate_file(self, file_path: str) -> Tuple[bool, str]:
        """
        È™åËØÅÊñá‰ª∂
        """
        try:
            # Ê£ÄÊü•Êñá‰ª∂ÊòØÂê¶Â≠òÂú®
            if not os.path.exists(file_path):
                return False, "Êñá‰ª∂‰∏çÂ≠òÂú®"
            
            # Ê£ÄÊü•Êñá‰ª∂Ê†ºÂºè
            file_extension = Path(file_path).suffix.lower()
            if file_extension not in self.supported_formats:
                return False, f"‰∏çÊîØÊåÅÁöÑÊñá‰ª∂Ê†ºÂºèÔºö{file_extension}ÔºåÊîØÊåÅÁöÑÊ†ºÂºèÔºö{', '.join(self.supported_formats)}"
            
            # Ê£ÄÊü•Êñá‰ª∂Â§ßÂ∞èÔºàÈôêÂà∂100MBÔºâ
            file_size = os.path.getsize(file_path)
            max_size = 100 * 1024 * 1024  # 100MB
            if file_size > max_size:
                return False, f"Êñá‰ª∂Â§ßÂ∞èË∂ÖËøáÈôêÂà∂Ôºà{max_size / 1024 / 1024:.0f}MBÔºâ"
            
            return True, "Êñá‰ª∂È™åËØÅÈÄöËøá"
            
        except Exception as e:
            return False, f"Êñá‰ª∂È™åËØÅÂ§±Ë¥•Ôºö{str(e)}"
    
    def read_file(self, file_path: str) -> Tuple[pd.DataFrame, str]:
        """
        ËØªÂèñÊñá‰ª∂Êï∞ÊçÆ
        """
        try:
            file_extension = Path(file_path).suffix.lower()
            
            if file_extension in ['.xlsx', '.xls']:
                df = pd.read_excel(file_path, engine='openpyxl')
            elif file_extension == '.csv':
                # Â¢ûÂº∫ÁöÑCSVÊñá‰ª∂ÁºñÁ†ÅÊ£ÄÊµã
                df = self._read_csv_with_encoding_detection(file_path)
                if df is None:
                    return None, "CSVÊñá‰ª∂ËØªÂèñÂ§±Ë¥•"
            else:
                return None, f"‰∏çÊîØÊåÅÁöÑÊñá‰ª∂Ê†ºÂºèÔºö{file_extension}"
            
            if df is not None and not df.empty:
                logger.info(f"ÊàêÂäüËØªÂèñÊñá‰ª∂Ôºö{file_path}ÔºåÂÖ± {len(df)} Ë°å")
                return df, ""
            else:
                return None, "Êñá‰ª∂‰∏∫Á©∫ÊàñÊó†Ê≥ïËØªÂèñÊúâÊïàÊï∞ÊçÆ"
            
        except Exception as e:
            error_msg = f"ËØªÂèñÊñá‰ª∂Â§±Ë¥•Ôºö{str(e)}"
            logger.error(error_msg)
            return None, error_msg
    
    def _read_csv_with_encoding_detection(self, file_path: str) -> pd.DataFrame:
        """
        ‰ΩøÁî®CSVÂä©ÊâãËøõË°åÂ¢ûÂº∫ÁöÑCSVÊñá‰ª∂ËØªÂèñ
        """
        try:
            # ‰ºòÂÖà‰ΩøÁî®ÁÆÄÂçïËØªÂèñÊñπÊ≥ï
            df, error_msg = CSVHelper.read_csv_simple(file_path)
            
            if df is not None:
                logger.info(f"‰ΩøÁî®ÁÆÄÂçïCSVËØªÂèñÊàêÂäü: {file_path}")
                return df
            
            # Â¶ÇÊûúÁÆÄÂçïÊñπÊ≥ïÂ§±Ë¥•ÔºåÂ∞ùËØïÂÅ•Â£ÆÊñπÊ≥ï
            logger.warning("ÁÆÄÂçïCSVËØªÂèñÂ§±Ë¥•ÔºåÂ∞ùËØïÂÅ•Â£ÆËØªÂèñÊñπÊ≥ï")
            df, error_msg = CSVHelper.read_csv_robust(file_path)
            
            if df is not None:
                logger.info(f"‰ΩøÁî®ÂÅ•Â£ÆCSVËØªÂèñÊàêÂäü: {file_path}")
                return df
            else:
                logger.error(f"ÊâÄÊúâCSVËØªÂèñÊñπÊ≥ïÈÉΩÂ§±Ë¥•: {error_msg}")
                raise Exception(error_msg)
                
        except Exception as e:
            logger.error(f"CSVÊñá‰ª∂ËØªÂèñÂ§±Ë¥•: {e}")
            raise e
    
    def clean_and_standardize_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ODSÂ±ÇÊï∞ÊçÆÂ§ÑÁêÜÔºö‰ªÖÂÅöÂàóÂêçÊ†áÂáÜÂåñÔºå‰øùÁïôÊâÄÊúâÂéüÂßãÊï∞ÊçÆ
        Êï∞ÊçÆÈ™åËØÅÂíåÊ∏ÖÊ¥óÁî±ETLËøáÁ®ãÔºàODS‚ÜíDWDÔºâË¥üË¥£
        """
        try:
            # ÂàõÂª∫ÂâØÊú¨ÈÅøÂÖç‰øÆÊîπÂéüÊï∞ÊçÆ
            df_processed = df.copy()
            
            # ‰ªÖÂÅöÂàóÂêçÊ†áÂáÜÂåñÔºàÊò†Â∞Ñ‰∏≠Ëã±ÊñáÂàóÂêçÔºâ
            df_processed = self._standardize_columns(df_processed)
            
            logger.info(f"ODSÊï∞ÊçÆÂ§ÑÁêÜÂÆåÊàêÔºö‰øùÁïôÊâÄÊúâ {len(df_processed)} Ë°åÂéüÂßãÊï∞ÊçÆÔºàÂàóÂêçÂ∑≤Ê†áÂáÜÂåñÔºâ")
            return df_processed
            
        except Exception as e:
            logger.error(f"ODSÊï∞ÊçÆÂ§ÑÁêÜÂ§±Ë¥•Ôºö{e}")
            # ËøîÂõûÁ©∫DataFrameËÄå‰∏çÊòØÊäõÂá∫ÂºÇÂ∏∏
            return pd.DataFrame()
    
    def _standardize_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """Ê†áÂáÜÂåñÂàóÂêçÔºàÂåÖÂê´‰ΩçÁΩÆÊò†Â∞ÑÔºâ- ODSÂ±Ç‰ªÖÂÅöÂàóÂêçÊò†Â∞Ñ"""
        logger.info(f"ÂéüÂßãÂàóÂêç: {df.columns.tolist()}")
        
        # ÂàõÂª∫Êñ∞ÁöÑÂàóÂêçÊò†Â∞Ñ
        new_columns = {}
        mapped_standard_cols = set()  # Ë∑üË∏™Â∑≤Êò†Â∞ÑÁöÑÊ†áÂáÜÂàóÂêç
        
        for i, col in enumerate(df.columns):
            col_lower = str(col).strip().lower()
            mapped = False
            
            # 1. Áõ¥Êé•ÂåπÈÖç
            if col in self.column_mapping:
                new_columns[col] = self.column_mapping[col]
                mapped_standard_cols.add(self.column_mapping[col])
                mapped = True
            # 2. Â∞èÂÜôÂåπÈÖç
            elif col_lower in [k.lower() for k in self.column_mapping.keys()]:
                for k, v in self.column_mapping.items():
                    if k.lower() == col_lower:
                        new_columns[col] = v
                        mapped_standard_cols.add(v)
                        mapped = True
                        break
            
            if not mapped:
                # 3. ‰ΩçÁΩÆÊò†Â∞Ñ‰Ωú‰∏∫ÂêéÂ§áÊñπÊ°à
                if i in self.position_mapping:
                    std_col = self.position_mapping[i]
                    # Âè™ÊúâÂΩìËøô‰∏™Ê†áÂáÜÂàóÂêçËøòÊ≤°ÊúâË¢´Êò†Â∞ÑÊó∂Êâç‰ΩøÁî®‰ΩçÁΩÆÊò†Â∞Ñ
                    if std_col not in mapped_standard_cols:
                        new_columns[col] = std_col
                        mapped_standard_cols.add(std_col)
                        logger.info(f"‰ΩçÁΩÆÊò†Â∞Ñ: Á¨¨{i}Âàó '{col}' -> '{std_col}'")
                        mapped = True
                
                if not mapped:
                    # ‰øùÊåÅÂéüÂàóÂêç
                    new_columns[col] = col
        
        df_renamed = df.rename(columns=new_columns)
        logger.info(f"Êò†Â∞ÑÂêéÂàóÂêç: {df_renamed.columns.tolist()}")
        
        # ODSÂ±Ç‰∏çÂàõÂª∫Â≠óÊÆµÔºå‰øùÊåÅÂéüÂßãÊï∞ÊçÆÁªìÊûÑ
        # ETLËøáÁ®ã‰ºöÂ§ÑÁêÜÁº∫Â§±Â≠óÊÆµÁöÑÈóÆÈ¢ò
        
        return df_renamed
    
    # Êï∞ÊçÆÁ±ªÂûãÊ∏ÖÊ¥óÊñπÊ≥ïÂ∑≤ÁßªÈô§ÔºöÊï∞ÊçÆÈ™åËØÅÂíåÊ∏ÖÊ¥óÁî±ETLËøáÁ®ãÔºàODS‚ÜíDWDÔºâË¥üË¥£
    
    # Êó∂Èó¥Â≠óÊÆµÂ§ÑÁêÜÊñπÊ≥ïÂ∑≤ÁßªÈô§ÔºöÊó∂Èó¥È™åËØÅÂíåËΩ¨Êç¢Áî±ETLËøáÁ®ãÔºàODS‚ÜíDWDÔºâË¥üË¥£
    
    # ÂøÖÂ°´Â≠óÊÆµÈ™åËØÅÊñπÊ≥ïÂ∑≤ÁßªÈô§ÔºöÊï∞ÊçÆÈ™åËØÅÁî±ETLËøáÁ®ãÔºàODS‚ÜíDWDÔºâË¥üË¥£
    
    def save_to_ods(self, df: pd.DataFrame, batch_id: str) -> Tuple[int, int]:
        """
        ‰øùÂ≠òÊï∞ÊçÆÂà∞ODSË°®Ôºà‰ºòÂåñÁâà - ÈùôÈªòÂ§ÑÁêÜÈáçÂ§çÊï∞ÊçÆÂπ∂ËæìÂá∫Ê±áÊÄªÔºâ
        
        Returns:
            (success_count, error_count)
        """
        success_count = 0
        error_count = 0
        
        try:
            logger.info(f"ÂºÄÂßãÊâπÈáèÊèíÂÖ• {len(df)} Êù°Êï∞ÊçÆÂà∞ODSË°®...")
            
            for index, row in df.iterrows():
                try:
                    # ÂàõÂª∫ODSËÆ∞ÂΩï
                    # Â§ÑÁêÜÊó∂Èó¥Â≠óÊÆµÔºåÁ°Æ‰øùpandas NaTË¢´ËΩ¨Êç¢‰∏∫None
                    last_update_value = row.get('last_update')
                    if pd.isna(last_update_value):
                        last_update_value = None
                    
                    ods_record = ODSDashSocialComment(
                        last_update=last_update_value,
                        brand_label=str(row.get('brand_label', '')),
                        author_name=str(row.get('author_name', '')),
                        channel=str(row.get('channel', '')),
                        message_type=str(row.get('message_type', '')),
                        text=str(row.get('text', '')),
                        tags=str(row.get('tags', '')),
                        post_link=str(row.get('post_link', '')),
                        sentiment=str(row.get('sentiment', '')),
                        caption=str(row.get('caption', '')),
                        upload_batch_id=batch_id,
                        original_row_index=int(index),
                        processed_at=datetime.now(),
                        created_at=datetime.now(),
                        processed_flag=0  # Ê†áËÆ∞‰∏∫Êú™Â§ÑÁêÜ
                    )
                    
                    # ÊûÑÂª∫ÊèíÂÖ•SQL
                    sql = self._build_insert_sql(ods_record)
                    
                    # ODSÂ±ÇÂÖÅËÆ∏ÈáçÂ§çÊï∞ÊçÆÔºå‰∏çÂÅöÈáçÂ§çÊ£ÄÊü•
                    insert_success = self.db_config.execute_insert(sql)
                    if insert_success:
                        success_count += 1
                    else:
                        error_count += 1
                    
                except Exception as e:
                    logger.error(f"‰øùÂ≠òÁ¨¨ {index} Ë°åÊï∞ÊçÆÂ§±Ë¥•Ôºö{e}")
                    error_count += 1
                    continue
            
            logger.info(f"üìä ODSÊï∞ÊçÆ‰øùÂ≠òÂÆåÊàêÔºöÊàêÂäü {success_count} Êù°ÔºåÂ§±Ë¥• {error_count} Êù°")
            return success_count, error_count
            
        except Exception as e:
            logger.error(f"ÊâπÈáè‰øùÂ≠òÊï∞ÊçÆÂ§±Ë¥•Ôºö{e}")
            return success_count, error_count
    
    def _build_insert_sql(self, record: ODSDashSocialComment) -> str:
        """ÊûÑÂª∫ÊèíÂÖ•SQL"""
        # Â§ÑÁêÜÊó∂Èó¥Â≠óÊÆµÔºàÊ≠£Á°ÆÂ§ÑÁêÜNaTÂÄºÔºâ
        import pandas as pd
        
        # ÂÆâÂÖ®ÁöÑÊó∂Èó¥Ê†ºÂºèÂåñ
        def safe_datetime_format(dt):
            if dt is None or pd.isna(dt):
                return 'NULL'
            try:
                return f"'{dt.strftime('%Y-%m-%d %H:%M:%S')}'"
            except (AttributeError, ValueError):
                return 'NULL'
        
        last_update_str = safe_datetime_format(record.last_update)
        processed_at_str = safe_datetime_format(record.processed_at)
        created_at_str = safe_datetime_format(record.created_at)
        
        # Â§ÑÁêÜÊñáÊú¨Â≠óÊÆµÔºàÈò≤Ê≠¢SQLÊ≥®ÂÖ•Ôºâ
        def escape_sql_string(s):
            if s is None:
                return 'NULL'
            return "'" + str(s).replace("'", "''").replace("\\", "\\\\") + "'"
        
        sql = f"""
            INSERT INTO ods_dash_social_comments 
            (last_update, brand_label, author_name, channel, message_type, text, tags, 
             post_link, sentiment, caption, upload_batch_id, original_row_index, 
             processed_at, created_at, processed_flag)
            VALUES (
                {last_update_str},
                {escape_sql_string(record.brand_label)},
                {escape_sql_string(record.author_name)},
                {escape_sql_string(record.channel)},
                {escape_sql_string(record.message_type)},
                {escape_sql_string(record.text)},
                {escape_sql_string(record.tags)},
                {escape_sql_string(record.post_link)},
                {escape_sql_string(record.sentiment)},
                {escape_sql_string(record.caption)},
                {escape_sql_string(record.upload_batch_id)},
                {record.original_row_index},
                {processed_at_str},
                {created_at_str},
                {record.processed_flag}
            )
        """
        
        return sql
    
    def process_file(self, file_path: str, filename: str = None, user_id: str = None) -> Tuple[bool, str, Dict[str, Any]]:
        """
        Â§ÑÁêÜÊñá‰ª∂ÁöÑ‰∏ªÂÖ•Âè£ÔºàÁÆÄÂåñÁâàÔºâ
        
        Args:
            file_path: Êñá‰ª∂Ë∑ØÂæÑ
            filename: Êñá‰ª∂Âêç
            user_id: Áî®Êà∑IDÔºàÊîØÊåÅÂ§öÁî®Êà∑ÈöîÁ¶ªÔºâ
        
        Returns:
            (success, error_message, result_stats)
        """
        try:
            # ÁîüÊàêÊâπÊ¨°ID
            batch_id = f"batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"
            
            if not filename:
                filename = os.path.basename(file_path)
            
            file_size = os.path.getsize(file_path)
            
            # ÂàõÂª∫‰∏ä‰º†Êó•Âøó
            upload_log = FileUploadLog(
                filename=filename,
                file_path=file_path,
                file_size=file_size,
                batch_id=batch_id,
                upload_time=datetime.now(),
                user_upload=user_id or "system",  # ËÆæÁΩÆÁî®Êà∑ID
                process_start_time=datetime.now(),
                status='processing'
            )
            
            # ‰øùÂ≠ò‰∏ä‰º†ËÆ∞ÂΩï
            self._save_upload_log(upload_log)
            
            # 1. È™åËØÅÊñá‰ª∂
            is_valid, error_msg = self.validate_file(file_path)
            if not is_valid:
                upload_log.status = 'failed'
                upload_log.error_message = error_msg
                upload_log.process_end_time = datetime.now()
                upload_log.original_rows = 0
                self._update_upload_log(upload_log)
                return False, error_msg, {}
            
            # 2. ËØªÂèñÊñá‰ª∂
            df, error_msg = self.read_file(file_path)
            if df is None:
                upload_log.status = 'failed'
                upload_log.error_message = error_msg
                upload_log.process_end_time = datetime.now()
                upload_log.original_rows = 0
                self._update_upload_log(upload_log)
                return False, error_msg, {}
            
            upload_log.original_rows = len(df)
            
            # 3. Ê∏ÖÊ¥óÊï∞ÊçÆ
            df_cleaned = self.clean_and_standardize_data(df)
            upload_log.processed_rows = len(df_cleaned)
            
            if df_cleaned.empty:
                upload_log.status = 'failed'
                upload_log.error_message = "Êï∞ÊçÆÊ∏ÖÊ¥óÂêéÊó†ÊúâÊïàÊï∞ÊçÆ"
                upload_log.process_end_time = datetime.now()
                upload_log.success_rows = 0
                upload_log.error_rows = upload_log.original_rows
                self._update_upload_log(upload_log)
                return False, "Êï∞ÊçÆÊ∏ÖÊ¥óÂêéÊó†ÊúâÊïàÊï∞ÊçÆ", {}
            
            # 4. ‰øùÂ≠òÂà∞ODSË°®
            success_count, error_count = self.save_to_ods(df_cleaned, batch_id)
            
            # 5. Êõ¥Êñ∞‰∏ä‰º†Êó•Âøó
            upload_log.success_rows = success_count
            upload_log.error_rows = error_count
            upload_log.duplicate_rows = 0  # ODSÂ±Ç‰∏çÊ£ÄÊµãÈáçÂ§ç
            upload_log.status = 'completed' if error_count == 0 else 'partial'
            upload_log.process_end_time = datetime.now()
            
            self._update_upload_log(upload_log)
            
            # 6. ËøîÂõûÁªìÊûú
            result = {
                'batch_id': batch_id,
                'original_rows': upload_log.original_rows,
                'processed_rows': upload_log.processed_rows,
                'success_rows': success_count,
                'duplicate_rows': 0,  # ODSÂ±Ç‰∏çÂ§ÑÁêÜÈáçÂ§ç
                'error_rows': error_count,
                'filename': filename,
                'message': f"Êñá‰ª∂Â§ÑÁêÜÂÆåÊàêÔºöÊàêÂäü {success_count} Êù°ÔºåÂ§±Ë¥• {error_count} Êù°"
            }
            
            return True, "", result
            
        except Exception as e:
            error_msg = f"Êñá‰ª∂Â§ÑÁêÜÂ§±Ë¥•Ôºö{str(e)}"
            logger.error(error_msg)
            
            # Êõ¥Êñ∞Â§±Ë¥•Áä∂ÊÄÅ
            if 'upload_log' in locals():
                upload_log.status = 'failed'
                upload_log.error_message = error_msg
                upload_log.process_end_time = datetime.now()
                self._update_upload_log(upload_log)
            
            return False, error_msg, {}
    
    def _save_upload_log(self, upload_log: FileUploadLog):
        """‰øùÂ≠ò‰∏ä‰º†ËÆ∞ÂΩï"""
        try:
            sql = f"""
                INSERT INTO ods_dash_social_file_upload_logs 
                (filename, file_path, file_size, original_rows, processed_rows, 
                 success_rows, duplicate_rows, error_rows, upload_time, process_start_time, 
                 batch_id, status, error_message, user_upload)
                VALUES ('{upload_log.filename}', '{upload_log.file_path}', 
                        {upload_log.file_size}, {upload_log.original_rows}, 
                        {upload_log.processed_rows}, {upload_log.success_rows}, 
                        {upload_log.duplicate_rows}, {upload_log.error_rows}, 
                        '{upload_log.upload_time.strftime('%Y-%m-%d %H:%M:%S')}', 
                        '{upload_log.process_start_time.strftime('%Y-%m-%d %H:%M:%S')}', 
                        '{upload_log.batch_id}', '{upload_log.status}', 
                        '{upload_log.error_message}', '{upload_log.user_upload}')
            """
            self.db_config.execute_insert(sql)
        except Exception as e:
            logger.error(f"‰øùÂ≠ò‰∏ä‰º†ËÆ∞ÂΩïÂ§±Ë¥•: {e}")
    
    def _update_upload_log(self, upload_log: FileUploadLog):
        """Êõ¥Êñ∞‰∏ä‰º†ËÆ∞ÂΩï"""
        try:
            process_end_time_str = f"'{upload_log.process_end_time.strftime('%Y-%m-%d %H:%M:%S')}'" if upload_log.process_end_time else 'NULL'
            
            sql = f"""
                UPDATE ods_dash_social_file_upload_logs SET
                    original_rows = {upload_log.original_rows},
                    processed_rows = {upload_log.processed_rows},
                    success_rows = {upload_log.success_rows},
                    duplicate_rows = {upload_log.duplicate_rows},
                    error_rows = {upload_log.error_rows},
                    process_end_time = {process_end_time_str},
                    status = '{upload_log.status}',
                    error_message = '{upload_log.error_message.replace("'", "''")}'
                WHERE batch_id = '{upload_log.batch_id}'
            """
            self.db_config.execute_insert(sql)
        except Exception as e:
            logger.error(f"Êõ¥Êñ∞‰∏ä‰º†ËÆ∞ÂΩïÂ§±Ë¥•: {e}")
    
    def get_upload_statistics(self) -> Dict[str, Any]:
        """Ëé∑Âèñ‰∏ä‰º†ÁªüËÆ°‰ø°ÊÅØ"""
        try:
            # ÊÄª‰∏ä‰º†ÁªüËÆ°
            total_sql = "SELECT COUNT(*) as total_uploads FROM ods_dash_social_file_upload_logs"
            total_result = self.db_config.execute_query_dict(total_sql)
            total_uploads = total_result[0]['total_uploads'] if total_result else 0
            
            # ‰ªäÊó•‰∏ä‰º†ÁªüËÆ°
            today_sql = """
                SELECT COUNT(*) as today_uploads, 
                       SUM(success_rows) as today_success_rows
                FROM ods_dash_social_file_upload_logs 
                WHERE DATE(upload_time) = CURDATE()
            """
            today_result = self.db_config.execute_query_dict(today_sql)
            today_uploads = today_result[0]['today_uploads'] if today_result else 0
            today_success_rows = today_result[0]['today_success_rows'] if today_result else 0
            
            # ODSË°®ÊÄªËÆ∞ÂΩïÊï∞
            ods_sql = "SELECT COUNT(*) as total_ods_records FROM ods_dash_social_comments"
            ods_result = self.db_config.execute_query_dict(ods_sql)
            total_ods_records = ods_result[0]['total_ods_records'] if ods_result else 0
            
            return {
                'total_uploads': total_uploads,
                'today_uploads': today_uploads,
                'today_success_rows': today_success_rows,
                'total_ods_records': total_ods_records,
                'status': 'success'
            }
            
        except Exception as e:
            logger.error(f"Ëé∑Âèñ‰∏ä‰º†ÁªüËÆ°Â§±Ë¥•Ôºö{e}")
            return {'error': str(e)}
